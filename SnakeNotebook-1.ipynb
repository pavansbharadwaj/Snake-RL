{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snake with Q-learning algorithm from HA3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the model\n",
    "import dqn_model\n",
    "\n",
    "# Import dependencies\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import gym\n",
    "from collections import namedtuple\n",
    "from dqn_model import DoubleQLearningModel, ExperienceReplay\n",
    "\n",
    "# Set device as cpu/cuda\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the greedy policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eps_greedy_policy(q_values, eps):\n",
    "    '''\n",
    "    Creates an epsilon-greedy policy\n",
    "    :param q_values: set of Q-values of shape (num actions,)\n",
    "    :param eps: probability of taking a uniform random action \n",
    "    :return: policy of shape (num actions,)\n",
    "    '''\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    policy = np.zeros(len(q_values))\n",
    "    Pe = random.uniform(0,1)\n",
    "    \n",
    "    if(Pe <= eps):\n",
    "        policy = np.full((len(q_values),), 1/len(q_values))\n",
    "    else:\n",
    "        policy[np.argmax(q_values)] = 1\n",
    "    \n",
    "    return policy\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the three functions needed for dqn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_q_and_take_action(ddqn, state, eps):\n",
    "    '''\n",
    "    Calculate Q-values for current state, and take an action according to an epsilon-greedy policy.\n",
    "    Inputs:\n",
    "        ddqn   - DDQN model. An object holding the online / offline Q-networks, and some related methods.\n",
    "        state  - Current state. Numpy array, shape (1, num_states).\n",
    "        eps    - Exploration parameter.\n",
    "    Returns:\n",
    "        q_online_curr   - Q(s,a) for current state s. Numpy array, shape (1, num_actions) or  (num_actions,).\n",
    "        curr_action     - Selected action (0 or 1, i.e. left or right), sampled from epsilon-greedy policy. Integer.\n",
    "    '''\n",
    "    # FYI:\n",
    "    # ddqn.online_model & ddqn.offline_model are Pytorch modules for online / offline Q-networks, which take the state as input, and output the Q-values for all actions.\n",
    "    # Input shape (batch_size, num_states). Output shape (batch_size, num_actions).\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # Convert the state into a tensor\n",
    "    torch_state = torch.tensor(state,dtype=torch.float32)\n",
    "    \n",
    "    # --------- Online -------- # \n",
    "    \n",
    "    # Compute the Q-values and convert into numpy\n",
    "    Q_tensor_online = ddqn.online_model(torch_state)  \n",
    "    q_online_curr = Q_tensor_online.detach().cpu().numpy()\n",
    "    \n",
    "    # Compute the policy and convert into tensor\n",
    "    policy = eps_greedy_policy(q_online_curr, eps)\n",
    "    \n",
    "    return q_online_curr, curr_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_q_targets(q1_batch, q2_batch, r_batch, nonterminal_batch, gamma=.99):\n",
    "    '''\n",
    "    Calculates the Q target used for the loss\n",
    "    : param q1_batch: Batch of Q(s', a) from online network. FloatTensor, shape (N, num actions)\n",
    "    : param q2_batch: Batch of Q(s', a) from target network. FloatTensor, shape (N, num actions)\n",
    "    : param r_batch: Batch of rewards. FloatTensor, shape (N,)\n",
    "    : param nonterminal_batch: Batch of booleans, with False elements if state s' is terminal and True otherwise. BoolTensor, shape (N,)\n",
    "    : param gamma: Discount factor, float.\n",
    "    : return: Q target. FloatTensor, shape (N,)\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    N = q1_batch.shape[0]\n",
    "    Y = np.zeros(N)\n",
    "    \n",
    "    for i in range(N): \n",
    "        \n",
    "        if nonterminal_batch[i] == True:\n",
    "            values, a = q1_batch[i,:].max(0)\n",
    "            Y[i] = r_batch[i] + gamma * q2_batch[i,a]\n",
    "        \n",
    "        else: \n",
    "            Y[i] = r_batch[i]\n",
    "    \n",
    "    Y = torch.tensor(Y,dtype=torch.float32)\n",
    "    \n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_batch_and_calculate_loss(ddqn, replay_buffer, batch_size, gamma):\n",
    "    '''\n",
    "    Sample mini-batch from replay buffer, and compute the mini-batch loss\n",
    "    Inputs:\n",
    "        ddqn          - DDQN model. An object holding the online / offline Q-networks, and some related methods.\n",
    "        replay_buffer - Replay buffer object (from which smaples will be drawn)\n",
    "        batch_size    - Batch size\n",
    "        gamma         - Discount factor\n",
    "    Returns:\n",
    "        Mini-batch loss, on which .backward() will be called to compute gradient.\n",
    "    '''\n",
    "    # Sample a minibatch of transitions from replay buffer\n",
    "    curr_state, curr_action, reward, next_state, nonterminal = replay_buffer.sample_minibatch(batch_size)\n",
    "\n",
    "    # FYI:\n",
    "    # ddqn.online_model & ddqn.offline_model are Pytorch modules for online / offline Q-networks, which take the state as input, and output the Q-values for all actions.\n",
    "    # Input shape (batch_size, num_states). Output shape (batch_size, num_actions).\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # Define the q_online_curr\n",
    "    q_online_curr = ddqn.online_model(curr_state)\n",
    "    \n",
    "    # Define the q_online_next\n",
    "    q_online_next = ddqn.online_model(next_state)      \n",
    "    \n",
    "    # Define the q_offline_next\n",
    "    with torch.no_grad():\n",
    "        q_offline_next = ddqn.offline_model(next_state)  \n",
    "    \n",
    "    \n",
    "    q_target = calculate_q_targets(q_online_next, q_offline_next, reward, nonterminal, gamma=gamma)\n",
    "    loss = ddqn.calc_loss(q_online_curr, q_target, curr_action)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop_ddqn(ddqn, env, replay_buffer, num_episodes, enable_visualization=False, batch_size=64, gamma=.94):        \n",
    "    Transition = namedtuple(\"Transition\", [\"s\", \"a\", \"r\", \"next_s\", \"t\"])\n",
    "    eps = 1.\n",
    "    eps_end = .1 \n",
    "    eps_decay = .001\n",
    "    tau = 1000\n",
    "    cnt_updates = 0\n",
    "    R_buffer = []\n",
    "    R_avg = []\n",
    "    for i in range(num_episodes):\n",
    "        state = env.reset() # Initial state\n",
    "        #state = state[None,:] # Add singleton dimension, to represent as batch of size 1.\n",
    "        finish_episode = False # Initialize\n",
    "        ep_reward = 0 # Initialize \"Episodic reward\", i.e. the total reward for episode, when disregarding discount factor.\n",
    "        q_buffer = []\n",
    "        steps = 0\n",
    "        while not finish_episode:\n",
    "            if enable_visualization:\n",
    "                env.render() # comment this line out if you don't want to / cannot render the environment on your system\n",
    "            steps += 1\n",
    "            #print(steps)\n",
    "            # Take one step in environment. No need to compute gradients,\n",
    "            # we will just store transition to replay buffer, and later sample a whole batch\n",
    "            # from the replay buffer to actually take a gradient step.\n",
    "            q_online_curr, curr_action = calc_q_and_take_action(ddqn, state, eps)\n",
    "            q_buffer.append(q_online_curr)\n",
    "            \n",
    "            new_state, reward, finish_episode, _ = env.step(curr_action) # take one step in the evironment\n",
    "            #print(new_state.reshape(5,5))\n",
    "            #print(reward)\n",
    "            #print(finish_episode)\n",
    "            #new_state = new_state[None,:]\n",
    "            \n",
    "            # Assess whether terminal state was reached.\n",
    "            # The episode may end due to having reached 200 steps, but we should not regard this as reaching the terminal state, and hence not disregard Q(s',a) from the Q target.\n",
    "            # https://arxiv.org/abs/1712.00378\n",
    "            #print(steps)\n",
    "            nonterminal_to_buffer = not finish_episode or steps == 2000\n",
    "            #print(nonterminal_to_buffer)\n",
    "            \n",
    "            # Store experienced transition to replay buffer\n",
    "            replay_buffer.add(Transition(s=state, a=curr_action, r=reward, next_s=new_state, t=nonterminal_to_buffer))\n",
    "\n",
    "            state = new_state\n",
    "            ep_reward += reward\n",
    "            \n",
    "            # If replay buffer contains more than 1000 samples, perform one training step\n",
    "            if replay_buffer.buffer_length > 1000:\n",
    "                loss = sample_batch_and_calculate_loss(ddqn, replay_buffer, batch_size, gamma)\n",
    "                ddqn.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                ddqn.optimizer.step()\n",
    "\n",
    "                cnt_updates += 1\n",
    "                if cnt_updates % tau == 0:\n",
    "                    ddqn.update_target_network()\n",
    "            if steps == 200:\n",
    "                break\n",
    "                \n",
    "        eps = max(eps - eps_decay, eps_end) # decrease epsilon        \n",
    "        R_buffer.append(ep_reward)\n",
    "        \n",
    "        # Running average of episodic rewards (total reward, disregarding discount factor)\n",
    "        R_avg.append(.05 * R_buffer[i] + .95 * R_avg[i-1]) if i > 0 else R_avg.append(R_buffer[i])\n",
    "\n",
    "        print('Episode: {:d}, Total Reward (running avg): {:4.0f} ({:.2f}) Epsilon: {:.3f}, Avg Q: {:.4g}'.format(i, ep_reward, R_avg[-1], eps, np.mean(np.array(q_buffer))))\n",
    "        \n",
    "        # If running average > 195 (close to 200), the task is considered solved\n",
    "        if R_avg[-1] > 195:\n",
    "            return R_buffer, R_avg\n",
    "    return R_buffer, R_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import error, spaces, utils\n",
    "from gym.utils import seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the environment\n",
    "env = gym.make(\"snake5x5-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Enable visualization? Does not work in all environments.\n",
    "enable_visualization = False;\n",
    "\n",
    "\n",
    "# Initializations\n",
    "num_actions = env.action_space.n\n",
    "num_states = env.observation_space.shape[1]\n",
    "num_episodes = 10\n",
    "batch_size = 8\n",
    "gamma = .99\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# Object holding our online / offline Q-Networks\n",
    "ddqn = DoubleQLearningModel(device, num_states, num_actions, learning_rate)\n",
    "\n",
    "# Create replay buffer, where experience in form of tuples <s,a,r,s',t>, gathered from the environment is stored \n",
    "# for training\n",
    "replay_buffer = ExperienceReplay(device, num_states)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-42ccd25a4ac8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#print(state[0].reshape(9,9))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#print(state[1])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#print(state)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0minfo\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0mauxiliary\u001b[0m \u001b[0mdiagnostic\u001b[0m \u001b[0minformation\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhelpful\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdebugging\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msometimes\u001b[0m \u001b[0mlearning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \"\"\"\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "state = env.step(0)\n",
    "#print(state[0].reshape(9,9))\n",
    "#print(state[1])\n",
    "#print(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-4d3e0a997d14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR_avg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loop_ddqn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mddqn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplay_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_episodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menable_visualization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menable_visualization\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-cc45ee70207f>\u001b[0m in \u001b[0;36mtrain_loop_ddqn\u001b[0;34m(ddqn, env, replay_buffer, num_episodes, enable_visualization, batch_size, gamma)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mR_avg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Initial state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;31m#state = state[None,:] # Add singleton dimension, to represent as batch of size 1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mfinish_episode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;31m# Initialize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/gym/core.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0mobservation\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minitial\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \"\"\"\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "R, R_avg = train_loop_ddqn(ddqn, env, replay_buffer, num_episodes, enable_visualization=enable_visualization, batch_size=batch_size, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rewards = plt.plot(R, alpha=.4, label='R')\n",
    "avg_rewards = plt.plot(R_avg,label='avg R')\n",
    "plt.legend(bbox_to_anchor=(1.01, 1), loc=2, borderaxespad=0.)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylim(0, 80)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below for visualizing results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "display Surface quit",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-8788659fdcda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mterminal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;31m# reset terminal flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mterminal\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                 \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m.05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gym/envs/classic_control/snake5x5.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m     76\u001b[0m                     \u001b[0mpygame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mredrawWindow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0;31m#clock.tick(10)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gym/envs/classic_control/snake5x5.py\u001b[0m in \u001b[0;36mredrawWindow\u001b[0;34m(self, surface)\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;31m#global rows, width, s, snack, headersize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0msurface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Fills the screen with black\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrawGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msurface\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Will draw our grid lines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msurface\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Draw the snake\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: display Surface quit"
     ]
    }
   ],
   "source": [
    "import time\n",
    "num_episodes = 10\n",
    "\n",
    "if True:\n",
    "    for i in range(num_episodes):\n",
    "            state = env.reset() #reset to initial state\n",
    "            state = state[None,:]\n",
    "            terminal = False # reset terminal flag\n",
    "            while not terminal:\n",
    "                env.render()\n",
    "                time.sleep(.05)\n",
    "                with torch.no_grad():\n",
    "                    q_values = ddqn.online_model(torch.tensor(state, dtype=torch.float, device=device)).cpu().numpy()\n",
    "                policy = eps_greedy_policy(q_values.squeeze(), .1) # greedy policy\n",
    "                action = np.random.choice(num_actions, p=policy)\n",
    "                state, reward, terminal, _ = env.step(action) # take one step in the evironment\n",
    "                state = state[None,:]\n",
    "    # close window\n",
    "    env.close();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDQN - 11x11 Environment without function approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop_ddqn(ddqn, env, replay_buffer, num_episodes, enable_visualization=False, batch_size=64, gamma=.94):        \n",
    "    Transition = namedtuple(\"Transition\", [\"s\", \"a\", \"r\", \"next_s\", \"t\"])\n",
    "    eps = 1.\n",
    "    eps_end = .1 \n",
    "    eps_decay = .001\n",
    "    tau = 1000\n",
    "    cnt_updates = 0\n",
    "    R_buffer = []\n",
    "    R_avg = []\n",
    "    for i in range(num_episodes):\n",
    "        state = env.reset() # Initial state\n",
    "        #state = state[None,:] # Add singleton dimension, to represent as batch of size 1.\n",
    "        finish_episode = False # Initialize\n",
    "        ep_reward = 0 # Initialize \"Episodic reward\", i.e. the total reward for episode, when disregarding discount factor.\n",
    "        q_buffer = []\n",
    "        steps = 0\n",
    "        while not finish_episode:\n",
    "            if enable_visualization:\n",
    "                env.render() # comment this line out if you don't want to / cannot render the environment on your system\n",
    "            steps += 1\n",
    "            #print(steps)\n",
    "            # Take one step in environment. No need to compute gradients,\n",
    "            # we will just store transition to replay buffer, and later sample a whole batch\n",
    "            # from the replay buffer to actually take a gradient step.\n",
    "            q_online_curr, curr_action = calc_q_and_take_action(ddqn, state, eps)\n",
    "            q_buffer.append(q_online_curr)\n",
    "            \n",
    "            new_state, reward, finish_episode, score = env.step(curr_action) # take one step in the evironment\n",
    "            #print(new_state.reshape(5,5))\n",
    "            #print(reward)\n",
    "            #print(finish_episode)\n",
    "            #new_state = new_state[None,:]\n",
    "            \n",
    "            # Assess whether terminal state was reached.\n",
    "            # The episode may end due to having reached 200 steps, but we should not regard this as reaching the terminal state, and hence not disregard Q(s',a) from the Q target.\n",
    "            # https://arxiv.org/abs/1712.00378\n",
    "            #print(steps)\n",
    "            nonterminal_to_buffer = not finish_episode or steps == 2000\n",
    "            #print(nonterminal_to_buffer)\n",
    "            \n",
    "            # Store experienced transition to replay buffer\n",
    "            replay_buffer.add(Transition(s=state, a=curr_action, r=reward, next_s=new_state, t=nonterminal_to_buffer))\n",
    "\n",
    "            state = new_state\n",
    "            ep_reward += reward\n",
    "            \n",
    "            # If replay buffer contains more than 1000 samples, perform one training step\n",
    "            if replay_buffer.buffer_length > 1000:\n",
    "                loss = sample_batch_and_calculate_loss(ddqn, replay_buffer, batch_size, gamma)\n",
    "                ddqn.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                ddqn.optimizer.step()\n",
    "\n",
    "                cnt_updates += 1\n",
    "                if cnt_updates % tau == 0:\n",
    "                    ddqn.update_target_network()\n",
    "          \n",
    "        eps = max(eps - eps_decay, eps_end) # decrease epsilon        \n",
    "        R_buffer.append(ep_reward)\n",
    "        \n",
    "        # Running average of episodic rewards (total reward, disregarding discount factor)\n",
    "        R_avg.append(.05 * R_buffer[i] + .95 * R_avg[i-1]) if i > 0 else R_avg.append(R_buffer[i])\n",
    "\n",
    "        print('Episode: {:d}, Total Reward (running avg): {:4.0f} ({:.2f}) Epsilon: {:.3f}, Avg Q: {:.4g}, Score {}'.format(i, ep_reward, R_avg[-1], eps, np.mean(np.array(q_buffer)),score))\n",
    "        \n",
    "        # If running average > 195 (close to 200), the task is considered solved\n",
    "        if R_avg[-1] > 195:\n",
    "            return R_buffer, R_avg\n",
    "    return R_buffer, R_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "# Create the environment\n",
    "env = gym.make(\"Snake11x11-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable visualization? Does not work in all environments.\n",
    "enable_visualization = False;\n",
    "\n",
    "\n",
    "# Initializations\n",
    "num_actions = env.action_space.n\n",
    "num_states = env.observation_space.shape[1]\n",
    "num_episodes = 10\n",
    "batch_size = 10\n",
    "gamma = .94\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# Object holding our online / offline Q-Networks\n",
    "ddqn = DoubleQLearningModel(device, num_states, num_actions, learning_rate)\n",
    "\n",
    "# Create replay buffer, where experience in form of tuples <s,a,r,s',t>, gathered from the environment is stored \n",
    "# for training\n",
    "replay_buffer = ExperienceReplay(device, num_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Total Reward (running avg):    4 (4.00) Epsilon: 0.999, Avg Q: 1.277e-07, Score []\n",
      "Episode: 1, Total Reward (running avg):    4 (4.00) Epsilon: 0.998, Avg Q: 1.333e-07, Score []\n",
      "Episode: 2, Total Reward (running avg):    4 (4.00) Epsilon: 0.997, Avg Q: 1.531e-07, Score []\n",
      "Episode: 3, Total Reward (running avg):    4 (4.00) Epsilon: 0.996, Avg Q: 0.001113, Score []\n",
      "Episode: 4, Total Reward (running avg):    4 (4.00) Epsilon: 0.995, Avg Q: 0.00529, Score []\n",
      "Episode: 5, Total Reward (running avg):    4 (4.00) Epsilon: 0.994, Avg Q: 0.011, Score []\n",
      "Episode: 6, Total Reward (running avg):    4 (4.00) Epsilon: 0.993, Avg Q: 0.02011, Score []\n",
      "Episode: 7, Total Reward (running avg):    5 (4.05) Epsilon: 0.992, Avg Q: 0.01872, Score []\n",
      "Episode: 8, Total Reward (running avg):    4 (4.05) Epsilon: 0.991, Avg Q: 0.03319, Score []\n",
      "Episode: 9, Total Reward (running avg):    4 (4.05) Epsilon: 0.990, Avg Q: 0.04295, Score []\n"
     ]
    }
   ],
   "source": [
    "R, R_avg = train_loop_ddqn(ddqn, env, replay_buffer, num_episodes, enable_visualization=enable_visualization, batch_size=batch_size, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rewards = plt.plot(R, alpha=.4, label='R')\n",
    "avg_rewards = plt.plot(R_avg,label='avg R')\n",
    "plt.legend(bbox_to_anchor=(1.01, 1), loc=2, borderaxespad=0.)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylim(0, 100)\n",
    "plt.xlim(0, 1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "display Surface quit",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-f25050089bb7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mterminal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;31m# reset terminal flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mterminal\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                 \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m.05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gym/envs/classic_control/snake11x11.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m     80\u001b[0m                     \u001b[0mpygame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mredrawWindow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0;31m#clock.tick(10)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gym/envs/classic_control/snake11x11.py\u001b[0m in \u001b[0;36mredrawWindow\u001b[0;34m(self, surface)\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;31m#global rows, width, s, snack, headersize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m         \u001b[0msurface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Fills the screen with black\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrawGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msurface\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Will draw our grid lines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msurface\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Draw the snake\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: display Surface quit"
     ]
    }
   ],
   "source": [
    "import time\n",
    "num_episodes = 10\n",
    "#env = gym.make(\"Snake11x11-v0\")\n",
    "\n",
    "if True:\n",
    "    for i in range(num_episodes):\n",
    "            state = env.reset() #reset to initial state\n",
    "            state = state[None,:]\n",
    "            terminal = False # reset terminal flag\n",
    "            while not terminal:\n",
    "                env.render()\n",
    "                time.sleep(.05)\n",
    "                with torch.no_grad():\n",
    "                    q_values = ddqn.online_model(torch.tensor(state, dtype=torch.float, device=device)).cpu().numpy()\n",
    "                policy = eps_greedy_policy(q_values.squeeze(), .1) # greedy policy\n",
    "                action = np.random.choice(num_actions, p=policy)\n",
    "                state, reward, terminal, _ = env.step(action) # take one step in the evironment\n",
    "                state = state[None,:]\n",
    "    # close window\n",
    "    env.close();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDQN - 11x11 Environment with function approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the environment\n",
    "env = gym.make(\"Snake11x11-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the neural network which converts the states into a predicted action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs, hidden_size=256):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(num_inputs,  hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, num_outputs)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.linear1(state))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs  = env.observation_space.shape[1]\n",
    "num_outputs = env.action_space.n\n",
    "hidden_size = 256\n",
    "\n",
    "model = Model(num_inputs, num_outputs, hidden_size).to(device)\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop_ddqn(ddqn, model, env, replay_buffer, num_episodes, enable_visualization=False, batch_size=64, gamma=.94):        \n",
    "    Transition = namedtuple(\"Transition\", [\"s\", \"a\", \"r\", \"next_s\", \"t\"])\n",
    "    eps = 1.\n",
    "    eps_end = .1 \n",
    "    eps_decay = .001\n",
    "    tau = 1000\n",
    "    cnt_updates = 0\n",
    "    R_buffer = []\n",
    "    R_avg = []\n",
    "    for i in range(num_episodes):\n",
    "        state = env.reset() # Initial state\n",
    "        #state = state[None,:] # Add singleton dimension, to represent as batch of size 1.\n",
    "        finish_episode = False # Initialize\n",
    "        ep_reward = 0 # Initialize \"Episodic reward\", i.e. the total reward for episode, when disregarding discount factor.\n",
    "        q_buffer = []\n",
    "        steps = 0\n",
    "        while not finish_episode:\n",
    "            if enable_visualization:\n",
    "                env.render() # comment this line out if you don't want to / cannot render the environment on your system\n",
    "            steps += 1\n",
    "                    \n",
    "            state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "            q_values = model(state)\n",
    "            print(q_values)\n",
    "            q_values = q_values.detach().cpu().numpy()\n",
    "            print(q_values)\n",
    "            print(q_values[0])\n",
    "            \n",
    "            policy = eps_greedy_policy(q_values[0], eps)\n",
    "            \n",
    "            torch_policy = torch.tensor(policy,dtype=torch.float32)\n",
    "    \n",
    "            # Compute the action from the policy\n",
    "            actions = [i for i in range(len(policy))]\n",
    "            curr_action = random.choices(actions,policy)\n",
    "    \n",
    "            # Convert the curr_action into an int\n",
    "            curr_action = int(curr_action[0])\n",
    "            #curr_action = dist.sample()\n",
    "            \n",
    "            q_buffer.append(q_values[0])\n",
    "            \n",
    "            new_state, reward, finish_episode, score = env.step(curr_action) # take one step in the evironment\n",
    "            #print(new_state.reshape(5,5))\n",
    "            #print(reward)\n",
    "            #print(finish_episode)\n",
    "            #new_state = new_state[None,:]\n",
    "            \n",
    "            # Assess whether terminal state was reached.\n",
    "            # The episode may end due to having reached 200 steps, but we should not regard this as reaching the terminal state, and hence not disregard Q(s',a) from the Q target.\n",
    "            # https://arxiv.org/abs/1712.00378\n",
    "            #print(steps)\n",
    "            nonterminal_to_buffer = not finish_episode or steps == 2000\n",
    "            #print(nonterminal_to_buffer)\n",
    "            \n",
    "            # Store experienced transition to replay buffer\n",
    "            replay_buffer.add(Transition(s=state, a=curr_action, r=reward, next_s=new_state, t=nonterminal_to_buffer))\n",
    "\n",
    "            state = new_state\n",
    "            ep_reward += reward\n",
    "            \n",
    "            # If replay buffer contains more than 1000 samples, perform one training step\n",
    "            if replay_buffer.buffer_length > 1000:\n",
    "                loss = sample_batch_and_calculate_loss(model, replay_buffer, batch_size, gamma)\n",
    "                model.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                model.optimizer.step()\n",
    "\n",
    "                cnt_updates += 1\n",
    "                if cnt_updates % tau == 0:\n",
    "                    model.update_target_network()\n",
    "          \n",
    "        eps = max(eps - eps_decay, eps_end) # decrease epsilon        \n",
    "        R_buffer.append(ep_reward)\n",
    "        \n",
    "        # Running average of episodic rewards (total reward, disregarding discount factor)\n",
    "        R_avg.append(.05 * R_buffer[i] + .95 * R_avg[i-1]) if i > 0 else R_avg.append(R_buffer[i])\n",
    "\n",
    "        print('Episode: {:d}, Total Reward (running avg): {:4.0f} ({:.2f}) Epsilon: {:.3f}, Avg Q: {:.4g}'.format(i, ep_reward, R_avg[-1], eps, np.mean(np.array(q_buffer))))\n",
    "        \n",
    "        # If running average > 195 (close to 200), the task is considered solved\n",
    "        if R_avg[-1] > 195:\n",
    "            return R_buffer, R_avg\n",
    "    return R_buffer, R_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable visualization? Does not work in all environments.\n",
    "enable_visualization = False;\n",
    "\n",
    "\n",
    "# Initializations\n",
    "num_actions = env.action_space.n\n",
    "num_states = env.observation_space.shape[1]\n",
    "num_episodes = 1000\n",
    "batch_size = 10\n",
    "gamma = .94\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# Object holding our online / offline Q-Networks\n",
    "ddqn = DoubleQLearningModel(device, num_states, num_actions, learning_rate)\n",
    "\n",
    "# Create replay buffer, where experience in form of tuples <s,a,r,s',t>, gathered from the environment is stored \n",
    "# for training\n",
    "replay_buffer = ExperienceReplay(device, num_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 0.0972, 0.0000, 0.0066]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.09715915 0.         0.00663432]]\n",
      "[0.         0.09715915 0.         0.00663432]\n",
      "tensor([[0.0000, 0.1607, 0.0000, 0.0203]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.16073844 0.         0.0203396 ]]\n",
      "[0.         0.16073844 0.         0.0203396 ]\n",
      "tensor([[0.0000, 0.0394, 0.0000, 0.0325]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.03942755 0.         0.03247177]]\n",
      "[0.         0.03942755 0.         0.03247177]\n",
      "tensor([[0.0000, 0.1708, 0.0365, 0.0173]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.17081125 0.03646655 0.01730601]]\n",
      "[0.         0.17081125 0.03646655 0.01730601]\n",
      "tensor([[0.0000, 0.0557, 0.0085, 0.0581]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.05569209 0.00849248 0.05805771]]\n",
      "[0.         0.05569209 0.00849248 0.05805771]\n",
      "tensor([[0.0000, 0.0874, 0.0000, 0.0786]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.08744999 0.         0.07864934]]\n",
      "[0.         0.08744999 0.         0.07864934]\n",
      "tensor([[0.0000, 0.1653, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.16526085 0.         0.        ]]\n",
      "[0.         0.16526085 0.         0.        ]\n",
      "tensor([[0.0000, 0.0000, 0.0517, 0.0861]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.         0.05165702 0.08606257]]\n",
      "[0.         0.         0.05165702 0.08606257]\n",
      "tensor([[0.0000, 0.0567, 0.0283, 0.0602]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.05671835 0.02834381 0.06020895]]\n",
      "[0.         0.05671835 0.02834381 0.06020895]\n",
      "tensor([[0.0000, 0.1669, 0.0000, 0.0223]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.16689837 0.         0.0222559 ]]\n",
      "[0.         0.16689837 0.         0.0222559 ]\n",
      "tensor([[0.0220, 0.1234, 0.0000, 0.0503]], grad_fn=<ReluBackward0>)\n",
      "[[0.02202775 0.12335736 0.         0.05031086]]\n",
      "[0.02202775 0.12335736 0.         0.05031086]\n",
      "tensor([[0.0000, 0.0767, 0.0000, 0.0125]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.07673296 0.         0.0125279 ]]\n",
      "[0.         0.07673296 0.         0.0125279 ]\n",
      "tensor([[0.0000, 0.2005, 0.0347, 0.0867]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.20048538 0.03465756 0.08673979]]\n",
      "[0.         0.20048538 0.03465756 0.08673979]\n",
      "tensor([[0.0000, 0.1996, 0.0000, 0.0733]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.19958426 0.         0.07333019]]\n",
      "[0.         0.19958426 0.         0.07333019]\n",
      "tensor([[0.0000, 0.2333, 0.0362, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.23328969 0.03618588 0.        ]]\n",
      "[0.         0.23328969 0.03618588 0.        ]\n",
      "tensor([[0.0000, 0.1440, 0.0170, 0.0777]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.14395654 0.01695046 0.0776806 ]]\n",
      "[0.         0.14395654 0.01695046 0.0776806 ]\n",
      "tensor([[0.0000, 0.1891, 0.0102, 0.0380]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.1890873  0.01021241 0.03802674]]\n",
      "[0.         0.1890873  0.01021241 0.03802674]\n",
      "tensor([[0.0000, 0.0541, 0.0000, 0.0809]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.05414326 0.         0.08087718]]\n",
      "[0.         0.05414326 0.         0.08087718]\n",
      "tensor([[0.0000, 0.1971, 0.0000, 0.0088]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.19711763 0.         0.00876655]]\n",
      "[0.         0.19711763 0.         0.00876655]\n",
      "tensor([[0.0000, 0.0949, 0.0000, 0.0063]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.09486176 0.         0.00625056]]\n",
      "[0.         0.09486176 0.         0.00625056]\n",
      "tensor([[0.0095, 0.0984, 0.0000, 0.0016]], grad_fn=<ReluBackward0>)\n",
      "[[0.00945507 0.09837377 0.         0.0015638 ]]\n",
      "[0.00945507 0.09837377 0.         0.0015638 ]\n",
      "tensor([[0.0000, 0.1669, 0.0000, 0.0223]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.16689837 0.         0.0222559 ]]\n",
      "[0.         0.16689837 0.         0.0222559 ]\n",
      "tensor([[0.0000, 0.1685, 0.0000, 0.0292]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.16846403 0.         0.02919924]]\n",
      "[0.         0.16846403 0.         0.02919924]\n",
      "tensor([[0.0000, 0.1191, 0.0000, 0.0125]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.11911604 0.         0.01253251]]\n",
      "[0.         0.11911604 0.         0.01253251]\n",
      "tensor([[0.0220, 0.1234, 0.0000, 0.0503]], grad_fn=<ReluBackward0>)\n",
      "[[0.02202775 0.12335736 0.         0.05031086]]\n",
      "[0.02202775 0.12335736 0.         0.05031086]\n",
      "tensor([[0.0000, 0.0158, 0.0077, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.01581155 0.00767375 0.        ]]\n",
      "[0.         0.01581155 0.00767375 0.        ]\n",
      "tensor([[0.0000, 0.2012, 0.0000, 0.0734]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.20123194 0.         0.07337503]]\n",
      "[0.         0.20123194 0.         0.07337503]\n",
      "tensor([[0.0119, 0.1319, 0.0744, 0.1017]], grad_fn=<ReluBackward0>)\n",
      "[[0.01187843 0.13190992 0.07441185 0.10172003]]\n",
      "[0.01187843 0.13190992 0.07441185 0.10172003]\n",
      "tensor([[0.0000, 0.0730, 0.0087, 0.0476]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.07299574 0.00872893 0.04757761]]\n",
      "[0.         0.07299574 0.00872893 0.04757761]\n",
      "tensor([[0.0000, 0.1871, 0.0205, 0.1026]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.1870799  0.02046767 0.10256782]]\n",
      "[0.         0.1870799  0.02046767 0.10256782]\n",
      "tensor([[0.0000, 0.1152, 0.0000, 0.0234]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.11523686 0.         0.02338288]]\n",
      "[0.         0.11523686 0.         0.02338288]\n",
      "tensor([[0.0000, 0.2260, 0.0000, 0.0929]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.22598639 0.         0.09289242]]\n",
      "[0.         0.22598639 0.         0.09289242]\n",
      "tensor([[0.0000, 0.0658, 0.0322, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.06582455 0.03224598 0.        ]]\n",
      "[0.         0.06582455 0.03224598 0.        ]\n",
      "tensor([[0.0000, 0.1127, 0.0000, 0.0046]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.1126679  0.         0.00455797]]\n",
      "[0.         0.1126679  0.         0.00455797]\n",
      "tensor([[0.0000, 0.1576, 0.0000, 0.1063]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.15764992 0.         0.10633217]]\n",
      "[0.         0.15764992 0.         0.10633217]\n",
      "tensor([[0.0000, 0.1847, 0.0222, 0.1100]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.18472472 0.02224877 0.10996342]]\n",
      "[0.         0.18472472 0.02224877 0.10996342]\n",
      "tensor([[0.0000, 0.1400, 0.0096, 0.1111]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.13996844 0.00955062 0.11114144]]\n",
      "[0.         0.13996844 0.00955062 0.11114144]\n",
      "tensor([[0.0000, 0.1397, 0.0016, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.13965856 0.00161304 0.        ]]\n",
      "[0.         0.13965856 0.00161304 0.        ]\n",
      "tensor([[0.0000, 0.1319, 0.0000, 0.1050]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.13194679 0.         0.10500229]]\n",
      "[0.         0.13194679 0.         0.10500229]\n",
      "tensor([[0.0000, 0.0438, 0.0450, 0.0539]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.04384495 0.04502706 0.05385141]]\n",
      "[0.         0.04384495 0.04502706 0.05385141]\n",
      "tensor([[0.0000, 0.0777, 0.0000, 0.0313]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.07765079 0.         0.0313229 ]]\n",
      "[0.         0.07765079 0.         0.0313229 ]\n",
      "tensor([[0.0000, 0.1325, 0.0398, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.1324601  0.03984443 0.        ]]\n",
      "[0.         0.1324601  0.03984443 0.        ]\n",
      "tensor([[0.0000e+00, 1.5866e-01, 0.0000e+00, 4.6562e-05]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "[[0.0000000e+00 1.5865856e-01 0.0000000e+00 4.6562403e-05]]\n",
      "[0.0000000e+00 1.5865856e-01 0.0000000e+00 4.6562403e-05]\n",
      "tensor([[0.0000, 0.2077, 0.0000, 0.0934]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.20771524 0.         0.09337102]]\n",
      "[0.         0.20771524 0.         0.09337102]\n",
      "tensor([[0.0000, 0.1969, 0.0566, 0.0650]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.19685803 0.05657362 0.06495835]]\n",
      "[0.         0.19685803 0.05657362 0.06495835]\n",
      "tensor([[0.0000, 0.1340, 0.0000, 0.0421]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.13395381 0.         0.04207561]]\n",
      "[0.         0.13395381 0.         0.04207561]\n",
      "tensor([[0.0000, 0.0567, 0.0283, 0.0602]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.05671835 0.02834381 0.06020895]]\n",
      "[0.         0.05671835 0.02834381 0.06020895]\n",
      "tensor([[0.0000, 0.0725, 0.0000, 0.0337]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.07252765 0.         0.03368969]]\n",
      "[0.         0.07252765 0.         0.03368969]\n",
      "tensor([[0.0095, 0.0984, 0.0000, 0.0016]], grad_fn=<ReluBackward0>)\n",
      "[[0.00945507 0.09837377 0.         0.0015638 ]]\n",
      "[0.00945507 0.09837377 0.         0.0015638 ]\n",
      "tensor([[0.0000, 0.1669, 0.0000, 0.0223]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.16689837 0.         0.0222559 ]]\n",
      "[0.         0.16689837 0.         0.0222559 ]\n",
      "tensor([[0.0000, 0.1685, 0.0000, 0.0292]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.16846403 0.         0.02919924]]\n",
      "[0.         0.16846403 0.         0.02919924]\n",
      "tensor([[0.0000, 0.1073, 0.0179, 0.0953]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.1073169  0.01793623 0.09527986]]\n",
      "[0.         0.1073169  0.01793623 0.09527986]\n",
      "tensor([[0.0000, 0.1603, 0.0080, 0.0392]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.16030711 0.00801338 0.0392357 ]]\n",
      "[0.         0.16030711 0.00801338 0.0392357 ]\n",
      "tensor([[0.0000, 0.1191, 0.0000, 0.0125]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.11911604 0.         0.01253251]]\n",
      "[0.         0.11911604 0.         0.01253251]\n",
      "tensor([[0.0000, 0.1685, 0.0000, 0.0292]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.16846403 0.         0.02919924]]\n",
      "[0.         0.16846403 0.         0.02919924]\n",
      "tensor([[0.0000, 0.1340, 0.0000, 0.0421]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.13395381 0.         0.04207561]]\n",
      "[0.         0.13395381 0.         0.04207561]\n",
      "tensor([[0.0000, 0.0567, 0.0283, 0.0602]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.05671835 0.02834381 0.06020895]]\n",
      "[0.         0.05671835 0.02834381 0.06020895]\n",
      "tensor([[0.0000, 0.0725, 0.0000, 0.0337]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.07252765 0.         0.03368969]]\n",
      "[0.         0.07252765 0.         0.03368969]\n",
      "tensor([[0.0095, 0.0984, 0.0000, 0.0016]], grad_fn=<ReluBackward0>)\n",
      "[[0.00945507 0.09837377 0.         0.0015638 ]]\n",
      "[0.00945507 0.09837377 0.         0.0015638 ]\n",
      "tensor([[0.0000, 0.0767, 0.0000, 0.0125]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.07673296 0.         0.0125279 ]]\n",
      "[0.         0.07673296 0.         0.0125279 ]\n",
      "tensor([[0.0220, 0.1234, 0.0000, 0.0503]], grad_fn=<ReluBackward0>)\n",
      "[[0.02202775 0.12335736 0.         0.05031086]]\n",
      "[0.02202775 0.12335736 0.         0.05031086]\n",
      "tensor([[0.0000, 0.1191, 0.0000, 0.0125]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.11911604 0.         0.01253251]]\n",
      "[0.         0.11911604 0.         0.01253251]\n",
      "tensor([[0.0000, 0.1685, 0.0000, 0.0292]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.16846403 0.         0.02919924]]\n",
      "[0.         0.16846403 0.         0.02919924]\n",
      "tensor([[0.0000, 0.1669, 0.0000, 0.0223]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.16689837 0.         0.0222559 ]]\n",
      "[0.         0.16689837 0.         0.0222559 ]\n",
      "tensor([[0.0095, 0.0984, 0.0000, 0.0016]], grad_fn=<ReluBackward0>)\n",
      "[[0.00945507 0.09837377 0.         0.0015638 ]]\n",
      "[0.00945507 0.09837377 0.         0.0015638 ]\n",
      "tensor([[0.0000, 0.0767, 0.0000, 0.0125]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.07673296 0.         0.0125279 ]]\n",
      "[0.         0.07673296 0.         0.0125279 ]\n",
      "tensor([[0.0000, 0.1971, 0.0000, 0.0088]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.19711763 0.         0.00876655]]\n",
      "[0.         0.19711763 0.         0.00876655]\n",
      "tensor([[0.0000, 0.0541, 0.0000, 0.0809]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.05414326 0.         0.08087718]]\n",
      "[0.         0.05414326 0.         0.08087718]\n",
      "tensor([[0.0000, 0.0972, 0.0000, 0.0066]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.09715915 0.         0.00663432]]\n",
      "[0.         0.09715915 0.         0.00663432]\n",
      "tensor([[0.0000, 0.1681, 0.0000, 0.1041]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.16808686 0.         0.10408671]]\n",
      "[0.         0.16808686 0.         0.10408671]\n",
      "tensor([[0.0000, 0.0881, 0.0256, 0.0488]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.08814646 0.02564962 0.04884421]]\n",
      "[0.         0.08814646 0.02564962 0.04884421]\n",
      "tensor([[0.0000, 0.0751, 0.0799, 0.0308]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.07506515 0.07986861 0.0307664 ]]\n",
      "[0.         0.07506515 0.07986861 0.0307664 ]\n",
      "tensor([[0.0000, 0.0797, 0.0000, 0.0154]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.07966228 0.         0.0153519 ]]\n",
      "[0.         0.07966228 0.         0.0153519 ]\n",
      "tensor([[0.0000, 0.1030, 0.0000, 0.0173]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.10298092 0.         0.01728786]]\n",
      "[0.         0.10298092 0.         0.01728786]\n",
      "tensor([[0.0000, 0.0350, 0.0195, 0.0070]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.03496804 0.01945038 0.00696329]]\n",
      "[0.         0.03496804 0.01945038 0.00696329]\n",
      "tensor([[0.0000, 0.0848, 0.0000, 0.0201]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.08483891 0.         0.020073  ]]\n",
      "[0.         0.08483891 0.         0.020073  ]\n",
      "tensor([[0.0000, 0.0438, 0.0450, 0.0539]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.04384495 0.04502706 0.05385141]]\n",
      "[0.         0.04384495 0.04502706 0.05385141]\n",
      "tensor([[0.0000, 0.1325, 0.0982, 0.0874]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.13251194 0.09820447 0.08737358]]\n",
      "[0.         0.13251194 0.09820447 0.08737358]\n",
      "tensor([[0.0000, 0.1314, 0.0000, 0.0327]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.13142684 0.         0.03269374]]\n",
      "[0.         0.13142684 0.         0.03269374]\n",
      "tensor([[0.0000, 0.0873, 0.0000, 0.0729]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.08725414 0.         0.07291479]]\n",
      "[0.         0.08725414 0.         0.07291479]\n",
      "tensor([[0.0000, 0.1847, 0.0222, 0.1100]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.18472472 0.02224877 0.10996342]]\n",
      "[0.         0.18472472 0.02224877 0.10996342]\n",
      "tensor([[0.0000, 0.1958, 0.0193, 0.0730]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.19583422 0.01929801 0.07301047]]\n",
      "[0.         0.19583422 0.01929801 0.07301047]\n",
      "tensor([[0.0000, 0.0959, 0.0000, 0.0692]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.09588685 0.         0.06921443]]\n",
      "[0.         0.09588685 0.         0.06921443]\n",
      "tensor([[0.0000, 0.0658, 0.0322, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.06582455 0.03224598 0.        ]]\n",
      "[0.         0.06582455 0.03224598 0.        ]\n",
      "tensor([[0.0000, 0.0961, 0.0413, 0.0709]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.09613679 0.04132368 0.07090718]]\n",
      "[0.         0.09613679 0.04132368 0.07090718]\n",
      "tensor([[0.0000, 0.2333, 0.0362, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.23328969 0.03618588 0.        ]]\n",
      "[0.         0.23328969 0.03618588 0.        ]\n",
      "tensor([[0.0000, 0.1440, 0.0170, 0.0777]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.14395654 0.01695046 0.0776806 ]]\n",
      "[0.         0.14395654 0.01695046 0.0776806 ]\n",
      "tensor([[0.0000, 0.1550, 0.0000, 0.0494]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.15501934 0.         0.04943351]]\n",
      "[0.         0.15501934 0.         0.04943351]\n",
      "tensor([[0.0000, 0.2387, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.23869774 0.         0.        ]]\n",
      "[0.         0.23869774 0.         0.        ]\n",
      "tensor([[0.0000, 0.1738, 0.0000, 0.1044]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.17384472 0.         0.10438143]]\n",
      "[0.         0.17384472 0.         0.10438143]\n",
      "tensor([[0.0000, 0.2460, 0.0348, 0.1369]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.24604918 0.03482236 0.13693811]]\n",
      "[0.         0.24604918 0.03482236 0.13693811]\n",
      "tensor([[0.0000, 0.1741, 0.0189, 0.0904]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.17407787 0.0189151  0.09041858]]\n",
      "[0.         0.17407787 0.0189151  0.09041858]\n",
      "tensor([[0.0000, 0.1245, 0.0039, 0.0432]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.12447803 0.00387293 0.04320908]]\n",
      "[0.         0.12447803 0.00387293 0.04320908]\n",
      "tensor([[0.0000, 0.1735, 0.0000, 0.0418]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.17351447 0.         0.04181233]]\n",
      "[0.         0.17351447 0.         0.04181233]\n",
      "tensor([[0.0000, 0.1470, 0.0000, 0.0437]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.14702298 0.         0.04367911]]\n",
      "[0.         0.14702298 0.         0.04367911]\n",
      "tensor([[0.0000, 0.1928, 0.0180, 0.0994]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.19279443 0.01798903 0.0993577 ]]\n",
      "[0.         0.19279443 0.01798903 0.0993577 ]\n",
      "tensor([[0.0000, 0.1762, 0.0406, 0.0844]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.17617545 0.04056763 0.08436924]]\n",
      "[0.         0.17617545 0.04056763 0.08436924]\n",
      "tensor([[0.0000, 0.2252, 0.0697, 0.0903]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.22524248 0.06965137 0.09027338]]\n",
      "[0.         0.22524248 0.06965137 0.09027338]\n",
      "tensor([[0.0000, 0.1398, 0.0000, 0.0575]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.13984634 0.         0.05748416]]\n",
      "[0.         0.13984634 0.         0.05748416]\n",
      "tensor([[0.0000, 0.0793, 0.0000, 0.0364]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.07928494 0.         0.03640247]]\n",
      "[0.         0.07928494 0.         0.03640247]\n",
      "tensor([[0.0000, 0.1123, 0.0000, 0.0760]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.11231162 0.         0.07602151]]\n",
      "[0.         0.11231162 0.         0.07602151]\n",
      "tensor([[0.0000, 0.1426, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.        0.1425839 0.        0.       ]]\n",
      "[0.        0.1425839 0.        0.       ]\n",
      "tensor([[0.0000, 0.1627, 0.0000, 0.0551]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.16272941 0.         0.05512524]]\n",
      "[0.         0.16272941 0.         0.05512524]\n",
      "tensor([[0.0000, 0.0929, 0.0633, 0.1230]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.09293069 0.06334838 0.12304326]]\n",
      "[0.         0.09293069 0.06334838 0.12304326]\n",
      "tensor([[0.0000, 0.0570, 0.0253, 0.1622]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.05703354 0.0252521  0.16218969]]\n",
      "[0.         0.05703354 0.0252521  0.16218969]\n",
      "tensor([[0.0000, 0.1128, 0.0000, 0.1393]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.11277086 0.         0.13928476]]\n",
      "[0.         0.11277086 0.         0.13928476]\n",
      "tensor([[0.0000, 0.1791, 0.0000, 0.1104]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.1791273  0.         0.11037964]]\n",
      "[0.         0.1791273  0.         0.11037964]\n",
      "tensor([[0.0000, 0.1666, 0.0000, 0.1067]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.16661978 0.         0.10667985]]\n",
      "[0.         0.16661978 0.         0.10667985]\n",
      "tensor([[0.0000, 0.1651, 0.0000, 0.1271]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.16505969 0.         0.1270825 ]]\n",
      "[0.         0.16505969 0.         0.1270825 ]\n",
      "tensor([[0.0640, 0.2428, 0.0429, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.06396089 0.24283388 0.04287045 0.        ]]\n",
      "[0.06396089 0.24283388 0.04287045 0.        ]\n",
      "tensor([[0.0000, 0.1999, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.19987895 0.         0.        ]]\n",
      "[0.         0.19987895 0.         0.        ]\n",
      "tensor([[0.0000, 0.1491, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.14914961 0.         0.        ]]\n",
      "[0.         0.14914961 0.         0.        ]\n",
      "tensor([[0.0000, 0.1081, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.10806047 0.         0.        ]]\n",
      "[0.         0.10806047 0.         0.        ]\n",
      "tensor([[0.0000, 0.1039, 0.0000, 0.0500]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.10386536 0.         0.04998123]]\n",
      "[0.         0.10386536 0.         0.04998123]\n",
      "tensor([[0.0000, 0.1792, 0.0000, 0.0446]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.17920795 0.         0.04461648]]\n",
      "[0.         0.17920795 0.         0.04461648]\n",
      "tensor([[0.0000, 0.0647, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.06471179 0.         0.        ]]\n",
      "[0.         0.06471179 0.         0.        ]\n",
      "tensor([[0.0000, 0.0245, 0.0228, 0.0252]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.02447542 0.02282877 0.02522902]]\n",
      "[0.         0.02447542 0.02282877 0.02522902]\n",
      "tensor([[0.0000, 0.0718, 0.0478, 0.0704]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.0718065  0.04775511 0.07038218]]\n",
      "[0.         0.0718065  0.04775511 0.07038218]\n",
      "tensor([[0.0621, 0.0198, 0.0346, 0.0577]], grad_fn=<ReluBackward0>)\n",
      "[[0.06213725 0.01984839 0.03461394 0.05774206]]\n",
      "[0.06213725 0.01984839 0.03461394 0.05774206]\n",
      "tensor([[0.0083, 0.0429, 0.0160, 0.0731]], grad_fn=<ReluBackward0>)\n",
      "[[0.00834879 0.04287298 0.01598641 0.07307531]]\n",
      "[0.00834879 0.04287298 0.01598641 0.07307531]\n",
      "tensor([[0.0000, 0.1248, 0.0000, 0.1220]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.12478042 0.         0.12203322]]\n",
      "[0.         0.12478042 0.         0.12203322]\n",
      "tensor([[0.0000, 0.1365, 0.0000, 0.0095]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.13647445 0.         0.00953121]]\n",
      "[0.         0.13647445 0.         0.00953121]\n",
      "tensor([[0.0000, 0.0644, 0.0000, 0.0447]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.0644303  0.         0.04467228]]\n",
      "[0.         0.0644303  0.         0.04467228]\n",
      "tensor([[0.0000, 0.0326, 0.0000, 0.1195]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.03256436 0.         0.11946934]]\n",
      "[0.         0.03256436 0.         0.11946934]\n",
      "tensor([[0.0365, 0.0809, 0.0000, 0.0410]], grad_fn=<ReluBackward0>)\n",
      "[[0.03645031 0.08094958 0.         0.04097211]]\n",
      "[0.03645031 0.08094958 0.         0.04097211]\n",
      "tensor([[0.0000, 0.1556, 0.0000, 0.0780]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.15562478 0.         0.07800866]]\n",
      "[0.         0.15562478 0.         0.07800866]\n",
      "tensor([[0.0000, 0.1285, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.12845533 0.         0.        ]]\n",
      "[0.         0.12845533 0.         0.        ]\n",
      "tensor([[0.0000, 0.1884, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.18838227 0.         0.        ]]\n",
      "[0.         0.18838227 0.         0.        ]\n",
      "tensor([[0.0000, 0.0638, 0.0000, 0.0540]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.06382672 0.         0.05396043]]\n",
      "[0.         0.06382672 0.         0.05396043]\n",
      "tensor([[0.0000, 0.1274, 0.0000, 0.0649]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.12743074 0.         0.06486224]]\n",
      "[0.         0.12743074 0.         0.06486224]\n",
      "tensor([[0.0000, 0.1822, 0.0000, 0.0476]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.18222454 0.         0.04764886]]\n",
      "[0.         0.18222454 0.         0.04764886]\n",
      "tensor([[0.0000, 0.2862, 0.0000, 0.0449]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.28622374 0.         0.04491706]]\n",
      "[0.         0.28622374 0.         0.04491706]\n",
      "tensor([[0.0000, 0.1773, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.17730963 0.         0.        ]]\n",
      "[0.         0.17730963 0.         0.        ]\n",
      "tensor([[0.0000, 0.0652, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.06524502 0.         0.        ]]\n",
      "[0.         0.06524502 0.         0.        ]\n",
      "tensor([[0.0000, 0.2013, 0.0000, 0.0914]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.20132585 0.         0.09137167]]\n",
      "[0.         0.20132585 0.         0.09137167]\n",
      "tensor([[0.0000, 0.1643, 0.0000, 0.0879]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.1642755  0.         0.08790077]]\n",
      "[0.         0.1642755  0.         0.08790077]\n",
      "tensor([[0.0000, 0.1697, 0.0000, 0.0077]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.16966978 0.         0.00774856]]\n",
      "[0.         0.16966978 0.         0.00774856]\n",
      "tensor([[0.0103, 0.1103, 0.0000, 0.0015]], grad_fn=<ReluBackward0>)\n",
      "[[0.01026884 0.1103074  0.         0.00150216]]\n",
      "[0.01026884 0.1103074  0.         0.00150216]\n",
      "tensor([[0.0000, 0.0337, 0.0634, 0.0353]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.03367425 0.06336164 0.0352629 ]]\n",
      "[0.         0.03367425 0.06336164 0.0352629 ]\n",
      "tensor([[0.0000, 0.0586, 0.0031, 0.0849]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.05855639 0.00307032 0.0849165 ]]\n",
      "[0.         0.05855639 0.00307032 0.0849165 ]\n",
      "tensor([[0.0055, 0.2251, 0.0000, 0.0425]], grad_fn=<ReluBackward0>)\n",
      "[[0.00548123 0.22513896 0.         0.04250291]]\n",
      "[0.00548123 0.22513896 0.         0.04250291]\n",
      "tensor([[0.0103, 0.1103, 0.0000, 0.0015]], grad_fn=<ReluBackward0>)\n",
      "[[0.01026884 0.1103074  0.         0.00150216]]\n",
      "[0.01026884 0.1103074  0.         0.00150216]\n",
      "tensor([[0.0000, 0.0337, 0.0634, 0.0353]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.03367425 0.06336164 0.0352629 ]]\n",
      "[0.         0.03367425 0.06336164 0.0352629 ]\n",
      "tensor([[0.0000, 0.0586, 0.0031, 0.0849]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.05855639 0.00307032 0.0849165 ]]\n",
      "[0.         0.05855639 0.00307032 0.0849165 ]\n",
      "tensor([[0.0000, 0.1858, 0.0000, 0.0605]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.1857698  0.         0.06051034]]\n",
      "[0.         0.1857698  0.         0.06051034]\n",
      "tensor([[0.0000, 0.1891, 0.0000, 0.1117]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.1891059  0.         0.11173159]]\n",
      "[0.         0.1891059  0.         0.11173159]\n",
      "tensor([[0.0047, 0.0958, 0.0000, 0.0363]], grad_fn=<ReluBackward0>)\n",
      "[[0.00470509 0.09577399 0.         0.03627544]]\n",
      "[0.00470509 0.09577399 0.         0.03627544]\n",
      "tensor([[0.0230, 0.0188, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.02301738 0.01881985 0.         0.        ]]\n",
      "[0.02301738 0.01881985 0.         0.        ]\n",
      "tensor([[0.0021, 0.0705, 0.0000, 0.0228]], grad_fn=<ReluBackward0>)\n",
      "[[0.00213576 0.07045797 0.         0.02282118]]\n",
      "[0.00213576 0.07045797 0.         0.02282118]\n",
      "tensor([[0.0000, 0.0266, 0.0000, 0.0101]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.02663252 0.         0.01007335]]\n",
      "[0.         0.02663252 0.         0.01007335]\n",
      "tensor([[0.0000, 0.0000, 0.0368, 0.0563]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.         0.03681672 0.05630411]]\n",
      "[0.         0.         0.03681672 0.05630411]\n",
      "tensor([[0.0000, 0.0724, 0.0543, 0.0689]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.07235508 0.05432475 0.06889537]]\n",
      "[0.         0.07235508 0.05432475 0.06889537]\n",
      "tensor([[0.0000, 0.0089, 0.0000, 0.1552]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.00890009 0.         0.15523195]]\n",
      "[0.         0.00890009 0.         0.15523195]\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.1691]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.         0.         0.16905043]]\n",
      "[0.         0.         0.         0.16905043]\n",
      "tensor([[0.0000, 0.0802, 0.0000, 0.0641]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.08024355 0.         0.0640762 ]]\n",
      "[0.         0.08024355 0.         0.0640762 ]\n",
      "tensor([[0.0000, 0.1815, 0.0000, 0.0387]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.18146314 0.         0.03872896]]\n",
      "[0.         0.18146314 0.         0.03872896]\n",
      "tensor([[0.0000, 0.2041, 0.0147, 0.0534]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.20406726 0.01470832 0.0533989 ]]\n",
      "[0.         0.20406726 0.01470832 0.0533989 ]\n",
      "tensor([[0.0000, 0.1971, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.19708088 0.         0.        ]]\n",
      "[0.         0.19708088 0.         0.        ]\n",
      "tensor([[0.0000, 0.1466, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.14661011 0.         0.        ]]\n",
      "[0.         0.14661011 0.         0.        ]\n",
      "tensor([[0.0000, 0.0827, 0.0000, 0.0078]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.08273001 0.         0.00783677]]\n",
      "[0.         0.08273001 0.         0.00783677]\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0080]], grad_fn=<ReluBackward0>)\n",
      "[[0.        0.        0.        0.0079684]]\n",
      "[0.        0.        0.        0.0079684]\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0235]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.         0.         0.02353276]]\n",
      "[0.         0.         0.         0.02353276]\n",
      "tensor([[0., 0., 0., 0.]], grad_fn=<ReluBackward0>)\n",
      "[[0. 0. 0. 0.]]\n",
      "[0. 0. 0. 0.]\n",
      "tensor([[0.0000, 0.0372, 0.0478, 0.0139]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.03723376 0.04778714 0.01391461]]\n",
      "[0.         0.03723376 0.04778714 0.01391461]\n",
      "tensor([[0.0000, 0.0709, 0.0000, 0.1061]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.07085005 0.         0.10612401]]\n",
      "[0.         0.07085005 0.         0.10612401]\n",
      "tensor([[0.0000, 0.1609, 0.0791, 0.1695]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.1608567  0.07905486 0.16953105]]\n",
      "[0.         0.1608567  0.07905486 0.16953105]\n",
      "tensor([[0.0000, 0.2573, 0.0491, 0.1578]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.25725424 0.04909829 0.15775296]]\n",
      "[0.         0.25725424 0.04909829 0.15775296]\n",
      "tensor([[0.0000, 0.2049, 0.0676, 0.1360]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.20492065 0.06759001 0.13598737]]\n",
      "[0.         0.20492065 0.06759001 0.13598737]\n",
      "tensor([[0.0000, 0.2661, 0.0385, 0.1231]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.26609313 0.03852215 0.12312523]]\n",
      "[0.         0.26609313 0.03852215 0.12312523]\n",
      "tensor([[0.0000, 0.2571, 0.0057, 0.0285]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.25709093 0.00570527 0.0285029 ]]\n",
      "[0.         0.25709093 0.00570527 0.0285029 ]\n",
      "tensor([[0.0000, 0.2865, 0.0049, 0.0503]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.286465   0.00490398 0.05032995]]\n",
      "[0.         0.286465   0.00490398 0.05032995]\n",
      "tensor([[0.0278, 0.3030, 0.0681, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.02776927 0.3030074  0.06807563 0.        ]]\n",
      "[0.02776927 0.3030074  0.06807563 0.        ]\n",
      "tensor([[0.0074, 0.2606, 0.0000, 0.0356]], grad_fn=<ReluBackward0>)\n",
      "[[0.00738078 0.26058125 0.         0.03564865]]\n",
      "[0.00738078 0.26058125 0.         0.03564865]\n",
      "tensor([[0.0000, 0.3038, 0.0000, 0.0103]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.30378687 0.         0.01034747]]\n",
      "[0.         0.30378687 0.         0.01034747]\n",
      "tensor([[0.0000, 0.1148, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.11483829 0.         0.        ]]\n",
      "[0.         0.11483829 0.         0.        ]\n",
      "tensor([[0.0000, 0.1295, 0.0888, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.12954871 0.08884054 0.        ]]\n",
      "[0.         0.12954871 0.08884054 0.        ]\n",
      "tensor([[0.0000, 0.0007, 0.0644, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.00070249 0.0643835  0.        ]]\n",
      "[0.         0.00070249 0.0643835  0.        ]\n",
      "tensor([[0.0000, 0.0755, 0.0554, 0.0759]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.07548881 0.05542576 0.07589754]]\n",
      "[0.         0.07548881 0.05542576 0.07589754]\n",
      "tensor([[0.0000, 0.0262, 0.0000, 0.0791]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.02615391 0.         0.07910457]]\n",
      "[0.         0.02615391 0.         0.07910457]\n",
      "tensor([[0.0192, 0.0618, 0.0000, 0.0162]], grad_fn=<ReluBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01917358 0.06182375 0.         0.01619397]]\n",
      "[0.01917358 0.06182375 0.         0.01619397]\n",
      "tensor([[0.0000, 0.1261, 0.0000, 0.0550]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.12605864 0.         0.05504212]]\n",
      "[0.         0.12605864 0.         0.05504212]\n",
      "tensor([[0.0000, 0.1446, 0.0314, 0.0366]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.14460108 0.03135077 0.03662673]]\n",
      "[0.         0.14460108 0.03135077 0.03662673]\n",
      "tensor([[0.0000, 0.2069, 0.0298, 0.0682]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.20691708 0.02981114 0.06819324]]\n",
      "[0.         0.20691708 0.02981114 0.06819324]\n",
      "tensor([[0.0000, 0.1504, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.15035668 0.         0.        ]]\n",
      "[0.         0.15035668 0.         0.        ]\n",
      "tensor([[0.0006, 0.0929, 0.0000, 0.0224]], grad_fn=<ReluBackward0>)\n",
      "[[0.00058616 0.09289595 0.         0.02235676]]\n",
      "[0.00058616 0.09289595 0.         0.02235676]\n",
      "tensor([[0.0044, 0.0580, 0.0000, 0.0040]], grad_fn=<ReluBackward0>)\n",
      "[[0.00440502 0.05799954 0.         0.00399192]]\n",
      "[0.00440502 0.05799954 0.         0.00399192]\n",
      "tensor([[0., 0., 0., 0.]], grad_fn=<ReluBackward0>)\n",
      "[[0. 0. 0. 0.]]\n",
      "[0. 0. 0. 0.]\n",
      "tensor([[0.0000, 0.0802, 0.0000, 0.0444]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.08019121 0.         0.04443378]]\n",
      "[0.         0.08019121 0.         0.04443378]\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0656]], grad_fn=<ReluBackward0>)\n",
      "[[0.        0.        0.        0.0655508]]\n",
      "[0.        0.        0.        0.0655508]\n",
      "tensor([[0.0192, 0.0618, 0.0000, 0.0162]], grad_fn=<ReluBackward0>)\n",
      "[[0.01917358 0.06182375 0.         0.01619397]]\n",
      "[0.01917358 0.06182375 0.         0.01619397]\n",
      "tensor([[0., 0., 0., 0.]], grad_fn=<ReluBackward0>)\n",
      "[[0. 0. 0. 0.]]\n",
      "[0. 0. 0. 0.]\n",
      "tensor([[0.0000, 0.1094, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.10941058 0.         0.        ]]\n",
      "[0.         0.10941058 0.         0.        ]\n",
      "tensor([[0.0000, 0.1481, 0.0426, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.14807482 0.04256041 0.        ]]\n",
      "[0.         0.14807482 0.04256041 0.        ]\n",
      "tensor([[0.0000, 0.2503, 0.0292, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.2503491  0.02920121 0.        ]]\n",
      "[0.         0.2503491  0.02920121 0.        ]\n",
      "tensor([[0.0000, 0.1629, 0.0613, 0.0365]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.16285409 0.06133343 0.03647586]]\n",
      "[0.         0.16285409 0.06133343 0.03647586]\n",
      "tensor([[0.0000, 0.0684, 0.0000, 0.0081]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.06840751 0.         0.00805057]]\n",
      "[0.         0.06840751 0.         0.00805057]\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.1283]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.         0.         0.12830056]]\n",
      "[0.         0.         0.         0.12830056]\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.1884]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.         0.         0.18844466]]\n",
      "[0.         0.         0.         0.18844466]\n",
      "tensor([[0.0000, 0.0416, 0.0000, 0.0983]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.04163542 0.         0.09830789]]\n",
      "[0.         0.04163542 0.         0.09830789]\n",
      "tensor([[0.0000, 0.1825, 0.0187, 0.0638]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.18248641 0.01865558 0.06378964]]\n",
      "[0.         0.18248641 0.01865558 0.06378964]\n",
      "tensor([[0.0000, 0.0721, 0.0000, 0.1791]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.07212134 0.         0.179136  ]]\n",
      "[0.         0.07212134 0.         0.179136  ]\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.1750]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.         0.         0.17496715]]\n",
      "[0.         0.         0.         0.17496715]\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.1607]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.         0.         0.16074094]]\n",
      "[0.         0.         0.         0.16074094]\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0796]], grad_fn=<ReluBackward0>)\n",
      "[[0.        0.        0.        0.0795693]]\n",
      "[0.        0.        0.        0.0795693]\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.1283]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.         0.         0.12830056]]\n",
      "[0.         0.         0.         0.12830056]\n",
      "tensor([[0.0000, 0.0053, 0.0000, 0.0890]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.00532643 0.         0.08896604]]\n",
      "[0.         0.00532643 0.         0.08896604]\n",
      "tensor([[0.0000, 0.1141, 0.0000, 0.0364]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.11414303 0.         0.03637782]]\n",
      "[0.         0.11414303 0.         0.03637782]\n",
      "tensor([[0.0000, 0.1477, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.        0.1476976 0.        0.       ]]\n",
      "[0.        0.1476976 0.        0.       ]\n",
      "tensor([[0.0000, 0.0828, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.08275316 0.         0.        ]]\n",
      "[0.         0.08275316 0.         0.        ]\n",
      "tensor([[0.0000, 0.1141, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.11411046 0.         0.        ]]\n",
      "[0.         0.11411046 0.         0.        ]\n",
      "tensor([[0.0000, 0.0907, 0.0000, 0.0974]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.09072436 0.         0.09739858]]\n",
      "[0.         0.09072436 0.         0.09739858]\n",
      "tensor([[0.0000, 0.1929, 0.0000, 0.0478]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.1929463  0.         0.04784257]]\n",
      "[0.         0.1929463  0.         0.04784257]\n",
      "tensor([[0.0000, 0.2353, 0.0000, 0.0485]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.23532043 0.         0.04853612]]\n",
      "[0.         0.23532043 0.         0.04853612]\n",
      "tensor([[0.0000, 0.1434, 0.0062, 0.0289]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.14337648 0.00618527 0.02891392]]\n",
      "[0.         0.14337648 0.00618527 0.02891392]\n",
      "tensor([[0.0000, 0.1724, 0.0778, 0.0852]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.17241305 0.07778434 0.08520016]]\n",
      "[0.         0.17241305 0.07778434 0.08520016]\n",
      "tensor([[0.0000, 0.0773, 0.0000, 0.0300]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.07733488 0.         0.0299551 ]]\n",
      "[0.         0.07733488 0.         0.0299551 ]\n",
      "tensor([[0.0000, 0.0737, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.07374597 0.         0.        ]]\n",
      "[0.         0.07374597 0.         0.        ]\n",
      "tensor([[0., 0., 0., 0.]], grad_fn=<ReluBackward0>)\n",
      "[[0. 0. 0. 0.]]\n",
      "[0. 0. 0. 0.]\n",
      "tensor([[0.0000, 0.0423, 0.0000, 0.0714]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.04225583 0.         0.0714146 ]]\n",
      "[0.         0.04225583 0.         0.0714146 ]\n",
      "tensor([[0.0000, 0.0572, 0.0000, 0.0918]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.05718959 0.         0.09175086]]\n",
      "[0.         0.05718959 0.         0.09175086]\n",
      "tensor([[0.0000, 0.1179, 0.0000, 0.0502]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.11791313 0.         0.05022528]]\n",
      "[0.         0.11791313 0.         0.05022528]\n",
      "tensor([[0.0000, 0.1649, 0.0000, 0.0703]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.16486402 0.         0.0702938 ]]\n",
      "[0.         0.16486402 0.         0.0702938 ]\n",
      "tensor([[0.0000, 0.0629, 0.0000, 0.0924]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.06291988 0.         0.09236921]]\n",
      "[0.         0.06291988 0.         0.09236921]\n",
      "tensor([[0.0000, 0.0856, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.08559191 0.         0.        ]]\n",
      "[0.         0.08559191 0.         0.        ]\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0217]], grad_fn=<ReluBackward0>)\n",
      "[[0.        0.        0.        0.0216733]]\n",
      "[0.        0.        0.        0.0216733]\n",
      "tensor([[0.0622, 0.0684, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.06224299 0.06841887 0.         0.        ]]\n",
      "[0.06224299 0.06841887 0.         0.        ]\n",
      "tensor([[0.0000, 0.0176, 0.0000, 0.0188]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.01758089 0.         0.01875025]]\n",
      "[0.         0.01758089 0.         0.01875025]\n",
      "tensor([[0.0000, 0.1173, 0.0000, 0.0051]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.11725862 0.         0.00506304]]\n",
      "[0.         0.11725862 0.         0.00506304]\n",
      "tensor([[0.0000, 0.2101, 0.0000, 0.0281]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.21014509 0.         0.02806256]]\n",
      "[0.         0.21014509 0.         0.02806256]\n",
      "tensor([[0.0000, 0.2290, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.        0.2289634 0.        0.       ]]\n",
      "[0.        0.2289634 0.        0.       ]\n",
      "tensor([[0.0000, 0.1868, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.18675281 0.         0.        ]]\n",
      "[0.         0.18675281 0.         0.        ]\n",
      "tensor([[0.0000, 0.1152, 0.0000, 0.1038]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.11515798 0.         0.10375653]]\n",
      "[0.         0.11515798 0.         0.10375653]\n",
      "tensor([[0.0000, 0.0638, 0.0000, 0.0650]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.06378882 0.         0.06500685]]\n",
      "[0.         0.06378882 0.         0.06500685]\n",
      "tensor([[0.0000, 0.1538, 0.0000, 0.0530]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.15379116 0.         0.05303508]]\n",
      "[0.         0.15379116 0.         0.05303508]\n",
      "tensor([[0.0000, 0.1985, 0.0000, 0.0653]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.19846752 0.         0.06532428]]\n",
      "[0.         0.19846752 0.         0.06532428]\n",
      "tensor([[0.0000, 0.2742, 0.0533, 0.0026]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.27423793 0.053314   0.00256397]]\n",
      "[0.         0.27423793 0.053314   0.00256397]\n",
      "tensor([[0.0000, 0.2025, 0.0427, 0.0676]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.20249344 0.04270573 0.06761445]]\n",
      "[0.         0.20249344 0.04270573 0.06761445]\n",
      "tensor([[0.0000, 0.1516, 0.0325, 0.1039]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.1515895  0.0325348  0.10390022]]\n",
      "[0.         0.1515895  0.0325348  0.10390022]\n",
      "tensor([[0.0000, 0.0000, 0.0188, 0.0816]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.         0.01882746 0.08159154]]\n",
      "[0.         0.         0.01882746 0.08159154]\n",
      "tensor([[0.0000, 0.0814, 0.0192, 0.0022]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.08139339 0.01924165 0.00222075]]\n",
      "[0.         0.08139339 0.01924165 0.00222075]\n",
      "tensor([[0., 0., 0., 0.]], grad_fn=<ReluBackward0>)\n",
      "[[0. 0. 0. 0.]]\n",
      "[0. 0. 0. 0.]\n",
      "tensor([[0.0000, 0.1094, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.10941058 0.         0.        ]]\n",
      "[0.         0.10941058 0.         0.        ]\n",
      "tensor([[0.0000, 0.1481, 0.0426, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.14807482 0.04256041 0.        ]]\n",
      "[0.         0.14807482 0.04256041 0.        ]\n",
      "tensor([[0.0000, 0.2503, 0.0292, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.2503491  0.02920121 0.        ]]\n",
      "[0.         0.2503491  0.02920121 0.        ]\n",
      "tensor([[0.0000, 0.1629, 0.0613, 0.0365]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.16285409 0.06133343 0.03647586]]\n",
      "[0.         0.16285409 0.06133343 0.03647586]\n",
      "tensor([[0.0000, 0.0963, 0.0000, 0.0818]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.09630028 0.         0.08182248]]\n",
      "[0.         0.09630028 0.         0.08182248]\n",
      "tensor([[0.0000, 0.0445, 0.0000, 0.0864]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.04450838 0.         0.0864315 ]]\n",
      "[0.         0.04450838 0.         0.0864315 ]\n",
      "tensor([[0.0000, 0.0788, 0.0000, 0.0420]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.07884423 0.         0.04195315]]\n",
      "[0.         0.07884423 0.         0.04195315]\n",
      "tensor([[0.0000, 0.0884, 0.0000, 0.0159]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.08839169 0.         0.01589584]]\n",
      "[0.         0.08839169 0.         0.01589584]\n",
      "tensor([[0.0000, 0.0546, 0.0000, 0.0654]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.05459908 0.         0.06538527]]\n",
      "[0.         0.05459908 0.         0.06538527]\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.1848]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.         0.         0.18479909]]\n",
      "[0.         0.         0.         0.18479909]\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.1933]], grad_fn=<ReluBackward0>)\n",
      "[[0.        0.        0.        0.1932949]]\n",
      "[0.        0.        0.        0.1932949]\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0806]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.         0.         0.08058856]]\n",
      "[0.         0.         0.         0.08058856]\n",
      "tensor([[0.0000, 0.1849, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.18485455 0.         0.        ]]\n",
      "[0.         0.18485455 0.         0.        ]\n",
      "tensor([[0.0000, 0.1213, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.12127797 0.         0.        ]]\n",
      "[0.         0.12127797 0.         0.        ]\n",
      "tensor([[0.0000, 0.1674, 0.0000, 0.0179]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.16737354 0.         0.01793836]]\n",
      "[0.         0.16737354 0.         0.01793836]\n",
      "tensor([[0.0000, 0.1281, 0.0000, 0.0371]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.12810451 0.         0.03706291]]\n",
      "[0.         0.12810451 0.         0.03706291]\n",
      "tensor([[0.0000, 0.1882, 0.0000, 0.0562]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.18818964 0.         0.05624595]]\n",
      "[0.         0.18818964 0.         0.05624595]\n",
      "tensor([[0.0277, 0.1153, 0.0000, 0.0275]], grad_fn=<ReluBackward0>)\n",
      "[[0.0277198  0.11526258 0.         0.02753945]]\n",
      "[0.0277198  0.11526258 0.         0.02753945]\n",
      "tensor([[0.0587, 0.2661, 0.0000, 0.0520]], grad_fn=<ReluBackward0>)\n",
      "[[0.05873334 0.26612923 0.         0.05203767]]\n",
      "[0.05873334 0.26612923 0.         0.05203767]\n",
      "tensor([[0.0234, 0.0466, 0.0000, 0.0063]], grad_fn=<ReluBackward0>)\n",
      "[[0.02338251 0.04661854 0.         0.00633941]]\n",
      "[0.02338251 0.04661854 0.         0.00633941]\n",
      "tensor([[0.0000, 0.0414, 0.0594, 0.0630]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.04137296 0.05943652 0.06295075]]\n",
      "[0.         0.04137296 0.05943652 0.06295075]\n",
      "tensor([[0.0670, 0.1076, 0.0467, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.06699885 0.10758105 0.04667553 0.        ]]\n",
      "[0.06699885 0.10758105 0.04667553 0.        ]\n",
      "tensor([[0.0000, 0.1878, 0.0000, 0.0115]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.1878101  0.         0.01153236]]\n",
      "[0.         0.1878101  0.         0.01153236]\n",
      "tensor([[0.0160, 0.2370, 0.0000, 0.0016]], grad_fn=<ReluBackward0>)\n",
      "[[0.01596744 0.23704676 0.         0.00164139]]\n",
      "[0.01596744 0.23704676 0.         0.00164139]\n",
      "tensor([[0.0000, 0.0997, 0.0000, 0.0646]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.09973341 0.         0.06463468]]\n",
      "[0.         0.09973341 0.         0.06463468]\n",
      "tensor([[0.1212, 0.0829, 0.0000, 0.0101]], grad_fn=<ReluBackward0>)\n",
      "[[0.12118189 0.08286488 0.         0.01008306]]\n",
      "[0.12118189 0.08286488 0.         0.01008306]\n",
      "tensor([[0.0768, 0.1087, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.07677743 0.1086619  0.         0.        ]]\n",
      "[0.07677743 0.1086619  0.         0.        ]\n",
      "tensor([[0.0587, 0.0984, 0.0000, 0.0064]], grad_fn=<ReluBackward0>)\n",
      "[[0.05867723 0.09839331 0.         0.00639746]]\n",
      "[0.05867723 0.09839331 0.         0.00639746]\n",
      "tensor([[0.0278, 0.0000, 0.0223, 0.0930]], grad_fn=<ReluBackward0>)\n",
      "[[0.0277868  0.         0.02228943 0.09298813]]\n",
      "[0.0277868  0.         0.02228943 0.09298813]\n",
      "tensor([[0.0000, 0.0857, 0.0097, 0.0226]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.08572347 0.0097493  0.02264137]]\n",
      "[0.         0.08572347 0.0097493  0.02264137]\n",
      "tensor([[0.0000, 0.1155, 0.0531, 0.0830]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.11549021 0.05306691 0.08300772]]\n",
      "[0.         0.11549021 0.05306691 0.08300772]\n",
      "tensor([[0.0000, 0.1977, 0.1192, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.1976887  0.11920122 0.        ]]\n",
      "[0.         0.1976887  0.11920122 0.        ]\n",
      "tensor([[0.0000, 0.1492, 0.0136, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.1492184  0.01364226 0.        ]]\n",
      "[0.         0.1492184  0.01364226 0.        ]\n",
      "tensor([[0.0161, 0.2101, 0.1656, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.01607512 0.21011487 0.16560563 0.        ]]\n",
      "[0.01607512 0.21011487 0.16560563 0.        ]\n",
      "tensor([[0.0057, 0.2135, 0.1058, 0.0086]], grad_fn=<ReluBackward0>)\n",
      "[[0.00566329 0.21352118 0.10584246 0.00860189]]\n",
      "[0.00566329 0.21352118 0.10584246 0.00860189]\n",
      "tensor([[0.0368, 0.1792, 0.0508, 0.0754]], grad_fn=<ReluBackward0>)\n",
      "[[0.03682717 0.17918938 0.05082879 0.07543875]]\n",
      "[0.03682717 0.17918938 0.05082879 0.07543875]\n",
      "tensor([[0.0525, 0.1702, 0.0000, 0.0972]], grad_fn=<ReluBackward0>)\n",
      "[[0.05251906 0.17018217 0.         0.09719954]]\n",
      "[0.05251906 0.17018217 0.         0.09719954]\n",
      "tensor([[0.0248, 0.2173, 0.0000, 0.1311]], grad_fn=<ReluBackward0>)\n",
      "[[0.02479497 0.21727581 0.         0.13105479]]\n",
      "[0.02479497 0.21727581 0.         0.13105479]\n",
      "tensor([[0.0000, 0.2472, 0.0175, 0.0960]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.24716339 0.01748699 0.09595349]]\n",
      "[0.         0.24716339 0.01748699 0.09595349]\n",
      "tensor([[0.0000, 0.0909, 0.0000, 0.1323]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.09086415 0.         0.13230519]]\n",
      "[0.         0.09086415 0.         0.13230519]\n",
      "tensor([[0.0000, 0.1709, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.17092147 0.         0.        ]]\n",
      "[0.         0.17092147 0.         0.        ]\n",
      "tensor([[0.0000, 0.0414, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.04141394 0.         0.        ]]\n",
      "[0.         0.04141394 0.         0.        ]\n",
      "tensor([[0.0000, 0.1667, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.16665994 0.         0.        ]]\n",
      "[0.         0.16665994 0.         0.        ]\n",
      "tensor([[0.0000, 0.0948, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.09477634 0.         0.        ]]\n",
      "[0.         0.09477634 0.         0.        ]\n",
      "tensor([[0.0000, 0.1190, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.11897007 0.         0.        ]]\n",
      "[0.         0.11897007 0.         0.        ]\n",
      "tensor([[0.0000, 0.0924, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.09244737 0.         0.        ]]\n",
      "[0.         0.09244737 0.         0.        ]\n",
      "tensor([[0.0000, 0.1892, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.18921515 0.         0.        ]]\n",
      "[0.         0.18921515 0.         0.        ]\n",
      "tensor([[0.0000, 0.1128, 0.0155, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.11284406 0.01547509 0.        ]]\n",
      "[0.         0.11284406 0.01547509 0.        ]\n",
      "tensor([[0.0525, 0.1544, 0.0268, 0.0353]], grad_fn=<ReluBackward0>)\n",
      "[[0.05253708 0.15439373 0.02679469 0.03528716]]\n",
      "[0.05253708 0.15439373 0.02679469 0.03528716]\n",
      "tensor([[0.0000, 0.1242, 0.0327, 0.0158]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.12421632 0.03265608 0.01578167]]\n",
      "[0.         0.12421632 0.03265608 0.01578167]\n",
      "tensor([[0.0150, 0.1328, 0.0000, 0.0004]], grad_fn=<ReluBackward0>)\n",
      "[[0.01495722 0.1328104  0.         0.00037378]]\n",
      "[0.01495722 0.1328104  0.         0.00037378]\n",
      "tensor([[0.0000, 0.1094, 0.0000, 0.0675]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.10935066 0.         0.0675355 ]]\n",
      "[0.         0.10935066 0.         0.0675355 ]\n",
      "tensor([[0.0000, 0.0907, 0.0000, 0.0760]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.09074556 0.         0.07599095]]\n",
      "[0.         0.09074556 0.         0.07599095]\n",
      "tensor([[0.0000, 0.1929, 0.0000, 0.0478]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.1929463  0.         0.04784257]]\n",
      "[0.         0.1929463  0.         0.04784257]\n",
      "tensor([[0.0000, 0.2353, 0.0000, 0.0485]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.23532043 0.         0.04853612]]\n",
      "[0.         0.23532043 0.         0.04853612]\n",
      "tensor([[0.0000, 0.1101, 0.0320, 0.0274]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.11008468 0.03197414 0.02743647]]\n",
      "[0.         0.11008468 0.03197414 0.02743647]\n",
      "tensor([[0.0000, 0.1291, 0.0000, 0.0793]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.12906082 0.         0.07929557]]\n",
      "[0.         0.12906082 0.         0.07929557]\n",
      "tensor([[0.0000, 0.0498, 0.0000, 0.0814]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.04984729 0.         0.08137863]]\n",
      "[0.         0.04984729 0.         0.08137863]\n",
      "tensor([[0.0000, 0.1062, 0.0000, 0.0570]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.10618362 0.         0.05697079]]\n",
      "[0.         0.10618362 0.         0.05697079]\n",
      "tensor([[0.0000, 0.1222, 0.0714, 0.0953]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.12218706 0.07136671 0.09528121]]\n",
      "[0.         0.12218706 0.07136671 0.09528121]\n",
      "tensor([[0.0000, 0.1033, 0.0693, 0.0175]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.1033452  0.06928148 0.01754199]]\n",
      "[0.         0.1033452  0.06928148 0.01754199]\n",
      "tensor([[0.0000, 0.0000, 0.0493, 0.0631]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.         0.04932655 0.06311657]]\n",
      "[0.         0.         0.04932655 0.06311657]\n",
      "tensor([[0.0000, 0.0329, 0.0000, 0.0864]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.03287223 0.         0.08640875]]\n",
      "[0.         0.03287223 0.         0.08640875]\n",
      "tensor([[0.0000, 0.0429, 0.0000, 0.0006]], grad_fn=<ReluBackward0>)\n",
      "[[0.        0.0429259 0.        0.0005588]]\n",
      "[0.        0.0429259 0.        0.0005588]\n",
      "tensor([[0.0000, 0.0337, 0.0000, 0.0974]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.03369321 0.         0.09736007]]\n",
      "[0.         0.03369321 0.         0.09736007]\n",
      "tensor([[0.0000, 0.0194, 0.0000, 0.0244]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.01938887 0.         0.024438  ]]\n",
      "[0.         0.01938887 0.         0.024438  ]\n",
      "tensor([[0.0000, 0.0639, 0.0540, 0.0155]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.0638938  0.05397435 0.01546979]]\n",
      "[0.         0.0638938  0.05397435 0.01546979]\n",
      "tensor([[0.0291, 0.1241, 0.0863, 0.0604]], grad_fn=<ReluBackward0>)\n",
      "[[0.02909611 0.12407963 0.08632775 0.06035313]]\n",
      "[0.02909611 0.12407963 0.08632775 0.06035313]\n",
      "tensor([[0.0222, 0.1153, 0.1295, 0.0544]], grad_fn=<ReluBackward0>)\n",
      "[[0.02224193 0.11532187 0.12951979 0.05441962]]\n",
      "[0.02224193 0.11532187 0.12951979 0.05441962]\n",
      "tensor([[0.1090, 0.1544, 0.0625, 0.1396]], grad_fn=<ReluBackward0>)\n",
      "[[0.10903628 0.15442504 0.06245711 0.13959011]]\n",
      "[0.10903628 0.15442504 0.06245711 0.13959011]\n",
      "tensor([[0.0000, 0.1096, 0.0010, 0.0444]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.1096469  0.00103454 0.04443535]]\n",
      "[0.         0.1096469  0.00103454 0.04443535]\n",
      "tensor([[0.0000, 0.1133, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.11325162 0.         0.        ]]\n",
      "[0.         0.11325162 0.         0.        ]\n",
      "tensor([[0.0000, 0.0947, 0.0000, 0.0359]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.09466339 0.         0.03592127]]\n",
      "[0.         0.09466339 0.         0.03592127]\n",
      "tensor([[0.0000, 0.1407, 0.0742, 0.0654]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.14065772 0.07416865 0.06542892]]\n",
      "[0.         0.14065772 0.07416865 0.06542892]\n",
      "tensor([[0.0000, 0.1845, 0.0564, 0.0670]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.18447866 0.05638693 0.06703527]]\n",
      "[0.         0.18447866 0.05638693 0.06703527]\n",
      "tensor([[0.0000, 0.1282, 0.0095, 0.0331]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.12816872 0.00950097 0.03312548]]\n",
      "[0.         0.12816872 0.00950097 0.03312548]\n",
      "tensor([[0.0000, 0.1476, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.14758551 0.         0.        ]]\n",
      "[0.         0.14758551 0.         0.        ]\n",
      "tensor([[0.0006, 0.0929, 0.0000, 0.0224]], grad_fn=<ReluBackward0>)\n",
      "[[0.00058616 0.09289595 0.         0.02235676]]\n",
      "[0.00058616 0.09289595 0.         0.02235676]\n",
      "tensor([[0.0000, 0.1816, 0.0000, 0.0613]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.18155709 0.         0.06131573]]\n",
      "[0.         0.18155709 0.         0.06131573]\n",
      "tensor([[0.0000, 0.1172, 0.0000, 0.0328]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.11724253 0.         0.03278182]]\n",
      "[0.         0.11724253 0.         0.03278182]\n",
      "tensor([[0.0000, 0.2353, 0.0000, 0.0485]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.23532043 0.         0.04853612]]\n",
      "[0.         0.23532043 0.         0.04853612]\n",
      "tensor([[0.0000, 0.1883, 0.0000, 0.0429]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.18834153 0.         0.04292838]]\n",
      "[0.         0.18834153 0.         0.04292838]\n",
      "tensor([[0.0000, 0.0876, 0.0000, 0.0081]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.08761217 0.         0.00812007]]\n",
      "[0.         0.08761217 0.         0.00812007]\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0402]], grad_fn=<ReluBackward0>)\n",
      "[[0.        0.        0.        0.0401945]]\n",
      "[0.        0.        0.        0.0401945]\n",
      "tensor([[0.1031, 0.0608, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.10313644 0.06081723 0.         0.        ]]\n",
      "[0.10313644 0.06081723 0.         0.        ]\n",
      "tensor([[0.0052, 0.0270, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.00523471 0.02702122 0.         0.        ]]\n",
      "[0.00523471 0.02702122 0.         0.        ]\n",
      "tensor([[0.0000, 0.1460, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.14599684 0.         0.        ]]\n",
      "[0.         0.14599684 0.         0.        ]\n",
      "tensor([[0.0000, 0.1517, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.15166274 0.         0.        ]]\n",
      "[0.         0.15166274 0.         0.        ]\n",
      "tensor([[0.0000, 0.0986, 0.0086, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.09855697 0.00855603 0.        ]]\n",
      "[0.         0.09855697 0.00855603 0.        ]\n",
      "tensor([[0.0000, 0.1716, 0.0000, 0.0557]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.17164391 0.         0.05572031]]\n",
      "[0.         0.17164391 0.         0.05572031]\n",
      "tensor([[0.0000, 0.0963, 0.0000, 0.0587]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.09628946 0.         0.05873013]]\n",
      "[0.         0.09628946 0.         0.05873013]\n",
      "tensor([[0.0000, 0.1385, 0.0000, 0.0389]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.13852799 0.         0.03889624]]\n",
      "[0.         0.13852799 0.         0.03889624]\n",
      "tensor([[0.0000, 0.1466, 0.0793, 0.0422]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.1466484  0.07930616 0.04220527]]\n",
      "[0.         0.1466484  0.07930616 0.04220527]\n",
      "tensor([[0.0000, 0.1736, 0.0339, 0.0106]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.1735949  0.03391574 0.0105574 ]]\n",
      "[0.         0.1735949  0.03391574 0.0105574 ]\n",
      "tensor([[0.0000, 0.1237, 0.0709, 0.0468]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.12365311 0.07089644 0.04683214]]\n",
      "[0.         0.12365311 0.07089644 0.04683214]\n",
      "tensor([[0.0000, 0.2068, 0.0000, 0.0250]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.20677021 0.         0.02503036]]\n",
      "[0.         0.20677021 0.         0.02503036]\n",
      "tensor([[0.0000, 0.1212, 0.0092, 0.0177]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.12119515 0.00916211 0.0176823 ]]\n",
      "[0.         0.12119515 0.00916211 0.0176823 ]\n",
      "tensor([[0.0000, 0.1310, 0.0239, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.1309647  0.02386848 0.        ]]\n",
      "[0.         0.1309647  0.02386848 0.        ]\n",
      "tensor([[0.0000, 0.0000, 0.0823, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.         0.08233534 0.        ]]\n",
      "[0.         0.         0.08233534 0.        ]\n",
      "tensor([[0., 0., 0., 0.]], grad_fn=<ReluBackward0>)\n",
      "[[0. 0. 0. 0.]]\n",
      "[0. 0. 0. 0.]\n",
      "tensor([[0.0219, 0.1320, 0.0104, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.02186663 0.13204135 0.01044869 0.        ]]\n",
      "[0.02186663 0.13204135 0.01044869 0.        ]\n",
      "tensor([[0.1240, 0.1585, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.12404907 0.15847746 0.         0.        ]]\n",
      "[0.12404907 0.15847746 0.         0.        ]\n",
      "tensor([[0.0150, 0.1025, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.014993   0.10252631 0.         0.        ]]\n",
      "[0.014993   0.10252631 0.         0.        ]\n",
      "tensor([[0.0000, 0.1460, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.14599684 0.         0.        ]]\n",
      "[0.         0.14599684 0.         0.        ]\n",
      "tensor([[0.0000, 0.1331, 0.0000, 0.0275]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.13313894 0.         0.02748115]]\n",
      "[0.         0.13313894 0.         0.02748115]\n",
      "tensor([[0.0000, 0.0871, 0.0000, 0.0037]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.08710337 0.         0.00368444]]\n",
      "[0.         0.08710337 0.         0.00368444]\n",
      "tensor([[0.0681, 0.0123, 0.0000, 0.0221]], grad_fn=<ReluBackward0>)\n",
      "[[0.06810705 0.01234533 0.         0.02213118]]\n",
      "[0.06810705 0.01234533 0.         0.02213118]\n",
      "tensor([[0.0169, 0.0000, 0.0409, 0.0742]], grad_fn=<ReluBackward0>)\n",
      "[[0.01685066 0.         0.04088873 0.07415481]]\n",
      "[0.01685066 0.         0.04088873 0.07415481]\n",
      "tensor([[0.0000, 0.0620, 0.0821, 0.0780]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.06199175 0.08209962 0.0780493 ]]\n",
      "[0.         0.06199175 0.08209962 0.0780493 ]\n",
      "tensor([[0.0000, 0.1100, 0.0542, 0.0808]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.10998893 0.05418788 0.08079021]]\n",
      "[0.         0.10998893 0.05418788 0.08079021]\n",
      "tensor([[0.0000, 0.0963, 0.0000, 0.0818]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.09630028 0.         0.08182248]]\n",
      "[0.         0.09630028 0.         0.08182248]\n",
      "tensor([[0.0000, 0.0445, 0.0000, 0.0864]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.04450838 0.         0.0864315 ]]\n",
      "[0.         0.04450838 0.         0.0864315 ]\n",
      "tensor([[0.0000, 0.0788, 0.0000, 0.0420]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.07884423 0.         0.04195315]]\n",
      "[0.         0.07884423 0.         0.04195315]\n",
      "tensor([[0.0000, 0.1494, 0.0040, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.14944972 0.00396259 0.        ]]\n",
      "[0.         0.14944972 0.00396259 0.        ]\n",
      "tensor([[0.0000, 0.1155, 0.0181, 0.0816]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.11554924 0.01813106 0.0816447 ]]\n",
      "[0.         0.11554924 0.01813106 0.0816447 ]\n",
      "tensor([[0.0000, 0.0773, 0.0000, 0.0300]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.07733488 0.         0.0299551 ]]\n",
      "[0.         0.07733488 0.         0.0299551 ]\n",
      "tensor([[0.0000, 0.2236, 0.0000, 0.0272]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.22360775 0.         0.02724909]]\n",
      "[0.         0.22360775 0.         0.02724909]\n",
      "tensor([[0.0000, 0.1710, 0.0032, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.17104934 0.00322045 0.        ]]\n",
      "[0.         0.17104934 0.00322045 0.        ]\n",
      "tensor([[0.0000, 0.1962, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.19623566 0.         0.        ]]\n",
      "[0.         0.19623566 0.         0.        ]\n",
      "tensor([[0.0000, 0.1799, 0.0861, 0.0435]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.17988431 0.08614394 0.0434989 ]]\n",
      "[0.         0.17988431 0.08614394 0.0434989 ]\n",
      "tensor([[0.0000, 0.1291, 0.0000, 0.0793]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.12906082 0.         0.07929557]]\n",
      "[0.         0.12906082 0.         0.07929557]\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.1616]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.         0.         0.16159539]]\n",
      "[0.         0.         0.         0.16159539]\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.2034]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.         0.         0.20342685]]\n",
      "[0.         0.         0.         0.20342685]\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.1480]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.         0.         0.14801836]]\n",
      "[0.         0.         0.         0.14801836]\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0120]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.         0.         0.01201909]]\n",
      "[0.         0.         0.         0.01201909]\n",
      "tensor([[0.0000, 0.0695, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.06952802 0.         0.        ]]\n",
      "[0.         0.06952802 0.         0.        ]\n",
      "tensor([[0.0000, 0.0427, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.04274008 0.         0.        ]]\n",
      "[0.         0.04274008 0.         0.        ]\n",
      "tensor([[0.0000, 0.1778, 0.0117, 0.1259]], grad_fn=<ReluBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.17775461 0.01167174 0.12592685]]\n",
      "[0.         0.17775461 0.01167174 0.12592685]\n",
      "tensor([[0.0000, 0.1149, 0.0000, 0.1544]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.11492743 0.         0.15440339]]\n",
      "[0.         0.11492743 0.         0.15440339]\n",
      "tensor([[0.0000, 0.0999, 0.0000, 0.0570]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.09990527 0.         0.05696299]]\n",
      "[0.         0.09990527 0.         0.05696299]\n",
      "tensor([[0.0000, 0.1602, 0.0000, 0.1174]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.16024166 0.         0.11744191]]\n",
      "[0.         0.16024166 0.         0.11744191]\n",
      "tensor([[0.0000, 0.1498, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.        0.1497837 0.        0.       ]]\n",
      "[0.        0.1497837 0.        0.       ]\n",
      "tensor([[0.0000, 0.1739, 0.0057, 0.0211]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.17394528 0.00569118 0.02105635]]\n",
      "[0.         0.17394528 0.00569118 0.02105635]\n",
      "tensor([[0.0000, 0.1956, 0.0803, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.19564575 0.08025876 0.        ]]\n",
      "[0.         0.19564575 0.08025876 0.        ]\n",
      "tensor([[0.0000, 0.1941, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.        0.1941241 0.        0.       ]]\n",
      "[0.        0.1941241 0.        0.       ]\n",
      "tensor([[0.0000, 0.1478, 0.0881, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.1478152  0.08811095 0.        ]]\n",
      "[0.         0.1478152  0.08811095 0.        ]\n",
      "tensor([[0.0000, 0.1737, 0.0645, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.17373821 0.06454385 0.        ]]\n",
      "[0.         0.17373821 0.06454385 0.        ]\n",
      "tensor([[0.0000, 0.1333, 0.1160, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.13333964 0.11601606 0.        ]]\n",
      "[0.         0.13333964 0.11601606 0.        ]\n",
      "tensor([[0.0000, 0.0928, 0.0994, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.09276195 0.0993799  0.        ]]\n",
      "[0.         0.09276195 0.0993799  0.        ]\n",
      "tensor([[0.0000, 0.1006, 0.1003, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.1005844  0.10030064 0.        ]]\n",
      "[0.         0.1005844  0.10030064 0.        ]\n",
      "tensor([[0.0267, 0.1265, 0.0444, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.02673576 0.12646565 0.04444426 0.        ]]\n",
      "[0.02673576 0.12646565 0.04444426 0.        ]\n",
      "tensor([[0.0000, 0.0406, 0.0000, 0.0649]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.04060283 0.         0.06490551]]\n",
      "[0.         0.04060283 0.         0.06490551]\n",
      "tensor([[0.0035, 0.0322, 0.0000, 0.0770]], grad_fn=<ReluBackward0>)\n",
      "[[0.00352436 0.03217928 0.         0.07697223]]\n",
      "[0.00352436 0.03217928 0.         0.07697223]\n",
      "tensor([[0.0000, 0.1638, 0.0906, 0.0768]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.16377974 0.0906455  0.07684952]]\n",
      "[0.         0.16377974 0.0906455  0.07684952]\n",
      "tensor([[0.0123, 0.1463, 0.0364, 0.0960]], grad_fn=<ReluBackward0>)\n",
      "[[0.01234639 0.14634892 0.03644642 0.09601263]]\n",
      "[0.01234639 0.14634892 0.03644642 0.09601263]\n",
      "tensor([[0.0000, 0.1515, 0.0000, 0.1272]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.15147528 0.         0.12721708]]\n",
      "[0.         0.15147528 0.         0.12721708]\n",
      "tensor([[0.0000, 0.2093, 0.0015, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.2093195  0.00150121 0.        ]]\n",
      "[0.         0.2093195  0.00150121 0.        ]\n",
      "tensor([[0.0000, 0.1039, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.10394137 0.         0.        ]]\n",
      "[0.         0.10394137 0.         0.        ]\n",
      "tensor([[0.0292, 0.0995, 0.0000, 0.0122]], grad_fn=<ReluBackward0>)\n",
      "[[0.02921754 0.09951992 0.         0.0121833 ]]\n",
      "[0.02921754 0.09951992 0.         0.0121833 ]\n",
      "tensor([[0.0000, 0.2044, 0.0000, 0.0470]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.20438324 0.         0.04701156]]\n",
      "[0.         0.20438324 0.         0.04701156]\n",
      "tensor([[0.0000, 0.1127, 0.0000, 0.1115]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.11269252 0.         0.11148235]]\n",
      "[0.         0.11269252 0.         0.11148235]\n",
      "tensor([[0.0000, 0.2252, 0.0147, 0.1216]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.22518381 0.01467348 0.12156872]]\n",
      "[0.         0.22518381 0.01467348 0.12156872]\n",
      "tensor([[0.0000, 0.1906, 0.0000, 0.1722]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.19061533 0.         0.17223747]]\n",
      "[0.         0.19061533 0.         0.17223747]\n",
      "tensor([[0.0000, 0.1623, 0.0914, 0.0883]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.16234724 0.0913793  0.08832596]]\n",
      "[0.         0.16234724 0.0913793  0.08832596]\n",
      "tensor([[0.0000, 0.1662, 0.0000, 0.0734]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.16615681 0.         0.0734054 ]]\n",
      "[0.         0.16615681 0.         0.0734054 ]\n",
      "tensor([[0.0000, 0.1743, 0.0000, 0.0679]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.17430498 0.         0.06786028]]\n",
      "[0.         0.17430498 0.         0.06786028]\n",
      "tensor([[0.0000, 0.0846, 0.0000, 0.1282]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.08456287 0.         0.12819862]]\n",
      "[0.         0.08456287 0.         0.12819862]\n",
      "tensor([[0.0000, 0.0787, 0.0000, 0.0528]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.0786508  0.         0.05280718]]\n",
      "[0.         0.0786508  0.         0.05280718]\n",
      "tensor([[0.0000, 0.0400, 0.0000, 0.0931]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.03997577 0.         0.09314993]]\n",
      "[0.         0.03997577 0.         0.09314993]\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0457]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.         0.         0.04570416]]\n",
      "[0.         0.         0.         0.04570416]\n",
      "tensor([[0.0000, 0.0177, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.01772638 0.         0.        ]]\n",
      "[0.         0.01772638 0.         0.        ]\n",
      "tensor([[0.0000, 0.0273, 0.0000, 0.0183]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.02728129 0.         0.01831611]]\n",
      "[0.         0.02728129 0.         0.01831611]\n",
      "tensor([[0.0000e+00, 5.0493e-05, 0.0000e+00, 0.0000e+00]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "[[0.0000000e+00 5.0492585e-05 0.0000000e+00 0.0000000e+00]]\n",
      "[0.0000000e+00 5.0492585e-05 0.0000000e+00 0.0000000e+00]\n",
      "tensor([[0.0000, 0.0706, 0.0000, 0.0691]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.07057361 0.         0.06913605]]\n",
      "[0.         0.07057361 0.         0.06913605]\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0867]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.         0.         0.08666223]]\n",
      "[0.         0.         0.         0.08666223]\n",
      "tensor([[0.0000, 0.0413, 0.0000, 0.0922]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.0412797  0.         0.09224771]]\n",
      "[0.         0.0412797  0.         0.09224771]\n",
      "tensor([[0.0000, 0.0564, 0.0000, 0.0664]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.05635002 0.         0.06636859]]\n",
      "[0.         0.05635002 0.         0.06636859]\n",
      "tensor([[0.0000, 0.1404, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.14040288 0.         0.        ]]\n",
      "[0.         0.14040288 0.         0.        ]\n",
      "tensor([[0.0000, 0.0985, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.09848946 0.         0.        ]]\n",
      "[0.         0.09848946 0.         0.        ]\n",
      "tensor([[0.0000, 0.1116, 0.0000, 0.0330]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.11158671 0.         0.0330225 ]]\n",
      "[0.         0.11158671 0.         0.0330225 ]\n",
      "tensor([[0.0146, 0.1681, 0.0509, 0.0694]], grad_fn=<ReluBackward0>)\n",
      "[[0.01464112 0.16814029 0.05089664 0.06936288]]\n",
      "[0.01464112 0.16814029 0.05089664 0.06936288]\n",
      "tensor([[0.0000, 0.1553, 0.0000, 0.0087]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.1553388  0.         0.00868137]]\n",
      "[0.         0.1553388  0.         0.00868137]\n",
      "tensor([[0.0554, 0.2493, 0.1143, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.05535892 0.2493448  0.11426955 0.        ]]\n",
      "[0.05535892 0.2493448  0.11426955 0.        ]\n",
      "tensor([[0.0000, 0.3082, 0.1088, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.30816278 0.10884834 0.        ]]\n",
      "[0.         0.30816278 0.10884834 0.        ]\n",
      "tensor([[0.0000, 0.2772, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.27718914 0.         0.        ]]\n",
      "[0.         0.27718914 0.         0.        ]\n",
      "tensor([[0.0000, 0.1768, 0.0956, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.17678744 0.09564695 0.        ]]\n",
      "[0.         0.17678744 0.09564695 0.        ]\n",
      "tensor([[0.0000, 0.1113, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.11133547 0.         0.        ]]\n",
      "[0.         0.11133547 0.         0.        ]\n",
      "tensor([[0.0000, 0.1816, 0.0000, 0.0669]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.181589   0.         0.06692234]]\n",
      "[0.         0.181589   0.         0.06692234]\n",
      "tensor([[0.0000, 0.1949, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.19489506 0.         0.        ]]\n",
      "[0.         0.19489506 0.         0.        ]\n",
      "tensor([[0.0000, 0.1184, 0.0000, 0.0218]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.11844029 0.         0.0217513 ]]\n",
      "[0.         0.11844029 0.         0.0217513 ]\n",
      "tensor([[0.0000, 0.1532, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.        0.1532268 0.        0.       ]]\n",
      "[0.        0.1532268 0.        0.       ]\n",
      "tensor([[0.0000, 0.1307, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.13067472 0.         0.        ]]\n",
      "[0.         0.13067472 0.         0.        ]\n",
      "tensor([[0.0000, 0.0581, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.        0.0581279 0.        0.       ]]\n",
      "[0.        0.0581279 0.        0.       ]\n",
      "tensor([[0.0000, 0.1521, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.15208891 0.         0.        ]]\n",
      "[0.         0.15208891 0.         0.        ]\n",
      "tensor([[0.0000, 0.0985, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.09848946 0.         0.        ]]\n",
      "[0.         0.09848946 0.         0.        ]\n",
      "tensor([[0.0000, 0.1116, 0.0000, 0.0330]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.11158671 0.         0.0330225 ]]\n",
      "[0.         0.11158671 0.         0.0330225 ]\n",
      "tensor([[0.0146, 0.1681, 0.0509, 0.0694]], grad_fn=<ReluBackward0>)\n",
      "[[0.01464112 0.16814029 0.05089664 0.06936288]]\n",
      "[0.01464112 0.16814029 0.05089664 0.06936288]\n",
      "tensor([[0.0123, 0.1463, 0.0364, 0.0960]], grad_fn=<ReluBackward0>)\n",
      "[[0.01234639 0.14634892 0.03644642 0.09601263]]\n",
      "[0.01234639 0.14634892 0.03644642 0.09601263]\n",
      "tensor([[0.0110, 0.1996, 0.0230, 0.1332]], grad_fn=<ReluBackward0>)\n",
      "[[0.01096734 0.19964369 0.02302416 0.13318148]]\n",
      "[0.01096734 0.19964369 0.02302416 0.13318148]\n",
      "tensor([[0.0399, 0.2448, 0.0000, 0.1881]], grad_fn=<ReluBackward0>)\n",
      "[[0.03988662 0.2448057  0.         0.18814637]]\n",
      "[0.03988662 0.2448057  0.         0.18814637]\n",
      "tensor([[0.0000, 0.2115, 0.0063, 0.1393]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.21154013 0.00628878 0.13934514]]\n",
      "[0.         0.21154013 0.00628878 0.13934514]\n",
      "tensor([[0.0000, 0.0722, 0.0000, 0.0566]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.07216173 0.         0.0566167 ]]\n",
      "[0.         0.07216173 0.         0.0566167 ]\n",
      "tensor([[0.0000, 0.1296, 0.0000, 0.0619]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.12957357 0.         0.06186845]]\n",
      "[0.         0.12957357 0.         0.06186845]\n",
      "tensor([[0.0476, 0.1225, 0.0965, 0.0755]], grad_fn=<ReluBackward0>)\n",
      "[[0.04758809 0.12250091 0.0964618  0.07552267]]\n",
      "[0.04758809 0.12250091 0.0964618  0.07552267]\n",
      "tensor([[0.0557, 0.2080, 0.0960, 0.0908]], grad_fn=<ReluBackward0>)\n",
      "[[0.05573334 0.20795351 0.09596614 0.09083608]]\n",
      "[0.05573334 0.20795351 0.09596614 0.09083608]\n",
      "tensor([[0.0000, 0.2514, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.        0.2514331 0.        0.       ]]\n",
      "[0.        0.2514331 0.        0.       ]\n",
      "tensor([[0.0539, 0.2104, 0.0730, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.05394065 0.21035218 0.07301397 0.        ]]\n",
      "[0.05394065 0.21035218 0.07301397 0.        ]\n",
      "tensor([[0.0000, 0.2297, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.22966108 0.         0.        ]]\n",
      "[0.         0.22966108 0.         0.        ]\n",
      "tensor([[0.0635, 0.1627, 0.0316, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.06354015 0.16270158 0.03163096 0.        ]]\n",
      "[0.06354015 0.16270158 0.03163096 0.        ]\n",
      "tensor([[0.0000, 0.1225, 0.1228, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.12245525 0.12277543 0.        ]]\n",
      "[0.         0.12245525 0.12277543 0.        ]\n",
      "tensor([[0.0000, 0.2274, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.22740883 0.         0.        ]]\n",
      "[0.         0.22740883 0.         0.        ]\n",
      "tensor([[0.0000, 0.1768, 0.0956, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.17678744 0.09564695 0.        ]]\n",
      "[0.         0.17678744 0.09564695 0.        ]\n",
      "tensor([[0.0000, 0.2051, 0.0000, 0.0262]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.20510742 0.         0.02623449]]\n",
      "[0.         0.20510742 0.         0.02623449]\n",
      "tensor([[0.0000, 0.0727, 0.0112, 0.0304]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.07270113 0.01119964 0.03040518]]\n",
      "[0.         0.07270113 0.01119964 0.03040518]\n",
      "tensor([[0.0000, 0.0860, 0.0000, 0.0165]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.08595118 0.         0.01646062]]\n",
      "[0.         0.08595118 0.         0.01646062]\n",
      "tensor([[0.0000, 0.1663, 0.0000, 0.0682]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.16631213 0.         0.06815303]]\n",
      "[0.         0.16631213 0.         0.06815303]\n",
      "tensor([[0.0000, 0.1531, 0.0096, 0.0870]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.15307958 0.00958407 0.08701818]]\n",
      "[0.         0.15307958 0.00958407 0.08701818]\n",
      "tensor([[0.0000, 0.1757, 0.0000, 0.1505]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.17574957 0.         0.15049517]]\n",
      "[0.         0.17574957 0.         0.15049517]\n",
      "tensor([[0.0000, 0.1590, 0.0543, 0.1334]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.15904175 0.05431369 0.133369  ]]\n",
      "[0.         0.15904175 0.05431369 0.133369  ]\n",
      "tensor([[0.0000, 0.2350, 0.0276, 0.1418]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.23503321 0.02757881 0.14183038]]\n",
      "[0.         0.23503321 0.02757881 0.14183038]\n",
      "tensor([[0.0000, 0.1060, 0.0000, 0.0183]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.10604623 0.         0.0183394 ]]\n",
      "[0.         0.10604623 0.         0.0183394 ]\n",
      "tensor([[0.0000, 0.1005, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.10051788 0.         0.        ]]\n",
      "[0.         0.10051788 0.         0.        ]\n",
      "tensor([[0.0000, 0.0571, 0.0000, 0.0520]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.05706014 0.         0.05204405]]\n",
      "[0.         0.05706014 0.         0.05204405]\n",
      "tensor([[0.0000, 0.0222, 0.0000, 0.0404]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.02217674 0.         0.04037436]]\n",
      "[0.         0.02217674 0.         0.04037436]\n",
      "tensor([[0.0000, 0.1157, 0.0000, 0.0935]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.11571972 0.         0.09346121]]\n",
      "[0.         0.11571972 0.         0.09346121]\n",
      "tensor([[0.0000, 0.1182, 0.0000, 0.1169]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.11819448 0.         0.11689064]]\n",
      "[0.         0.11819448 0.         0.11689064]\n",
      "tensor([[0.0000, 0.1739, 0.0098, 0.0366]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.17393985 0.0098004  0.03661746]]\n",
      "[0.         0.17393985 0.0098004  0.03661746]\n",
      "tensor([[0.0000, 0.1530, 0.0747, 0.0486]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.1529524  0.07471464 0.04864862]]\n",
      "[0.         0.1529524  0.07471464 0.04864862]\n",
      "tensor([[0.0000, 0.2512, 0.0587, 0.0247]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.25124663 0.05869663 0.02465592]]\n",
      "[0.         0.25124663 0.05869663 0.02465592]\n",
      "tensor([[0.0000, 0.1938, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.19381025 0.         0.        ]]\n",
      "[0.         0.19381025 0.         0.        ]\n",
      "tensor([[0.0000, 0.2015, 0.0000, 0.0866]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.20152688 0.         0.08656445]]\n",
      "[0.         0.20152688 0.         0.08656445]\n",
      "tensor([[0.0000, 0.0926, 0.0000, 0.0804]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.09260724 0.         0.08039406]]\n",
      "[0.         0.09260724 0.         0.08039406]\n",
      "tensor([[0.0000, 0.0416, 0.0000, 0.0650]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.04161494 0.         0.06499475]]\n",
      "[0.         0.04161494 0.         0.06499475]\n",
      "tensor([[0.0000, 0.1201, 0.0000, 0.1345]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.12008554 0.         0.13447936]]\n",
      "[0.         0.12008554 0.         0.13447936]\n",
      "tensor([[0.0000, 0.0455, 0.0000, 0.0899]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.04549922 0.         0.08994053]]\n",
      "[0.         0.04549922 0.         0.08994053]\n",
      "tensor([[0.0361, 0.1051, 0.0492, 0.1081]], grad_fn=<ReluBackward0>)\n",
      "[[0.03606881 0.1050527  0.04917343 0.10810751]]\n",
      "[0.03606881 0.1050527  0.04917343 0.10810751]\n",
      "tensor([[0.0000, 0.1622, 0.0370, 0.0873]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.16224566 0.03704759 0.08732303]]\n",
      "[0.         0.16224566 0.03704759 0.08732303]\n",
      "tensor([[0.0227, 0.1258, 0.0000, 0.0564]], grad_fn=<ReluBackward0>)\n",
      "[[0.02272718 0.12575394 0.         0.05641257]]\n",
      "[0.02272718 0.12575394 0.         0.05641257]\n",
      "tensor([[0.0136, 0.1852, 0.0000, 0.1199]], grad_fn=<ReluBackward0>)\n",
      "[[0.01362343 0.185161   0.         0.11994719]]\n",
      "[0.01362343 0.185161   0.         0.11994719]\n",
      "tensor([[0.0000, 0.1258, 0.0000, 0.0307]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.12578335 0.         0.0306779 ]]\n",
      "[0.         0.12578335 0.         0.0306779 ]\n",
      "tensor([[0.0000, 0.1525, 0.0000, 0.0390]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.1524973  0.         0.03899946]]\n",
      "[0.         0.1524973  0.         0.03899946]\n",
      "tensor([[0.0000, 0.1461, 0.0176, 0.0942]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.14608547 0.01755766 0.09420481]]\n",
      "[0.         0.14608547 0.01755766 0.09420481]\n",
      "tensor([[0.0000, 0.1318, 0.0000, 0.2105]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.13183384 0.         0.21049744]]\n",
      "[0.         0.13183384 0.         0.21049744]\n",
      "tensor([[0.0000, 0.1215, 0.0000, 0.2712]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.12151861 0.         0.27124864]]\n",
      "[0.         0.12151861 0.         0.27124864]\n",
      "tensor([[0.0000, 0.1318, 0.0000, 0.1952]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.13179229 0.         0.19523276]]\n",
      "[0.         0.13179229 0.         0.19523276]\n",
      "tensor([[0.0000, 0.1049, 0.0203, 0.1177]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.10494159 0.02031105 0.11772306]]\n",
      "[0.         0.10494159 0.02031105 0.11772306]\n",
      "tensor([[0.0000, 0.1292, 0.0200, 0.0919]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.12916175 0.01999436 0.09189151]]\n",
      "[0.         0.12916175 0.01999436 0.09189151]\n",
      "tensor([[0.0583, 0.1394, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.0582642  0.13944079 0.         0.        ]]\n",
      "[0.0582642  0.13944079 0.         0.        ]\n",
      "tensor([[0.0000, 0.1463, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.14627114 0.         0.        ]]\n",
      "[0.         0.14627114 0.         0.        ]\n",
      "tensor([[0.0000, 0.2124, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.21236114 0.         0.        ]]\n",
      "[0.         0.21236114 0.         0.        ]\n",
      "tensor([[0.0000, 0.1295, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.12952107 0.         0.        ]]\n",
      "[0.         0.12952107 0.         0.        ]\n",
      "tensor([[0.0000, 0.0685, 0.0000, 0.0457]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.06848949 0.         0.04565393]]\n",
      "[0.         0.06848949 0.         0.04565393]\n",
      "tensor([[0.0000, 0.1838, 0.0000, 0.1018]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.18384974 0.         0.10182086]]\n",
      "[0.         0.18384974 0.         0.10182086]\n",
      "tensor([[0.0000, 0.1024, 0.0000, 0.0801]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.10242777 0.         0.08008914]]\n",
      "[0.         0.10242777 0.         0.08008914]\n",
      "tensor([[0.0000, 0.1233, 0.0208, 0.0243]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.12326634 0.02080217 0.02433291]]\n",
      "[0.         0.12326634 0.02080217 0.02433291]\n",
      "tensor([[0.0000, 0.1286, 0.0193, 0.1380]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.1285812  0.01927827 0.13796833]]\n",
      "[0.         0.1285812  0.01927827 0.13796833]\n",
      "tensor([[0.0000, 0.1985, 0.0739, 0.1031]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.19847149 0.07390432 0.10308851]]\n",
      "[0.         0.19847149 0.07390432 0.10308851]\n",
      "tensor([[0.0000, 0.3216, 0.1585, 0.1873]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.32156423 0.15850082 0.18730175]]\n",
      "[0.         0.32156423 0.15850082 0.18730175]\n",
      "tensor([[0.0000, 0.3466, 0.0000, 0.1532]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.34657365 0.         0.15317023]]\n",
      "[0.         0.34657365 0.         0.15317023]\n",
      "tensor([[0.0000, 0.1402, 0.0000, 0.1034]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.14023584 0.         0.10341161]]\n",
      "[0.         0.14023584 0.         0.10341161]\n",
      "tensor([[0.0000, 0.1440, 0.0842, 0.0859]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.14398877 0.08417787 0.08590645]]\n",
      "[0.         0.14398877 0.08417787 0.08590645]\n",
      "tensor([[0.0120, 0.0603, 0.0192, 0.0874]], grad_fn=<ReluBackward0>)\n",
      "[[0.01200566 0.06027305 0.01921128 0.08743015]]\n",
      "[0.01200566 0.06027305 0.01921128 0.08743015]\n",
      "tensor([[0.0000, 0.0000, 0.0367, 0.0978]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.         0.03673278 0.09779271]]\n",
      "[0.         0.         0.03673278 0.09779271]\n",
      "tensor([[0.0000, 0.0553, 0.0835, 0.0261]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.05529957 0.08350744 0.02614685]]\n",
      "[0.         0.05529957 0.08350744 0.02614685]\n",
      "tensor([[0.0000, 0.0955, 0.0000, 0.0542]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.09549685 0.         0.05416953]]\n",
      "[0.         0.09549685 0.         0.05416953]\n",
      "tensor([[0.0000, 0.1894, 0.0000, 0.0437]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.18938608 0.         0.0436908 ]]\n",
      "[0.         0.18938608 0.         0.0436908 ]\n",
      "tensor([[0.0000, 0.3032, 0.0000, 0.0345]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.30317974 0.         0.03452157]]\n",
      "[0.         0.30317974 0.         0.03452157]\n",
      "tensor([[0.0192, 0.3003, 0.0000, 0.0048]], grad_fn=<ReluBackward0>)\n",
      "[[0.01917979 0.30026844 0.         0.00476295]]\n",
      "[0.01917979 0.30026844 0.         0.00476295]\n",
      "tensor([[0.0867, 0.2726, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.08673392 0.27263886 0.         0.        ]]\n",
      "[0.08673392 0.27263886 0.         0.        ]\n",
      "tensor([[0.0758, 0.2915, 0.0000, 0.0083]], grad_fn=<ReluBackward0>)\n",
      "[[0.0757747  0.29148766 0.         0.00830388]]\n",
      "[0.0757747  0.29148766 0.         0.00830388]\n",
      "tensor([[0.0238, 0.0529, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.02376015 0.05294181 0.         0.        ]]\n",
      "[0.02376015 0.05294181 0.         0.        ]\n",
      "tensor([[0.0441, 0.1375, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.04405417 0.13746482 0.         0.        ]]\n",
      "[0.04405417 0.13746482 0.         0.        ]\n",
      "tensor([[0.0835, 0.1179, 0.0000, 0.0949]], grad_fn=<ReluBackward0>)\n",
      "[[0.08345477 0.11786851 0.         0.09488703]]\n",
      "[0.08345477 0.11786851 0.         0.09488703]\n",
      "tensor([[0.0000, 0.0281, 0.0000, 0.0425]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.0281135  0.         0.04250215]]\n",
      "[0.         0.0281135  0.         0.04250215]\n",
      "tensor([[0.0420, 0.1216, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.04197744 0.12158689 0.         0.        ]]\n",
      "[0.04197744 0.12158689 0.         0.        ]\n",
      "tensor([[0.0514, 0.1372, 0.0618, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.05137785 0.13719285 0.06179442 0.        ]]\n",
      "[0.05137785 0.13719285 0.06179442 0.        ]\n",
      "tensor([[0.0000, 0.0969, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.09688313 0.         0.        ]]\n",
      "[0.         0.09688313 0.         0.        ]\n",
      "tensor([[0.0482, 0.0578, 0.0239, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.04823105 0.05775613 0.02393541 0.        ]]\n",
      "[0.04823105 0.05775613 0.02393541 0.        ]\n",
      "tensor([[0.0503, 0.1211, 0.0314, 0.0527]], grad_fn=<ReluBackward0>)\n",
      "[[0.05033651 0.12107888 0.03138961 0.05272588]]\n",
      "[0.05033651 0.12107888 0.03138961 0.05272588]\n",
      "tensor([[0.0000, 0.0255, 0.0000, 0.0354]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.02549305 0.         0.03541867]]\n",
      "[0.         0.02549305 0.         0.03541867]\n",
      "tensor([[0.0000, 0.0413, 0.0000, 0.0922]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.0412797  0.         0.09224771]]\n",
      "[0.         0.0412797  0.         0.09224771]\n",
      "tensor([[0.0120, 0.0603, 0.0192, 0.0874]], grad_fn=<ReluBackward0>)\n",
      "[[0.01200566 0.06027305 0.01921128 0.08743015]]\n",
      "[0.01200566 0.06027305 0.01921128 0.08743015]\n",
      "tensor([[0.0000, 0.0059, 0.0000, 0.0434]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.00593228 0.         0.04339155]]\n",
      "[0.         0.00593228 0.         0.04339155]\n",
      "tensor([[0.0000, 0.0784, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.07837917 0.         0.        ]]\n",
      "[0.         0.07837917 0.         0.        ]\n",
      "tensor([[0.0113, 0.0856, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.01128995 0.08564232 0.         0.        ]]\n",
      "[0.01128995 0.08564232 0.         0.        ]\n",
      "tensor([[0.0000, 0.1122, 0.0000, 0.0068]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.11218444 0.         0.00678451]]\n",
      "[0.         0.11218444 0.         0.00678451]\n",
      "tensor([[0.0000, 0.1145, 0.0000, 0.0170]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.11450146 0.         0.01698935]]\n",
      "[0.         0.11450146 0.         0.01698935]\n",
      "tensor([[0.0000, 0.2653, 0.0319, 0.0118]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.26534146 0.03191542 0.01181637]]\n",
      "[0.         0.26534146 0.03191542 0.01181637]\n",
      "tensor([[0.0000, 0.1865, 0.0416, 0.0193]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.18650426 0.04157899 0.01934251]]\n",
      "[0.         0.18650426 0.04157899 0.01934251]\n",
      "tensor([[0.0000, 0.2435, 0.0708, 0.0977]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.24354927 0.0707598  0.09766617]]\n",
      "[0.         0.24354927 0.0707598  0.09766617]\n",
      "tensor([[0.0000, 0.2657, 0.0000, 0.1229]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.2656969  0.         0.12285833]]\n",
      "[0.         0.2656969  0.         0.12285833]\n",
      "tensor([[0.0000, 0.1439, 0.0000, 0.0900]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.14390099 0.         0.09001596]]\n",
      "[0.         0.14390099 0.         0.09001596]\n",
      "tensor([[0.0000, 0.1152, 0.0000, 0.0499]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.11518987 0.         0.04990738]]\n",
      "[0.         0.11518987 0.         0.04990738]\n",
      "tensor([[0.0000, 0.1342, 0.0000, 0.1821]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.13417906 0.         0.182073  ]]\n",
      "[0.         0.13417906 0.         0.182073  ]\n",
      "tensor([[0.0000, 0.0466, 0.0093, 0.2227]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.04657301 0.00929119 0.22268759]]\n",
      "[0.         0.04657301 0.00929119 0.22268759]\n",
      "tensor([[0.0000, 0.0600, 0.0025, 0.2201]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.05995318 0.00254576 0.22007667]]\n",
      "[0.         0.05995318 0.00254576 0.22007667]\n",
      "tensor([[0.0000, 0.1270, 0.0000, 0.1773]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.1269543  0.         0.17725168]]\n",
      "[0.         0.1269543  0.         0.17725168]\n",
      "tensor([[0.0000, 0.1560, 0.0037, 0.1160]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.1559976  0.00366236 0.11597931]]\n",
      "[0.         0.1559976  0.00366236 0.11597931]\n",
      "tensor([[0.0000, 0.0057, 0.0736, 0.0918]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.00572175 0.07356358 0.09177266]]\n",
      "[0.         0.00572175 0.07356358 0.09177266]\n",
      "tensor([[0.0000, 0.0930, 0.0000, 0.1718]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.0930278  0.         0.17182961]]\n",
      "[0.         0.0930278  0.         0.17182961]\n",
      "tensor([[0.0000, 0.1823, 0.0000, 0.1518]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.18233502 0.         0.151786  ]]\n",
      "[0.         0.18233502 0.         0.151786  ]\n",
      "tensor([[0.0000, 0.0834, 0.0000, 0.0879]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.08340966 0.         0.08793494]]\n",
      "[0.         0.08340966 0.         0.08793494]\n",
      "tensor([[0.0000, 0.1478, 0.0000, 0.0931]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.14784266 0.         0.09311058]]\n",
      "[0.         0.14784266 0.         0.09311058]\n",
      "tensor([[0.0000, 0.1249, 0.0000, 0.0047]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.12486216 0.         0.00467896]]\n",
      "[0.         0.12486216 0.         0.00467896]\n",
      "tensor([[0., 0., 0., 0.]], grad_fn=<ReluBackward0>)\n",
      "[[0. 0. 0. 0.]]\n",
      "[0. 0. 0. 0.]\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0308]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.         0.         0.03075739]]\n",
      "[0.         0.         0.         0.03075739]\n",
      "tensor([[0.0000, 0.0949, 0.0000, 0.0833]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.09493154 0.         0.0832952 ]]\n",
      "[0.         0.09493154 0.         0.0832952 ]\n",
      "tensor([[0.0000, 0.1378, 0.0000, 0.1238]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.13778749 0.         0.12375764]]\n",
      "[0.         0.13778749 0.         0.12375764]\n",
      "tensor([[0.0000, 0.1185, 0.0000, 0.1995]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.11852656 0.         0.19950263]]\n",
      "[0.         0.11852656 0.         0.19950263]\n",
      "tensor([[0.0000, 0.2367, 0.0952, 0.1368]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.2366879  0.0952314  0.13681716]]\n",
      "[0.         0.2366879  0.0952314  0.13681716]\n",
      "tensor([[0.0000, 0.1733, 0.0000, 0.1291]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.17329437 0.         0.12908192]]\n",
      "[0.         0.17329437 0.         0.12908192]\n",
      "tensor([[0.0000, 0.0150, 0.0832, 0.2704]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.01502673 0.08320202 0.27038175]]\n",
      "[0.         0.01502673 0.08320202 0.27038175]\n",
      "tensor([[0.0000, 0.0384, 0.0000, 0.2170]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.0383998  0.         0.21699496]]\n",
      "[0.         0.0383998  0.         0.21699496]\n",
      "tensor([[0.0000, 0.1001, 0.0328, 0.1805]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.10013402 0.03275426 0.18045   ]]\n",
      "[0.         0.10013402 0.03275426 0.18045   ]\n",
      "tensor([[0.0000, 0.1356, 0.0000, 0.0857]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.13558637 0.         0.08572579]]\n",
      "[0.         0.13558637 0.         0.08572579]\n",
      "tensor([[0.0000, 0.1214, 0.0538, 0.1675]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.12143293 0.05381652 0.16748913]]\n",
      "[0.         0.12143293 0.05381652 0.16748913]\n",
      "tensor([[0.0000, 0.2828, 0.0698, 0.0611]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.28276646 0.06979083 0.06110338]]\n",
      "[0.         0.28276646 0.06979083 0.06110338]\n",
      "tensor([[0.0000, 0.2562, 0.0322, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.25615183 0.0322216  0.        ]]\n",
      "[0.         0.25615183 0.0322216  0.        ]\n",
      "tensor([[0.0000, 0.1691, 0.1308, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.16906533 0.13083269 0.        ]]\n",
      "[0.         0.16906533 0.13083269 0.        ]\n",
      "tensor([[0.0000, 0.1963, 0.0142, 0.0346]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.19633529 0.0142104  0.03464385]]\n",
      "[0.         0.19633529 0.0142104  0.03464385]\n",
      "tensor([[0.0000, 0.2127, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.21266603 0.         0.        ]]\n",
      "[0.         0.21266603 0.         0.        ]\n",
      "tensor([[0.0000, 0.1114, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.11135808 0.         0.        ]]\n",
      "[0.         0.11135808 0.         0.        ]\n",
      "tensor([[0.0000, 0.1619, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.16186775 0.         0.        ]]\n",
      "[0.         0.16186775 0.         0.        ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 0.2240, 0.0000, 0.0874]], grad_fn=<ReluBackward0>)\n",
      "[[0.        0.2239669 0.        0.0873535]]\n",
      "[0.        0.2239669 0.        0.0873535]\n",
      "tensor([[0.0000, 0.2733, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.        0.2732876 0.        0.       ]]\n",
      "[0.        0.2732876 0.        0.       ]\n",
      "tensor([[0.0000, 0.2174, 0.0035, 0.0994]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.21740313 0.00349366 0.09943856]]\n",
      "[0.         0.21740313 0.00349366 0.09943856]\n",
      "Episode: 0, Total Reward (running avg):    4 (4.00) Epsilon: 0.999, Avg Q: 0.04925\n",
      "tensor([[0.0000, 0.1161, 0.0000, 0.0099]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.11605339 0.         0.0099329 ]]\n",
      "[0.         0.11605339 0.         0.0099329 ]\n",
      "tensor([[0.0000, 0.1360, 0.0000, 0.0671]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.13599235 0.         0.06707342]]\n",
      "[0.         0.13599235 0.         0.06707342]\n",
      "tensor([[0.0000, 0.0161, 0.0000, 0.0183]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.01608684 0.         0.01833603]]\n",
      "[0.         0.01608684 0.         0.01833603]\n",
      "tensor([[0.0000, 0.1464, 0.0000, 0.1483]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.14642826 0.         0.1483466 ]]\n",
      "[0.         0.14642826 0.         0.1483466 ]\n",
      "tensor([[0.0000, 0.1265, 0.0000, 0.1495]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.12652105 0.         0.14951101]]\n",
      "[0.         0.12652105 0.         0.14951101]\n",
      "tensor([[0.0000, 0.0656, 0.0602, 0.0902]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.06555304 0.0602252  0.09019852]]\n",
      "[0.         0.06555304 0.0602252  0.09019852]\n",
      "tensor([[0.0000, 0.0968, 0.0050, 0.0335]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.09677562 0.00499177 0.03345386]]\n",
      "[0.         0.09677562 0.00499177 0.03345386]\n",
      "tensor([[0.0000, 0.1215, 0.0000, 0.0856]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.1214788  0.         0.08557477]]\n",
      "[0.         0.1214788  0.         0.08557477]\n",
      "tensor([[0.0000, 0.0256, 0.0533, 0.0677]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.02559204 0.0532734  0.06766544]]\n",
      "[0.         0.02559204 0.0532734  0.06766544]\n",
      "tensor([[0.0000, 0.0809, 0.0000, 0.0499]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.08090798 0.         0.04993713]]\n",
      "[0.         0.08090798 0.         0.04993713]\n",
      "tensor([[0.0000, 0.0777, 0.0291, 0.0490]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.0776895  0.02914152 0.04895918]]\n",
      "[0.         0.0776895  0.02914152 0.04895918]\n",
      "tensor([[0.0000, 0.1343, 0.0000, 0.0017]], grad_fn=<ReluBackward0>)\n",
      "[[0.        0.1342859 0.        0.0016578]]\n",
      "[0.        0.1342859 0.        0.0016578]\n",
      "tensor([[0.0000, 0.0395, 0.0000, 0.0677]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.03947815 0.         0.0676727 ]]\n",
      "[0.         0.03947815 0.         0.0676727 ]\n",
      "tensor([[0.0000, 0.1623, 0.0065, 0.1637]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.16232002 0.00650293 0.16370371]]\n",
      "[0.         0.16232002 0.00650293 0.16370371]\n",
      "tensor([[0.0000, 0.0790, 0.0000, 0.0833]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.07896894 0.         0.08333787]]\n",
      "[0.         0.07896894 0.         0.08333787]\n",
      "tensor([[0.0000, 0.2099, 0.0000, 0.0734]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.20994137 0.         0.07336348]]\n",
      "[0.         0.20994137 0.         0.07336348]\n",
      "tensor([[0.0000, 0.0651, 0.0216, 0.0399]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.06513803 0.02159502 0.03992864]]\n",
      "[0.         0.06513803 0.02159502 0.03992864]\n",
      "tensor([[0.0000, 0.0554, 0.0488, 0.0984]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.0553645  0.04882665 0.0983736 ]]\n",
      "[0.         0.0553645  0.04882665 0.0983736 ]\n",
      "tensor([[0.0000, 0.2038, 0.0185, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.20375992 0.0185231  0.        ]]\n",
      "[0.         0.20375992 0.0185231  0.        ]\n",
      "tensor([[0.0000, 0.1089, 0.0000, 0.0106]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.10894857 0.         0.01059071]]\n",
      "[0.         0.10894857 0.         0.01059071]\n",
      "tensor([[0.0000, 0.1711, 0.0000, 0.0172]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.1711151  0.         0.01719775]]\n",
      "[0.         0.1711151  0.         0.01719775]\n",
      "tensor([[0.0000, 0.0485, 0.0000, 0.0436]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.04848785 0.         0.04364485]]\n",
      "[0.         0.04848785 0.         0.04364485]\n",
      "tensor([[0.0000, 0.1747, 0.0212, 0.0728]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.17472786 0.02118475 0.07279865]]\n",
      "[0.         0.17472786 0.02118475 0.07279865]\n",
      "tensor([[0.0000, 0.1089, 0.0000, 0.0106]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.10894857 0.         0.01059071]]\n",
      "[0.         0.10894857 0.         0.01059071]\n",
      "tensor([[0.0000, 0.1548, 0.0000, 0.0576]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.1548183  0.         0.05762461]]\n",
      "[0.         0.1548183  0.         0.05762461]\n",
      "tensor([[0.0000, 0.1350, 0.0300, 0.1046]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.13496868 0.02995085 0.10461314]]\n",
      "[0.         0.13496868 0.02995085 0.10461314]\n",
      "tensor([[0.0000, 0.1473, 0.0140, 0.0472]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.14734907 0.0139682  0.04717883]]\n",
      "[0.         0.14734907 0.0139682  0.04717883]\n",
      "tensor([[0.0000, 0.0494, 0.0284, 0.0725]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.04943727 0.02842144 0.07249998]]\n",
      "[0.         0.04943727 0.02842144 0.07249998]\n",
      "tensor([[0.0000, 0.1062, 0.0000, 0.1451]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.10619874 0.         0.14506009]]\n",
      "[0.         0.10619874 0.         0.14506009]\n",
      "tensor([[0.0000, 0.0165, 0.0000, 0.1350]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.01647221 0.         0.13501465]]\n",
      "[0.         0.01647221 0.         0.13501465]\n",
      "tensor([[0.0000, 0.0298, 0.0000, 0.0769]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.02975144 0.         0.07687531]]\n",
      "[0.         0.02975144 0.         0.07687531]\n",
      "tensor([[0.0000, 0.1604, 0.0000, 0.0660]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.16035007 0.         0.06600564]]\n",
      "[0.         0.16035007 0.         0.06600564]\n",
      "tensor([[0.0000, 0.0891, 0.0060, 0.0655]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.0890893  0.00603496 0.06553687]]\n",
      "[0.         0.0890893  0.00603496 0.06553687]\n",
      "tensor([[0.0822, 0.0000, 0.0006, 0.0357]], grad_fn=<ReluBackward0>)\n",
      "[[0.0822116  0.         0.0006288  0.03566684]]\n",
      "[0.0822116  0.         0.0006288  0.03566684]\n",
      "tensor([[0.0000, 0.0617, 0.0000, 0.0442]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.06167418 0.         0.04418401]]\n",
      "[0.         0.06167418 0.         0.04418401]\n",
      "tensor([[0.0000, 0.0656, 0.0602, 0.0902]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.06555304 0.0602252  0.09019852]]\n",
      "[0.         0.06555304 0.0602252  0.09019852]\n",
      "tensor([[0.0000, 0.1265, 0.0000, 0.1495]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.12652105 0.         0.14951101]]\n",
      "[0.         0.12652105 0.         0.14951101]\n",
      "tensor([[0.0000, 0.0317, 0.0732, 0.0454]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.03170867 0.07321525 0.04537247]]\n",
      "[0.         0.03170867 0.07321525 0.04537247]\n",
      "tensor([[0.0000, 0.0748, 0.0413, 0.0949]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.07479246 0.0413271  0.09491062]]\n",
      "[0.         0.07479246 0.0413271  0.09491062]\n",
      "tensor([[0.0822, 0.0000, 0.0006, 0.0357]], grad_fn=<ReluBackward0>)\n",
      "[[0.0822116  0.         0.0006288  0.03566684]]\n",
      "[0.0822116  0.         0.0006288  0.03566684]\n",
      "tensor([[0.0000, 0.0891, 0.0060, 0.0655]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.0890893  0.00603496 0.06553687]]\n",
      "[0.         0.0890893  0.00603496 0.06553687]\n",
      "tensor([[0.0000, 0.1326, 0.0000, 0.1414]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.13256842 0.         0.14136918]]\n",
      "[0.         0.13256842 0.         0.14136918]\n",
      "tensor([[0.0000, 0.0748, 0.0413, 0.0949]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.07479246 0.0413271  0.09491062]]\n",
      "[0.         0.07479246 0.0413271  0.09491062]\n",
      "tensor([[0.0000, 0.0317, 0.0732, 0.0454]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.03170867 0.07321525 0.04537247]]\n",
      "[0.         0.03170867 0.07321525 0.04537247]\n",
      "tensor([[0.0000, 0.0617, 0.0000, 0.0442]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.06167418 0.         0.04418401]]\n",
      "[0.         0.06167418 0.         0.04418401]\n",
      "tensor([[0.0822, 0.0000, 0.0006, 0.0357]], grad_fn=<ReluBackward0>)\n",
      "[[0.0822116  0.         0.0006288  0.03566684]]\n",
      "[0.0822116  0.         0.0006288  0.03566684]\n",
      "tensor([[0.0000, 0.0748, 0.0413, 0.0949]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.07479246 0.0413271  0.09491062]]\n",
      "[0.         0.07479246 0.0413271  0.09491062]\n",
      "tensor([[0.0000, 0.0317, 0.0732, 0.0454]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.03170867 0.07321525 0.04537247]]\n",
      "[0.         0.03170867 0.07321525 0.04537247]\n",
      "tensor([[0.0000, 0.1265, 0.0000, 0.1495]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.12652105 0.         0.14951101]]\n",
      "[0.         0.12652105 0.         0.14951101]\n",
      "tensor([[0.0000, 0.1081, 0.0000, 0.1553]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.10811871 0.         0.15528883]]\n",
      "[0.         0.10811871 0.         0.15528883]\n",
      "tensor([[0.0000, 0.2078, 0.0535, 0.1164]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.20775136 0.05354818 0.11640261]]\n",
      "[0.         0.20775136 0.05354818 0.11640261]\n",
      "tensor([[0.0000, 0.1063, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.10632712 0.         0.        ]]\n",
      "[0.         0.10632712 0.         0.        ]\n",
      "tensor([[0.0000, 0.1104, 0.1038, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.11042745 0.10379358 0.        ]]\n",
      "[0.         0.11042745 0.10379358 0.        ]\n",
      "tensor([[0.0000, 0.0732, 0.0000, 0.0265]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.07319622 0.         0.02651406]]\n",
      "[0.         0.07319622 0.         0.02651406]\n",
      "tensor([[0.0000, 0.0487, 0.0000, 0.0849]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.04868247 0.         0.08493869]]\n",
      "[0.         0.04868247 0.         0.08493869]\n",
      "tensor([[0.0000, 0.0651, 0.0216, 0.0399]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.06513803 0.02159502 0.03992864]]\n",
      "[0.         0.06513803 0.02159502 0.03992864]\n",
      "tensor([[0.0000, 0.1162, 0.0000, 0.0620]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.11616448 0.         0.06204119]]\n",
      "[0.         0.11616448 0.         0.06204119]\n",
      "tensor([[0.0000, 0.1430, 0.0000, 0.1344]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.14296839 0.         0.13443321]]\n",
      "[0.         0.14296839 0.         0.13443321]\n",
      "tensor([[0.0000, 0.0698, 0.0265, 0.1167]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.06981957 0.02648724 0.11674874]]\n",
      "[0.         0.06981957 0.02648724 0.11674874]\n",
      "tensor([[0.0000, 0.0694, 0.0805, 0.1248]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.06941445 0.08049868 0.12477022]]\n",
      "[0.         0.06941445 0.08049868 0.12477022]\n",
      "tensor([[0.0000, 0.1211, 0.0245, 0.0808]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.12112546 0.02453638 0.08076158]]\n",
      "[0.         0.12112546 0.02453638 0.08076158]\n",
      "tensor([[0.0000, 0.1353, 0.0845, 0.0880]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.13531223 0.08450942 0.08797094]]\n",
      "[0.         0.13531223 0.08450942 0.08797094]\n",
      "tensor([[0.0000, 0.0256, 0.0533, 0.0677]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.02559204 0.0532734  0.06766544]]\n",
      "[0.         0.02559204 0.0532734  0.06766544]\n",
      "tensor([[0.0000, 0.1215, 0.0000, 0.0856]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.1214788  0.         0.08557477]]\n",
      "[0.         0.1214788  0.         0.08557477]\n",
      "tensor([[0.0000, 0.0968, 0.0050, 0.0335]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.09677562 0.00499177 0.03345386]]\n",
      "[0.         0.09677562 0.00499177 0.03345386]\n",
      "tensor([[0.0000, 0.0656, 0.0602, 0.0902]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.06555304 0.0602252  0.09019852]]\n",
      "[0.         0.06555304 0.0602252  0.09019852]\n",
      "tensor([[0.0000, 0.0617, 0.0000, 0.0442]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.06167418 0.         0.04418401]]\n",
      "[0.         0.06167418 0.         0.04418401]\n",
      "tensor([[0.0822, 0.0000, 0.0006, 0.0357]], grad_fn=<ReluBackward0>)\n",
      "[[0.0822116  0.         0.0006288  0.03566684]]\n",
      "[0.0822116  0.         0.0006288  0.03566684]\n",
      "tensor([[0.0000, 0.0891, 0.0060, 0.0655]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.0890893  0.00603496 0.06553687]]\n",
      "[0.         0.0890893  0.00603496 0.06553687]\n",
      "tensor([[0.0000, 0.1326, 0.0000, 0.1414]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.13256842 0.         0.14136918]]\n",
      "[0.         0.13256842 0.         0.14136918]\n",
      "tensor([[0.0000, 0.0871, 0.0000, 0.1101]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.08714835 0.         0.11013757]]\n",
      "[0.         0.08714835 0.         0.11013757]\n",
      "tensor([[0.0000, 0.1410, 0.0000, 0.0608]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.14097397 0.         0.06076933]]\n",
      "[0.         0.14097397 0.         0.06076933]\n",
      "tensor([[0.0000, 0.1473, 0.0140, 0.0472]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.14734907 0.0139682  0.04717883]]\n",
      "[0.         0.14734907 0.0139682  0.04717883]\n",
      "tensor([[0.0000, 0.0494, 0.0284, 0.0725]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.04943727 0.02842144 0.07249998]]\n",
      "[0.         0.04943727 0.02842144 0.07249998]\n",
      "tensor([[0.0000, 0.1062, 0.0000, 0.1451]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.10619874 0.         0.14506009]]\n",
      "[0.         0.10619874 0.         0.14506009]\n",
      "tensor([[0.0000, 0.0165, 0.0000, 0.1350]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.01647221 0.         0.13501465]]\n",
      "[0.         0.01647221 0.         0.13501465]\n",
      "tensor([[0.0000, 0.0298, 0.0000, 0.0769]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.02975144 0.         0.07687531]]\n",
      "[0.         0.02975144 0.         0.07687531]\n",
      "tensor([[0.0000, 0.1316, 0.0328, 0.0425]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.131587   0.03284494 0.0424763 ]]\n",
      "[0.         0.131587   0.03284494 0.0424763 ]\n",
      "tensor([[0.0000, 0.1023, 0.0465, 0.1147]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.10232601 0.046487   0.11470219]]\n",
      "[0.         0.10232601 0.046487   0.11470219]\n",
      "tensor([[0.0822, 0.0000, 0.0006, 0.0357]], grad_fn=<ReluBackward0>)\n",
      "[[0.0822116  0.         0.0006288  0.03566684]]\n",
      "[0.0822116  0.         0.0006288  0.03566684]\n",
      "tensor([[0.0000, 0.0891, 0.0060, 0.0655]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.0890893  0.00603496 0.06553687]]\n",
      "[0.         0.0890893  0.00603496 0.06553687]\n",
      "tensor([[0.0000, 0.1326, 0.0000, 0.1414]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.13256842 0.         0.14136918]]\n",
      "[0.         0.13256842 0.         0.14136918]\n",
      "tensor([[0.0000, 0.0871, 0.0000, 0.1101]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.08714835 0.         0.11013757]]\n",
      "[0.         0.08714835 0.         0.11013757]\n",
      "tensor([[0.0000, 0.0519, 0.0000, 0.0953]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.05192556 0.         0.09534891]]\n",
      "[0.         0.05192556 0.         0.09534891]\n",
      "tensor([[0.0000, 0.1548, 0.0000, 0.0576]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.1548183  0.         0.05762461]]\n",
      "[0.         0.1548183  0.         0.05762461]\n",
      "tensor([[0.0000, 0.1350, 0.0300, 0.1046]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.13496868 0.02995085 0.10461314]]\n",
      "[0.         0.13496868 0.02995085 0.10461314]\n",
      "tensor([[0.0000, 0.1275, 0.0000, 0.1271]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.12751378 0.         0.12706192]]\n",
      "[0.         0.12751378 0.         0.12706192]\n",
      "tensor([[0.0000, 0.1297, 0.0000, 0.0348]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.12971777 0.         0.03482347]]\n",
      "[0.         0.12971777 0.         0.03482347]\n",
      "tensor([[0.0000, 0.0916, 0.0000, 0.1123]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.09156081 0.         0.11228947]]\n",
      "[0.         0.09156081 0.         0.11228947]\n",
      "tensor([[0.0000, 0.0494, 0.0284, 0.0725]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.04943727 0.02842144 0.07249998]]\n",
      "[0.         0.04943727 0.02842144 0.07249998]\n",
      "tensor([[0.0000, 0.1473, 0.0140, 0.0472]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.14734907 0.0139682  0.04717883]]\n",
      "[0.         0.14734907 0.0139682  0.04717883]\n",
      "tensor([[0.0000, 0.1350, 0.0300, 0.1046]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.13496868 0.02995085 0.10461314]]\n",
      "[0.         0.13496868 0.02995085 0.10461314]\n",
      "tensor([[0.0000, 0.2038, 0.0185, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.20375992 0.0185231  0.        ]]\n",
      "[0.         0.20375992 0.0185231  0.        ]\n",
      "tensor([[0.0000, 0.1351, 0.0000, 0.1008]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.13511458 0.         0.10078652]]\n",
      "[0.         0.13511458 0.         0.10078652]\n",
      "tensor([[0.0000, 0.0833, 0.0000, 0.0978]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.0832904  0.         0.09775185]]\n",
      "[0.         0.0832904  0.         0.09775185]\n",
      "tensor([[0.0000, 0.2099, 0.0000, 0.0734]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.20994137 0.         0.07336348]]\n",
      "[0.         0.20994137 0.         0.07336348]\n",
      "tensor([[0.0000, 0.0732, 0.0000, 0.0265]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.07319622 0.         0.02651406]]\n",
      "[0.         0.07319622 0.         0.02651406]\n",
      "tensor([[0.0000, 0.0744, 0.0000, 0.0373]], grad_fn=<ReluBackward0>)\n",
      "[[0.        0.0744461 0.        0.0372575]]\n",
      "[0.        0.0744461 0.        0.0372575]\n",
      "tensor([[0.0000, 0.0849, 0.0315, 0.1192]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.08485635 0.03147621 0.1191975 ]]\n",
      "[0.         0.08485635 0.03147621 0.1191975 ]\n",
      "tensor([[0.0000, 0.1617, 0.0000, 0.0357]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.16167134 0.         0.0356858 ]]\n",
      "[0.         0.16167134 0.         0.0356858 ]\n",
      "tensor([[0.0000, 0.0777, 0.0291, 0.0490]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.0776895  0.02914152 0.04895918]]\n",
      "[0.         0.0776895  0.02914152 0.04895918]\n",
      "tensor([[0.0000, 0.0062, 0.0000, 0.0672]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.00615934 0.         0.0672316 ]]\n",
      "[0.         0.00615934 0.         0.0672316 ]\n",
      "tensor([[0.0000, 0.0291, 0.0000, 0.0968]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.02909801 0.         0.09679402]]\n",
      "[0.         0.02909801 0.         0.09679402]\n",
      "tensor([[0.0000, 0.0763, 0.0000, 0.0387]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.07628205 0.         0.03869843]]\n",
      "[0.         0.07628205 0.         0.03869843]\n",
      "tensor([[0.0000, 0.0327, 0.0000, 0.0309]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.03271589 0.         0.03085659]]\n",
      "[0.         0.03271589 0.         0.03085659]\n",
      "tensor([[0.0000, 0.0410, 0.0000, 0.1125]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.04103263 0.         0.11248649]]\n",
      "[0.         0.04103263 0.         0.11248649]\n",
      "tensor([[0.0000, 0.0291, 0.0000, 0.0968]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.02909801 0.         0.09679402]]\n",
      "[0.         0.02909801 0.         0.09679402]\n",
      "tensor([[0.0000, 0.0062, 0.0000, 0.0672]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.00615934 0.         0.0672316 ]]\n",
      "[0.         0.00615934 0.         0.0672316 ]\n",
      "tensor([[0.0000, 0.0777, 0.0291, 0.0490]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.0776895  0.02914152 0.04895918]]\n",
      "[0.         0.0776895  0.02914152 0.04895918]\n",
      "tensor([[0.0000, 0.1343, 0.0000, 0.0017]], grad_fn=<ReluBackward0>)\n",
      "[[0.        0.1342859 0.        0.0016578]]\n",
      "[0.        0.1342859 0.        0.0016578]\n",
      "tensor([[0.0000, 0.0395, 0.0000, 0.0677]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.03947815 0.         0.0676727 ]]\n",
      "[0.         0.03947815 0.         0.0676727 ]\n",
      "tensor([[0.0000, 0.1170, 0.0000, 0.1303]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.11698075 0.         0.13033687]]\n",
      "[0.         0.11698075 0.         0.13033687]\n",
      "tensor([[0.0000, 0.0606, 0.0155, 0.0800]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.06057662 0.01547991 0.07995361]]\n",
      "[0.         0.06057662 0.01547991 0.07995361]\n",
      "tensor([[0.0000, 0.1623, 0.0065, 0.1637]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.16232002 0.00650293 0.16370371]]\n",
      "[0.         0.16232002 0.00650293 0.16370371]\n",
      "tensor([[0.0000, 0.2464, 0.0000, 0.1039]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.24635473 0.         0.10394597]]\n",
      "[0.         0.24635473 0.         0.10394597]\n",
      "tensor([[0.0000, 0.0744, 0.0000, 0.0373]], grad_fn=<ReluBackward0>)\n",
      "[[0.        0.0744461 0.        0.0372575]]\n",
      "[0.        0.0744461 0.        0.0372575]\n",
      "tensor([[0.0000, 0.0732, 0.0000, 0.0265]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.07319622 0.         0.02651406]]\n",
      "[0.         0.07319622 0.         0.02651406]\n",
      "tensor([[0.0000, 0.0487, 0.0000, 0.0849]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.04868247 0.         0.08493869]]\n",
      "[0.         0.04868247 0.         0.08493869]\n",
      "tensor([[0.0000, 0.1580, 0.0316, 0.0968]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.15801425 0.03161226 0.09682991]]\n",
      "[0.         0.15801425 0.03161226 0.09682991]\n",
      "tensor([[0.0000, 0.1162, 0.0000, 0.0620]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.11616448 0.         0.06204119]]\n",
      "[0.         0.11616448 0.         0.06204119]\n",
      "tensor([[0.0000, 0.0651, 0.0216, 0.0399]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.06513803 0.02159502 0.03992864]]\n",
      "[0.         0.06513803 0.02159502 0.03992864]\n",
      "tensor([[0.0000, 0.2099, 0.0000, 0.0734]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.20994137 0.         0.07336348]]\n",
      "[0.         0.20994137 0.         0.07336348]\n",
      "tensor([[0.0000, 0.0790, 0.0000, 0.0833]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.07896894 0.         0.08333787]]\n",
      "[0.         0.07896894 0.         0.08333787]\n",
      "tensor([[0.0000, 0.1623, 0.0065, 0.1637]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.16232002 0.00650293 0.16370371]]\n",
      "[0.         0.16232002 0.00650293 0.16370371]\n",
      "tensor([[0.0000, 0.0395, 0.0000, 0.0677]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.03947815 0.         0.0676727 ]]\n",
      "[0.         0.03947815 0.         0.0676727 ]\n",
      "tensor([[0.0000, 0.1170, 0.0000, 0.1303]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.11698075 0.         0.13033687]]\n",
      "[0.         0.11698075 0.         0.13033687]\n",
      "tensor([[0.0000, 0.1492, 0.0530, 0.0945]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.14923862 0.05304171 0.09451737]]\n",
      "[0.         0.14923862 0.05304171 0.09451737]\n",
      "tensor([[0.0000, 0.0876, 0.0000, 0.0970]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.08763715 0.         0.0969956 ]]\n",
      "[0.         0.08763715 0.         0.0969956 ]\n",
      "tensor([[0.0000, 0.0320, 0.0534, 0.1586]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.03196869 0.05335806 0.15855086]]\n",
      "[0.         0.03196869 0.05335806 0.15855086]\n",
      "tensor([[0.0000, 0.0401, 0.0000, 0.0172]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.04010011 0.         0.01715629]]\n",
      "[0.         0.04010011 0.         0.01715629]\n",
      "tensor([[0.0000, 0.1104, 0.0893, 0.0411]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.11039883 0.08926517 0.0411089 ]]\n",
      "[0.         0.11039883 0.08926517 0.0411089 ]\n",
      "tensor([[0.0000, 0.0812, 0.0000, 0.0487]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.08119635 0.         0.04870499]]\n",
      "[0.         0.08119635 0.         0.04870499]\n",
      "tensor([[0.0232, 0.0971, 0.0000, 0.0788]], grad_fn=<ReluBackward0>)\n",
      "[[0.02317564 0.09711187 0.         0.07883482]]\n",
      "[0.02317564 0.09711187 0.         0.07883482]\n",
      "tensor([[0.0000, 0.0485, 0.0000, 0.0436]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.04848785 0.         0.04364485]]\n",
      "[0.         0.04848785 0.         0.04364485]\n",
      "tensor([[0.0122, 0.0870, 0.0000, 0.0198]], grad_fn=<ReluBackward0>)\n",
      "[[0.01223202 0.0869993  0.         0.01982361]]\n",
      "[0.01223202 0.0869993  0.         0.01982361]\n",
      "tensor([[0.0000, 0.0715, 0.0000, 0.0771]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.07154414 0.         0.07707614]]\n",
      "[0.         0.07154414 0.         0.07707614]\n",
      "tensor([[0.0000, 0.0356, 0.0038, 0.0914]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.03555505 0.00381394 0.0913769 ]]\n",
      "[0.         0.03555505 0.00381394 0.0913769 ]\n",
      "tensor([[0.0000, 0.0000, 0.0365, 0.1245]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.         0.03650808 0.1245328 ]]\n",
      "[0.         0.         0.03650808 0.1245328 ]\n",
      "tensor([[0.0000, 0.1427, 0.0000, 0.0123]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.14274877 0.         0.0122704 ]]\n",
      "[0.         0.14274877 0.         0.0122704 ]\n",
      "tensor([[0.0000, 0.0849, 0.0315, 0.1192]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.08485635 0.03147621 0.1191975 ]]\n",
      "[0.         0.08485635 0.03147621 0.1191975 ]\n",
      "tensor([[0.0000, 0.0744, 0.0000, 0.0373]], grad_fn=<ReluBackward0>)\n",
      "[[0.        0.0744461 0.        0.0372575]]\n",
      "[0.        0.0744461 0.        0.0372575]\n",
      "tensor([[0.0000, 0.0732, 0.0000, 0.0265]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.07319622 0.         0.02651406]]\n",
      "[0.         0.07319622 0.         0.02651406]\n",
      "tensor([[0.0000, 0.0487, 0.0000, 0.0849]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.04868247 0.         0.08493869]]\n",
      "[0.         0.04868247 0.         0.08493869]\n",
      "tensor([[0.0000, 0.1580, 0.0316, 0.0968]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.15801425 0.03161226 0.09682991]]\n",
      "[0.         0.15801425 0.03161226 0.09682991]\n",
      "tensor([[0.0000, 0.1162, 0.0000, 0.0620]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.11616448 0.         0.06204119]]\n",
      "[0.         0.11616448 0.         0.06204119]\n",
      "tensor([[0.0000, 0.0651, 0.0216, 0.0399]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.06513803 0.02159502 0.03992864]]\n",
      "[0.         0.06513803 0.02159502 0.03992864]\n",
      "tensor([[0.0000, 0.2099, 0.0000, 0.0734]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.20994137 0.         0.07336348]]\n",
      "[0.         0.20994137 0.         0.07336348]\n",
      "tensor([[0.0000, 0.0790, 0.0000, 0.0833]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.07896894 0.         0.08333787]]\n",
      "[0.         0.07896894 0.         0.08333787]\n",
      "tensor([[0.0000, 0.1623, 0.0065, 0.1637]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.16232002 0.00650293 0.16370371]]\n",
      "[0.         0.16232002 0.00650293 0.16370371]\n",
      "tensor([[0.0000, 0.2464, 0.0000, 0.1039]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.24635473 0.         0.10394597]]\n",
      "[0.         0.24635473 0.         0.10394597]\n",
      "tensor([[0.0000, 0.1617, 0.0000, 0.0357]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.16167134 0.         0.0356858 ]]\n",
      "[0.         0.16167134 0.         0.0356858 ]\n",
      "tensor([[0.0000, 0.1798, 0.0000, 0.1103]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.17975534 0.         0.11026195]]\n",
      "[0.         0.17975534 0.         0.11026195]\n",
      "tensor([[0.0000, 0.1427, 0.0000, 0.0123]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.14274877 0.         0.0122704 ]]\n",
      "[0.         0.14274877 0.         0.0122704 ]\n",
      "tensor([[0.0000, 0.0849, 0.0315, 0.1192]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.08485635 0.03147621 0.1191975 ]]\n",
      "[0.         0.08485635 0.03147621 0.1191975 ]\n",
      "tensor([[0.0000, 0.0744, 0.0000, 0.0373]], grad_fn=<ReluBackward0>)\n",
      "[[0.        0.0744461 0.        0.0372575]]\n",
      "[0.        0.0744461 0.        0.0372575]\n",
      "tensor([[0.0000, 0.0790, 0.0000, 0.0833]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.07896894 0.         0.08333787]]\n",
      "[0.         0.07896894 0.         0.08333787]\n",
      "tensor([[0.0000, 0.1623, 0.0065, 0.1637]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.16232002 0.00650293 0.16370371]]\n",
      "[0.         0.16232002 0.00650293 0.16370371]\n",
      "tensor([[0.0000, 0.0606, 0.0155, 0.0800]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.06057662 0.01547991 0.07995361]]\n",
      "[0.         0.06057662 0.01547991 0.07995361]\n",
      "tensor([[0.0000, 0.1859, 0.0000, 0.0529]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.18594131 0.         0.05291136]]\n",
      "[0.         0.18594131 0.         0.05291136]\n",
      "tensor([[0.0000, 0.0833, 0.0000, 0.0978]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.0832904  0.         0.09775185]]\n",
      "[0.         0.0832904  0.         0.09775185]\n",
      "tensor([[0.0000, 0.0554, 0.0488, 0.0984]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.0553645  0.04882665 0.0983736 ]]\n",
      "[0.         0.0553645  0.04882665 0.0983736 ]\n",
      "tensor([[0.0000, 0.1275, 0.0000, 0.1271]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.12751378 0.         0.12706192]]\n",
      "[0.         0.12751378 0.         0.12706192]\n",
      "tensor([[0.0000, 0.1297, 0.0000, 0.0348]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.12971777 0.         0.03482347]]\n",
      "[0.         0.12971777 0.         0.03482347]\n",
      "tensor([[0.0000, 0.1430, 0.0000, 0.1344]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.14296839 0.         0.13443321]]\n",
      "[0.         0.14296839 0.         0.13443321]\n",
      "tensor([[0.0000, 0.0698, 0.0265, 0.1167]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.06981957 0.02648724 0.11674874]]\n",
      "[0.         0.06981957 0.02648724 0.11674874]\n",
      "tensor([[0.0000, 0.0730, 0.0000, 0.0737]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.07304344 0.         0.07371362]]\n",
      "[0.         0.07304344 0.         0.07371362]\n",
      "tensor([[0.0000, 0.0968, 0.0050, 0.0335]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.09677562 0.00499177 0.03345386]]\n",
      "[0.         0.09677562 0.00499177 0.03345386]\n",
      "tensor([[0.0000, 0.0656, 0.0602, 0.0902]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.06555304 0.0602252  0.09019852]]\n",
      "[0.         0.06555304 0.0602252  0.09019852]\n",
      "tensor([[0.0000, 0.0617, 0.0000, 0.0442]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.06167418 0.         0.04418401]]\n",
      "[0.         0.06167418 0.         0.04418401]\n",
      "tensor([[0.0000, 0.0317, 0.0732, 0.0454]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.03170867 0.07321525 0.04537247]]\n",
      "[0.         0.03170867 0.07321525 0.04537247]\n",
      "tensor([[0.0000, 0.0161, 0.0000, 0.0183]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.01608684 0.         0.01833603]]\n",
      "[0.         0.01608684 0.         0.01833603]\n",
      "tensor([[0.0000, 0.1233, 0.0157, 0.0723]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.12333041 0.01567454 0.07229042]]\n",
      "[0.         0.12333041 0.01567454 0.07229042]\n",
      "tensor([[0.0000, 0.0480, 0.0000, 0.0203]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.04800743 0.         0.02034485]]\n",
      "[0.         0.04800743 0.         0.02034485]\n",
      "tensor([[0.0000, 0.0574, 0.0000, 0.1221]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.05739197 0.         0.12207496]]\n",
      "[0.         0.05739197 0.         0.12207496]\n",
      "tensor([[0.0000, 0.1427, 0.0000, 0.0123]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.14274877 0.         0.0122704 ]]\n",
      "[0.         0.14274877 0.         0.0122704 ]\n",
      "tensor([[0.0000, 0.1798, 0.0000, 0.1103]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.17975534 0.         0.11026195]]\n",
      "[0.         0.17975534 0.         0.11026195]\n",
      "tensor([[0.0000, 0.1759, 0.0507, 0.0677]], grad_fn=<ReluBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.17591913 0.05066484 0.06772557]]\n",
      "[0.         0.17591913 0.05066484 0.06772557]\n",
      "tensor([[0.0000, 0.0000, 0.0365, 0.1245]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.         0.03650808 0.1245328 ]]\n",
      "[0.         0.         0.03650808 0.1245328 ]\n",
      "tensor([[0.0000, 0.1427, 0.0000, 0.0123]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.14274877 0.         0.0122704 ]]\n",
      "[0.         0.14274877 0.         0.0122704 ]\n",
      "tensor([[0.0000, 0.1798, 0.0000, 0.1103]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.17975534 0.         0.11026195]]\n",
      "[0.         0.17975534 0.         0.11026195]\n",
      "tensor([[0.0000, 0.1759, 0.0507, 0.0677]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.17591913 0.05066484 0.06772557]]\n",
      "[0.         0.17591913 0.05066484 0.06772557]\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.1632]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.         0.         0.16321371]]\n",
      "[0.         0.         0.         0.16321371]\n",
      "tensor([[0.0000, 0.0410, 0.0000, 0.1125]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.04103263 0.         0.11248649]]\n",
      "[0.         0.04103263 0.         0.11248649]\n",
      "tensor([[0.0000, 0.0327, 0.0000, 0.0309]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.03271589 0.         0.03085659]]\n",
      "[0.         0.03271589 0.         0.03085659]\n",
      "tensor([[0.0000, 0.0829, 0.0000, 0.0369]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.08291367 0.         0.03690981]]\n",
      "[0.         0.08291367 0.         0.03690981]\n",
      "tensor([[0.0000, 0.1033, 0.0148, 0.0976]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.10326672 0.01475697 0.09760509]]\n",
      "[0.         0.10326672 0.01475697 0.09760509]\n",
      "tensor([[0.0000, 0.1215, 0.0000, 0.0856]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.1214788  0.         0.08557477]]\n",
      "[0.         0.1214788  0.         0.08557477]\n",
      "tensor([[0.0000, 0.0256, 0.0533, 0.0677]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.02559204 0.0532734  0.06766544]]\n",
      "[0.         0.02559204 0.0532734  0.06766544]\n",
      "tensor([[0.0000, 0.0809, 0.0000, 0.0499]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.08090798 0.         0.04993713]]\n",
      "[0.         0.08090798 0.         0.04993713]\n",
      "tensor([[0.0000, 0.0291, 0.0000, 0.0968]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.02909801 0.         0.09679402]]\n",
      "[0.         0.02909801 0.         0.09679402]\n",
      "tensor([[0.0000, 0.0410, 0.0000, 0.1125]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.04103263 0.         0.11248649]]\n",
      "[0.         0.04103263 0.         0.11248649]\n",
      "tensor([[0.0000, 0.0828, 0.0000, 0.0586]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.08284985 0.         0.0585898 ]]\n",
      "[0.         0.08284985 0.         0.0585898 ]\n",
      "tensor([[0.0000, 0.1159, 0.0000, 0.0404]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.11592275 0.         0.04036447]]\n",
      "[0.         0.11592275 0.         0.04036447]\n",
      "tensor([[0.0000, 0.0911, 0.0949, 0.0995]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.09113793 0.09493275 0.09950263]]\n",
      "[0.         0.09113793 0.09493275 0.09950263]\n",
      "tensor([[0.0000, 0.1316, 0.0328, 0.0425]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.131587   0.03284494 0.0424763 ]]\n",
      "[0.         0.131587   0.03284494 0.0424763 ]\n",
      "tensor([[0.0000, 0.0891, 0.0060, 0.0655]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.0890893  0.00603496 0.06553687]]\n",
      "[0.         0.0890893  0.00603496 0.06553687]\n",
      "tensor([[0.0000, 0.1326, 0.0000, 0.1414]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.13256842 0.         0.14136918]]\n",
      "[0.         0.13256842 0.         0.14136918]\n",
      "tensor([[0.0000, 0.0871, 0.0000, 0.1101]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.08714835 0.         0.11013757]]\n",
      "[0.         0.08714835 0.         0.11013757]\n",
      "tensor([[0.0000, 0.1410, 0.0000, 0.0608]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.14097397 0.         0.06076933]]\n",
      "[0.         0.14097397 0.         0.06076933]\n",
      "tensor([[0.0000, 0.1062, 0.0000, 0.1451]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.10619874 0.         0.14506009]]\n",
      "[0.         0.10619874 0.         0.14506009]\n",
      "tensor([[0.0000, 0.0165, 0.0000, 0.1350]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.01647221 0.         0.13501465]]\n",
      "[0.         0.01647221 0.         0.13501465]\n",
      "tensor([[0.0000, 0.0790, 0.0000, 0.0760]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.07895602 0.         0.07603564]]\n",
      "[0.         0.07895602 0.         0.07603564]\n",
      "tensor([[0.0000, 0.0494, 0.0284, 0.0725]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.04943727 0.02842144 0.07249998]]\n",
      "[0.         0.04943727 0.02842144 0.07249998]\n",
      "tensor([[0.0000, 0.1062, 0.0000, 0.1451]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.10619874 0.         0.14506009]]\n",
      "[0.         0.10619874 0.         0.14506009]\n",
      "tensor([[0.0000, 0.1410, 0.0000, 0.0608]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.14097397 0.         0.06076933]]\n",
      "[0.         0.14097397 0.         0.06076933]\n",
      "tensor([[0.0000, 0.0871, 0.0000, 0.1101]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.08714835 0.         0.11013757]]\n",
      "[0.         0.08714835 0.         0.11013757]\n",
      "tensor([[0.0000, 0.1604, 0.0000, 0.0660]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.16035007 0.         0.06600564]]\n",
      "[0.         0.16035007 0.         0.06600564]\n",
      "tensor([[0.0000, 0.0298, 0.0000, 0.0769]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.02975144 0.         0.07687531]]\n",
      "[0.         0.02975144 0.         0.07687531]\n",
      "tensor([[0.0000, 0.1881, 0.0000, 0.0665]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.18805502 0.         0.06653671]]\n",
      "[0.         0.18805502 0.         0.06653671]\n",
      "tensor([[0.0000, 0.1047, 0.0000, 0.0177]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.1046554  0.         0.01771767]]\n",
      "[0.         0.1046554  0.         0.01771767]\n",
      "tensor([[0.0000, 0.0165, 0.0000, 0.1350]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.01647221 0.         0.13501465]]\n",
      "[0.         0.01647221 0.         0.13501465]\n",
      "tensor([[0.0000, 0.0298, 0.0000, 0.0769]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.02975144 0.         0.07687531]]\n",
      "[0.         0.02975144 0.         0.07687531]\n",
      "tensor([[0.0000, 0.1316, 0.0328, 0.0425]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.131587   0.03284494 0.0424763 ]]\n",
      "[0.         0.131587   0.03284494 0.0424763 ]\n",
      "tensor([[0.0000, 0.1023, 0.0465, 0.1147]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.10232601 0.046487   0.11470219]]\n",
      "[0.         0.10232601 0.046487   0.11470219]\n",
      "tensor([[0.0000, 0.0829, 0.0000, 0.0369]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.08291367 0.         0.03690981]]\n",
      "[0.         0.08291367 0.         0.03690981]\n",
      "tensor([[0.0000, 0.1033, 0.0148, 0.0976]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.10326672 0.01475697 0.09760509]]\n",
      "[0.         0.10326672 0.01475697 0.09760509]\n",
      "tensor([[0.0000, 0.0763, 0.0000, 0.0387]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.07628205 0.         0.03869843]]\n",
      "[0.         0.07628205 0.         0.03869843]\n",
      "tensor([[0.0000, 0.0327, 0.0000, 0.0309]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.03271589 0.         0.03085659]]\n",
      "[0.         0.03271589 0.         0.03085659]\n",
      "tensor([[0.0000, 0.0410, 0.0000, 0.1125]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.04103263 0.         0.11248649]]\n",
      "[0.         0.04103263 0.         0.11248649]\n",
      "tensor([[0.0000, 0.0291, 0.0000, 0.0968]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.02909801 0.         0.09679402]]\n",
      "[0.         0.02909801 0.         0.09679402]\n",
      "tensor([[0.0000, 0.0062, 0.0000, 0.0672]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.00615934 0.         0.0672316 ]]\n",
      "[0.         0.00615934 0.         0.0672316 ]\n",
      "tensor([[0.0000, 0.1798, 0.0000, 0.1103]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.17975534 0.         0.11026195]]\n",
      "[0.         0.17975534 0.         0.11026195]\n",
      "tensor([[0.0000, 0.1427, 0.0000, 0.0123]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.14274877 0.         0.0122704 ]]\n",
      "[0.         0.14274877 0.         0.0122704 ]\n",
      "tensor([[0.0000, 0.0000, 0.0365, 0.1245]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.         0.03650808 0.1245328 ]]\n",
      "[0.         0.         0.03650808 0.1245328 ]\n",
      "tensor([[0.0000, 0.0356, 0.0038, 0.0914]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.03555505 0.00381394 0.0913769 ]]\n",
      "[0.         0.03555505 0.00381394 0.0913769 ]\n",
      "tensor([[0.0000, 0.1277, 0.0153, 0.0227]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.12766221 0.01533808 0.0226599 ]]\n",
      "[0.         0.12766221 0.01533808 0.0226599 ]\n",
      "tensor([[0.0232, 0.0971, 0.0000, 0.0788]], grad_fn=<ReluBackward0>)\n",
      "[[0.02317564 0.09711187 0.         0.07883482]]\n",
      "[0.02317564 0.09711187 0.         0.07883482]\n",
      "tensor([[0.0000, 0.0000, 0.0144, 0.0047]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.         0.01436529 0.00465803]]\n",
      "[0.         0.         0.01436529 0.00465803]\n",
      "tensor([[0.0000, 0.2072, 0.0000, 0.0959]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.20719415 0.         0.09592453]]\n",
      "[0.         0.20719415 0.         0.09592453]\n",
      "tensor([[0.0000, 0.1859, 0.0000, 0.0529]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.18594131 0.         0.05291136]]\n",
      "[0.         0.18594131 0.         0.05291136]\n",
      "tensor([[0.0000, 0.0833, 0.0000, 0.0978]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.0832904  0.         0.09775185]]\n",
      "[0.         0.0832904  0.         0.09775185]\n",
      "tensor([[0.0000, 0.1351, 0.0000, 0.1008]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.13511458 0.         0.10078652]]\n",
      "[0.         0.13511458 0.         0.10078652]\n",
      "tensor([[0.0000, 0.2072, 0.0000, 0.0959]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.20719415 0.         0.09592453]]\n",
      "[0.         0.20719415 0.         0.09592453]\n",
      "tensor([[0.0000, 0.0000, 0.0144, 0.0047]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.         0.01436529 0.00465803]]\n",
      "[0.         0.         0.01436529 0.00465803]\n",
      "tensor([[0.0000, 0.1104, 0.0893, 0.0411]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.11039883 0.08926517 0.0411089 ]]\n",
      "[0.         0.11039883 0.08926517 0.0411089 ]\n",
      "tensor([[0.0000, 0.0401, 0.0000, 0.0172]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.04010011 0.         0.01715629]]\n",
      "[0.         0.04010011 0.         0.01715629]\n",
      "tensor([[0.0000, 0.1458, 0.0000, 0.0378]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.1458273  0.         0.03775241]]\n",
      "[0.         0.1458273  0.         0.03775241]\n",
      "tensor([[0.0000, 0.0812, 0.0000, 0.0487]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.08119635 0.         0.04870499]]\n",
      "[0.         0.08119635 0.         0.04870499]\n",
      "tensor([[0.0000, 0.1171, 0.0000, 0.0720]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.11713016 0.         0.07195638]]\n",
      "[0.         0.11713016 0.         0.07195638]\n",
      "tensor([[0.0000, 0.1277, 0.0153, 0.0227]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.12766221 0.01533808 0.0226599 ]]\n",
      "[0.         0.12766221 0.01533808 0.0226599 ]\n",
      "tensor([[0.0232, 0.0971, 0.0000, 0.0788]], grad_fn=<ReluBackward0>)\n",
      "[[0.02317564 0.09711187 0.         0.07883482]]\n",
      "[0.02317564 0.09711187 0.         0.07883482]\n",
      "tensor([[0.0000, 0.0000, 0.0144, 0.0047]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.         0.01436529 0.00465803]]\n",
      "[0.         0.         0.01436529 0.00465803]\n",
      "tensor([[0.0000, 0.2072, 0.0000, 0.0959]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.20719415 0.         0.09592453]]\n",
      "[0.         0.20719415 0.         0.09592453]\n",
      "tensor([[0.0000, 0.1351, 0.0000, 0.1008]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.13511458 0.         0.10078652]]\n",
      "[0.         0.13511458 0.         0.10078652]\n",
      "tensor([[0.0000, 0.0833, 0.0000, 0.0978]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.0832904  0.         0.09775185]]\n",
      "[0.         0.0832904  0.         0.09775185]\n",
      "tensor([[0.0000, 0.2099, 0.0000, 0.0734]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.20994137 0.         0.07336348]]\n",
      "[0.         0.20994137 0.         0.07336348]\n",
      "tensor([[0.0000, 0.0790, 0.0000, 0.0833]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.07896894 0.         0.08333787]]\n",
      "[0.         0.07896894 0.         0.08333787]\n",
      "tensor([[0.0000, 0.1623, 0.0065, 0.1637]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.16232002 0.00650293 0.16370371]]\n",
      "[0.         0.16232002 0.00650293 0.16370371]\n",
      "tensor([[0.0000, 0.2464, 0.0000, 0.1039]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.24635473 0.         0.10394597]]\n",
      "[0.         0.24635473 0.         0.10394597]\n",
      "tensor([[0.0000, 0.1617, 0.0000, 0.0357]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.16167134 0.         0.0356858 ]]\n",
      "[0.         0.16167134 0.         0.0356858 ]\n",
      "tensor([[0.0000, 0.1798, 0.0000, 0.1103]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.17975534 0.         0.11026195]]\n",
      "[0.         0.17975534 0.         0.11026195]\n",
      "tensor([[0.0000, 0.0062, 0.0000, 0.0672]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.00615934 0.         0.0672316 ]]\n",
      "[0.         0.00615934 0.         0.0672316 ]\n",
      "tensor([[0.0000, 0.0291, 0.0000, 0.0968]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.02909801 0.         0.09679402]]\n",
      "[0.         0.02909801 0.         0.09679402]\n",
      "tensor([[0.0000, 0.0809, 0.0000, 0.0499]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.08090798 0.         0.04993713]]\n",
      "[0.         0.08090798 0.         0.04993713]\n",
      "tensor([[0.0000, 0.0256, 0.0533, 0.0677]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.02559204 0.0532734  0.06766544]]\n",
      "[0.         0.02559204 0.0532734  0.06766544]\n",
      "tensor([[0.0000, 0.0763, 0.0000, 0.0387]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.07628205 0.         0.03869843]]\n",
      "[0.         0.07628205 0.         0.03869843]\n",
      "tensor([[0.0000, 0.0327, 0.0000, 0.0309]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.03271589 0.         0.03085659]]\n",
      "[0.         0.03271589 0.         0.03085659]\n",
      "tensor([[0.0000, 0.0410, 0.0000, 0.1125]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.04103263 0.         0.11248649]]\n",
      "[0.         0.04103263 0.         0.11248649]\n",
      "tensor([[0.0000, 0.0828, 0.0000, 0.0586]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.08284985 0.         0.0585898 ]]\n",
      "[0.         0.08284985 0.         0.0585898 ]\n",
      "tensor([[0.0000, 0.1159, 0.0000, 0.0404]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.11592275 0.         0.04036447]]\n",
      "[0.         0.11592275 0.         0.04036447]\n",
      "tensor([[0.0000, 0.1093, 0.0000, 0.0657]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.10934699 0.         0.06568744]]\n",
      "[0.         0.10934699 0.         0.06568744]\n",
      "tensor([[0.0000, 0.0130, 0.0000, 0.0987]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.01303402 0.         0.09874117]]\n",
      "[0.         0.01303402 0.         0.09874117]\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.1848]], grad_fn=<ReluBackward0>)\n",
      "[[0.        0.        0.        0.1848049]]\n",
      "[0.        0.        0.        0.1848049]\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.1956]], grad_fn=<ReluBackward0>)\n",
      "[[0.        0.        0.        0.1956204]]\n",
      "[0.        0.        0.        0.1956204]\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.2008]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.         0.         0.20084514]]\n",
      "[0.         0.         0.         0.20084514]\n",
      "tensor([[0.0000, 0.0096, 0.0000, 0.1849]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.00960158 0.         0.18492198]]\n",
      "[0.         0.00960158 0.         0.18492198]\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.2106]], grad_fn=<ReluBackward0>)\n",
      "[[0.        0.        0.        0.2106284]]\n",
      "[0.        0.        0.        0.2106284]\n",
      "tensor([[0.0000, 0.0547, 0.0000, 0.2074]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.05474868 0.         0.2073959 ]]\n",
      "[0.         0.05474868 0.         0.2073959 ]\n",
      "tensor([[0.0000, 0.0000, 0.0393, 0.1483]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.         0.03933597 0.14825632]]\n",
      "[0.         0.         0.03933597 0.14825632]\n",
      "tensor([[0.0000, 0.1909, 0.0359, 0.0244]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.19094448 0.03591406 0.02439682]]\n",
      "[0.         0.19094448 0.03591406 0.02439682]\n",
      "tensor([[0.0000, 0.1546, 0.0174, 0.0948]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.15456282 0.01744493 0.09477036]]\n",
      "[0.         0.15456282 0.01744493 0.09477036]\n",
      "tensor([[0.0000, 0.1929, 0.0031, 0.0376]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.19290663 0.00306433 0.03763334]]\n",
      "[0.         0.19290663 0.00306433 0.03763334]\n",
      "tensor([[0.0000, 0.2821, 0.0000, 0.0582]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.28208876 0.         0.05820389]]\n",
      "[0.         0.28208876 0.         0.05820389]\n",
      "tensor([[0.0000, 0.2733, 0.0000, 0.1008]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.2733208  0.         0.10080925]]\n",
      "[0.         0.2733208  0.         0.10080925]\n",
      "tensor([[0.0000, 0.1471, 0.0033, 0.0859]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.14710805 0.00334581 0.08589458]]\n",
      "[0.         0.14710805 0.00334581 0.08589458]\n",
      "tensor([[0.0000, 0.0936, 0.0443, 0.0610]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.09355621 0.04434459 0.06104281]]\n",
      "[0.         0.09355621 0.04434459 0.06104281]\n",
      "tensor([[0.0000, 0.1618, 0.0945, 0.0770]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.16179073 0.09452287 0.07696547]]\n",
      "[0.         0.16179073 0.09452287 0.07696547]\n",
      "tensor([[0.0000, 0.0845, 0.0058, 0.0573]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.08449188 0.0058143  0.05727816]]\n",
      "[0.         0.08449188 0.0058143  0.05727816]\n",
      "tensor([[0.0000, 0.1498, 0.0000, 0.0228]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.14977726 0.         0.02276841]]\n",
      "[0.         0.14977726 0.         0.02276841]\n",
      "tensor([[0.0000, 0.1482, 0.0000, 0.0372]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.14817359 0.         0.03723431]]\n",
      "[0.         0.14817359 0.         0.03723431]\n",
      "tensor([[0.0000, 0.1165, 0.1000, 0.0298]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.11648703 0.09995806 0.02983417]]\n",
      "[0.         0.11648703 0.09995806 0.02983417]\n",
      "tensor([[0.0000, 0.0772, 0.0049, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.077243   0.00489328 0.        ]]\n",
      "[0.         0.077243   0.00489328 0.        ]\n",
      "tensor([[0.0000, 0.0775, 0.0000, 0.0703]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.07752597 0.         0.07032464]]\n",
      "[0.         0.07752597 0.         0.07032464]\n",
      "tensor([[0.0000, 0.0397, 0.0937, 0.0630]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.03968994 0.09369736 0.06303644]]\n",
      "[0.         0.03968994 0.09369736 0.06303644]\n",
      "tensor([[0.0000, 0.0586, 0.0015, 0.0383]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.05855732 0.00146624 0.0382972 ]]\n",
      "[0.         0.05855732 0.00146624 0.0382972 ]\n",
      "tensor([[0.0044, 0.0447, 0.0332, 0.0357]], grad_fn=<ReluBackward0>)\n",
      "[[0.00438282 0.04471944 0.0332169  0.03565495]]\n",
      "[0.00438282 0.04471944 0.0332169  0.03565495]\n",
      "tensor([[0.0508, 0.0000, 0.0000, 0.0172]], grad_fn=<ReluBackward0>)\n",
      "[[0.05083287 0.         0.         0.01722161]]\n",
      "[0.05083287 0.         0.         0.01722161]\n",
      "tensor([[0.0000, 0.1125, 0.0000, 0.0555]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.11249863 0.         0.05550189]]\n",
      "[0.         0.11249863 0.         0.05550189]\n",
      "tensor([[0.0000, 0.1234, 0.0000, 0.0305]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.12342936 0.         0.03045693]]\n",
      "[0.         0.12342936 0.         0.03045693]\n",
      "tensor([[0.0314, 0.1550, 0.0000, 0.0302]], grad_fn=<ReluBackward0>)\n",
      "[[0.03136779 0.15497072 0.         0.03019762]]\n",
      "[0.03136779 0.15497072 0.         0.03019762]\n",
      "tensor([[0.0000, 0.0445, 0.0000, 0.0880]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.04453956 0.         0.08800682]]\n",
      "[0.         0.04453956 0.         0.08800682]\n",
      "tensor([[0.0000, 0.1198, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.        0.1197902 0.        0.       ]]\n",
      "[0.        0.1197902 0.        0.       ]\n",
      "tensor([[0.0283, 0.0740, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.02829091 0.07397312 0.         0.        ]]\n",
      "[0.02829091 0.07397312 0.         0.        ]\n",
      "tensor([[0.0000, 0.0828, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.        0.0827825 0.        0.       ]]\n",
      "[0.        0.0827825 0.        0.       ]\n",
      "tensor([[0.0000, 0.0873, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.08733711 0.         0.        ]]\n",
      "[0.         0.08733711 0.         0.        ]\n",
      "tensor([[0., 0., 0., 0.]], grad_fn=<ReluBackward0>)\n",
      "[[0. 0. 0. 0.]]\n",
      "[0. 0. 0. 0.]\n",
      "tensor([[0.0000, 0.0498, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.04984121 0.         0.        ]]\n",
      "[0.         0.04984121 0.         0.        ]\n",
      "tensor([[0.0000, 0.0733, 0.0000, 0.0442]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.07333853 0.         0.04416763]]\n",
      "[0.         0.07333853 0.         0.04416763]\n",
      "tensor([[0.0000, 0.1246, 0.0252, 0.1323]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.12458336 0.02515577 0.13234502]]\n",
      "[0.         0.12458336 0.02515577 0.13234502]\n",
      "tensor([[0.0000, 0.1406, 0.0000, 0.0625]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.1406092  0.         0.06248309]]\n",
      "[0.         0.1406092  0.         0.06248309]\n",
      "tensor([[0.0000, 0.1571, 0.1209, 0.1043]], grad_fn=<ReluBackward0>)\n",
      "[[0.        0.1570605 0.1209034 0.1043245]]\n",
      "[0.        0.1570605 0.1209034 0.1043245]\n",
      "tensor([[0.0000, 0.2070, 0.0000, 0.1266]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.20704135 0.         0.12660225]]\n",
      "[0.         0.20704135 0.         0.12660225]\n",
      "tensor([[0.0000, 0.1904, 0.0216, 0.0516]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.1903764  0.02157777 0.05155694]]\n",
      "[0.         0.1903764  0.02157777 0.05155694]\n",
      "tensor([[0.0000, 0.1516, 0.0000, 0.1594]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.15155989 0.         0.15935817]]\n",
      "[0.         0.15155989 0.         0.15935817]\n",
      "tensor([[0.0000, 0.1095, 0.0000, 0.0409]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.10954449 0.         0.04086031]]\n",
      "[0.         0.10954449 0.         0.04086031]\n",
      "tensor([[0.0000, 0.0808, 0.0000, 0.0954]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.08084793 0.         0.0953788 ]]\n",
      "[0.         0.08084793 0.         0.0953788 ]\n",
      "tensor([[0.0000, 0.0621, 0.0000, 0.1041]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.06208059 0.         0.10409442]]\n",
      "[0.         0.06208059 0.         0.10409442]\n",
      "tensor([[0.0414, 0.1521, 0.0000, 0.0451]], grad_fn=<ReluBackward0>)\n",
      "[[0.041366   0.152075   0.         0.04509755]]\n",
      "[0.041366   0.152075   0.         0.04509755]\n",
      "tensor([[0.0362, 0.1236, 0.0192, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.03620909 0.12364689 0.01920946 0.        ]]\n",
      "[0.03620909 0.12364689 0.01920946 0.        ]\n",
      "tensor([[0.0009, 0.1235, 0.0670, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.00089502 0.12354575 0.0670267  0.        ]]\n",
      "[0.00089502 0.12354575 0.0670267  0.        ]\n",
      "tensor([[0.0000, 0.0114, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.01140463 0.         0.        ]]\n",
      "[0.         0.01140463 0.         0.        ]\n",
      "tensor([[0.0000, 0.0369, 0.0867, 0.1077]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.0369264  0.0867387  0.10765731]]\n",
      "[0.         0.0369264  0.0867387  0.10765731]\n",
      "tensor([[0.0000, 0.0508, 0.0000, 0.0806]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.05079598 0.         0.08058155]]\n",
      "[0.         0.05079598 0.         0.08058155]\n",
      "tensor([[0.0000, 0.0895, 0.0000, 0.1479]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.08946844 0.         0.14790677]]\n",
      "[0.         0.08946844 0.         0.14790677]\n",
      "tensor([[0.0000, 0.0792, 0.0000, 0.0723]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.07921078 0.         0.07227266]]\n",
      "[0.         0.07921078 0.         0.07227266]\n",
      "tensor([[0.0000, 0.1915, 0.0000, 0.0394]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.19145653 0.         0.03939703]]\n",
      "[0.         0.19145653 0.         0.03939703]\n",
      "tensor([[0.0000, 0.1146, 0.0000, 0.0519]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.11459012 0.         0.05188781]]\n",
      "[0.         0.11459012 0.         0.05188781]\n",
      "tensor([[0.0000, 0.0556, 0.0000, 0.0673]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.05560928 0.         0.06731607]]\n",
      "[0.         0.05560928 0.         0.06731607]\n",
      "tensor([[0.0000, 0.0838, 0.0000, 0.1827]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.08375473 0.         0.18267618]]\n",
      "[0.         0.08375473 0.         0.18267618]\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.2561]], grad_fn=<ReluBackward0>)\n",
      "[[0.        0.        0.        0.2560856]]\n",
      "[0.        0.        0.        0.2560856]\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.3135]], grad_fn=<ReluBackward0>)\n",
      "[[0.        0.        0.        0.3134951]]\n",
      "[0.        0.        0.        0.3134951]\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.1919]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.         0.         0.19194248]]\n",
      "[0.         0.         0.         0.19194248]\n",
      "tensor([[0.0000, 0.1024, 0.0000, 0.0526]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.10241981 0.         0.0526013 ]]\n",
      "[0.         0.10241981 0.         0.0526013 ]\n",
      "tensor([[0.0000, 0.1548, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.        0.1548092 0.        0.       ]]\n",
      "[0.        0.1548092 0.        0.       ]\n",
      "tensor([[0.0000, 0.1096, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.10962132 0.         0.        ]]\n",
      "[0.         0.10962132 0.         0.        ]\n",
      "tensor([[0.0000, 0.2054, 0.0000, 0.0056]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.20535181 0.         0.00558858]]\n",
      "[0.         0.20535181 0.         0.00558858]\n",
      "tensor([[0.0000, 0.1979, 0.0491, 0.1758]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.19789964 0.04906211 0.17580147]]\n",
      "[0.         0.19789964 0.04906211 0.17580147]\n",
      "tensor([[0.0000, 0.1415, 0.0123, 0.2498]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.14150469 0.01233515 0.24982312]]\n",
      "[0.         0.14150469 0.01233515 0.24982312]\n",
      "tensor([[0.0000, 0.1593, 0.0848, 0.2711]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.1592823  0.08481656 0.27106535]]\n",
      "[0.         0.1592823  0.08481656 0.27106535]\n",
      "tensor([[0.0000, 0.0709, 0.0849, 0.1936]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.07088488 0.08489481 0.19357115]]\n",
      "[0.         0.07088488 0.08489481 0.19357115]\n",
      "tensor([[0.0000, 0.1312, 0.1023, 0.0979]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.13116913 0.10227909 0.0978799 ]]\n",
      "[0.         0.13116913 0.10227909 0.0978799 ]\n",
      "tensor([[0.0000, 0.2962, 0.0184, 0.1109]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.29617175 0.01839024 0.11092972]]\n",
      "[0.         0.29617175 0.01839024 0.11092972]\n",
      "tensor([[0.0000, 0.2706, 0.0000, 0.1294]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.27061057 0.         0.1294158 ]]\n",
      "[0.         0.27061057 0.         0.1294158 ]\n",
      "tensor([[0.0000, 0.2364, 0.0000, 0.0657]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.23642448 0.         0.06568298]]\n",
      "[0.         0.23642448 0.         0.06568298]\n",
      "tensor([[0.0000, 0.1346, 0.0000, 0.1007]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.13464808 0.         0.1007022 ]]\n",
      "[0.         0.13464808 0.         0.1007022 ]\n",
      "tensor([[0.0000, 0.1204, 0.0198, 0.0762]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.12043352 0.01982037 0.07622822]]\n",
      "[0.         0.12043352 0.01982037 0.07622822]\n",
      "tensor([[0.0930, 0.1388, 0.0089, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.09300491 0.13877179 0.00891329 0.        ]]\n",
      "[0.09300491 0.13877179 0.00891329 0.        ]\n",
      "tensor([[0.0000, 0.1569, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.15686956 0.         0.        ]]\n",
      "[0.         0.15686956 0.         0.        ]\n",
      "tensor([[0.0834, 0.2551, 0.0000, 0.0076]], grad_fn=<ReluBackward0>)\n",
      "[[0.08335072 0.25508156 0.         0.00764259]]\n",
      "[0.08335072 0.25508156 0.         0.00764259]\n",
      "tensor([[0.0628, 0.2741, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.06279247 0.27406362 0.         0.        ]]\n",
      "[0.06279247 0.27406362 0.         0.        ]\n",
      "tensor([[0.0246, 0.1845, 0.0000, 0.0105]], grad_fn=<ReluBackward0>)\n",
      "[[0.02462542 0.18450111 0.         0.01049258]]\n",
      "[0.02462542 0.18450111 0.         0.01049258]\n",
      "tensor([[0.0000, 0.1599, 0.0000, 0.0893]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.15994477 0.         0.08926058]]\n",
      "[0.         0.15994477 0.         0.08926058]\n",
      "tensor([[0.0000, 0.2379, 0.0000, 0.1199]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.23786263 0.         0.11986014]]\n",
      "[0.         0.23786263 0.         0.11986014]\n",
      "tensor([[0.0000, 0.1777, 0.0000, 0.0874]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.17769422 0.         0.08737596]]\n",
      "[0.         0.17769422 0.         0.08737596]\n",
      "tensor([[0.0000, 0.2784, 0.0000, 0.1025]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.2783974  0.         0.10254557]]\n",
      "[0.         0.2783974  0.         0.10254557]\n",
      "tensor([[0.0000, 0.2481, 0.0000, 0.0409]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.2481398  0.         0.04087075]]\n",
      "[0.         0.2481398  0.         0.04087075]\n",
      "tensor([[0.0000, 0.1143, 0.0000, 0.0368]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.11434332 0.         0.03676531]]\n",
      "[0.         0.11434332 0.         0.03676531]\n",
      "tensor([[0.0000, 0.0719, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.07188012 0.         0.        ]]\n",
      "[0.         0.07188012 0.         0.        ]\n",
      "tensor([[0.0000, 0.1314, 0.0000, 0.0383]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.13137792 0.         0.03831077]]\n",
      "[0.         0.13137792 0.         0.03831077]\n",
      "tensor([[0.0127, 0.1823, 0.0000, 0.0100]], grad_fn=<ReluBackward0>)\n",
      "[[0.01269487 0.18229197 0.         0.00998176]]\n",
      "[0.01269487 0.18229197 0.         0.00998176]\n",
      "tensor([[0.0381, 0.1374, 0.0211, 0.0164]], grad_fn=<ReluBackward0>)\n",
      "[[0.03812157 0.1373807  0.02106852 0.01635591]]\n",
      "[0.03812157 0.1373807  0.02106852 0.01635591]\n",
      "tensor([[0.0000, 0.1897, 0.0000, 0.0904]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.18970163 0.         0.09040507]]\n",
      "[0.         0.18970163 0.         0.09040507]\n",
      "tensor([[0.0000, 0.1472, 0.0000, 0.0966]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.14724949 0.         0.0966474 ]]\n",
      "[0.         0.14724949 0.         0.0966474 ]\n",
      "tensor([[0.0000, 0.1118, 0.0000, 0.0565]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.11177967 0.         0.05645513]]\n",
      "[0.         0.11177967 0.         0.05645513]\n",
      "tensor([[0.0000, 0.1876, 0.0000, 0.0289]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.18756838 0.         0.0289062 ]]\n",
      "[0.         0.18756838 0.         0.0289062 ]\n",
      "tensor([[0.0000, 0.1442, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.14423285 0.         0.        ]]\n",
      "[0.         0.14423285 0.         0.        ]\n",
      "tensor([[0.0000, 0.1580, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.15803978 0.         0.        ]]\n",
      "[0.         0.15803978 0.         0.        ]\n",
      "tensor([[0.0000, 0.1263, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.        0.1263394 0.        0.       ]]\n",
      "[0.        0.1263394 0.        0.       ]\n",
      "tensor([[0.0000, 0.0065, 0.0000, 0.1011]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.00645497 0.         0.10106862]]\n",
      "[0.         0.00645497 0.         0.10106862]\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.2186]], grad_fn=<ReluBackward0>)\n",
      "[[0.       0.       0.       0.218607]]\n",
      "[0.       0.       0.       0.218607]\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.1761]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.         0.         0.17613043]]\n",
      "[0.         0.         0.         0.17613043]\n",
      "tensor([[0.0000, 0.0409, 0.0000, 0.1716]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.04091192 0.         0.17164753]]\n",
      "[0.         0.04091192 0.         0.17164753]\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.2088]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.         0.         0.20880026]]\n",
      "[0.         0.         0.         0.20880026]\n",
      "tensor([[0.0000, 0.0329, 0.0000, 0.1292]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.03290047 0.         0.12922525]]\n",
      "[0.         0.03290047 0.         0.12922525]\n",
      "tensor([[0.0000, 0.0636, 0.0000, 0.1140]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.06359817 0.         0.11402325]]\n",
      "[0.         0.06359817 0.         0.11402325]\n",
      "tensor([[0.0000, 0.1024, 0.0000, 0.0526]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.10241981 0.         0.0526013 ]]\n",
      "[0.         0.10241981 0.         0.0526013 ]\n",
      "tensor([[0.0000, 0.1104, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.11039905 0.         0.        ]]\n",
      "[0.         0.11039905 0.         0.        ]\n",
      "tensor([[0.0000, 0.1095, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.10953735 0.         0.        ]]\n",
      "[0.         0.10953735 0.         0.        ]\n",
      "tensor([[0.0000, 0.1596, 0.0000, 0.0078]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.1595914  0.         0.00777423]]\n",
      "[0.         0.1595914  0.         0.00777423]\n",
      "tensor([[0.0000, 0.2634, 0.0769, 0.0745]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.26340723 0.07685189 0.07448276]]\n",
      "[0.         0.26340723 0.07685189 0.07448276]\n",
      "tensor([[0.0000, 0.1734, 0.0000, 0.0515]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.1734168  0.         0.05151235]]\n",
      "[0.         0.1734168  0.         0.05151235]\n",
      "tensor([[0.0000, 0.1405, 0.0029, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.1404835  0.00285898 0.        ]]\n",
      "[0.         0.1404835  0.00285898 0.        ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 0.2410, 0.0000, 0.0202]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.24097365 0.         0.02021137]]\n",
      "[0.         0.24097365 0.         0.02021137]\n",
      "tensor([[0.0000, 0.1381, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.        0.1380648 0.        0.       ]]\n",
      "[0.        0.1380648 0.        0.       ]\n",
      "tensor([[0.0000, 0.2464, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.24639873 0.         0.        ]]\n",
      "[0.         0.24639873 0.         0.        ]\n",
      "tensor([[0.0000, 0.2815, 0.0000, 0.0399]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.281532   0.         0.03993695]]\n",
      "[0.         0.281532   0.         0.03993695]\n",
      "tensor([[0.0000, 0.1516, 0.0000, 0.1594]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.15155989 0.         0.15935817]]\n",
      "[0.         0.15155989 0.         0.15935817]\n",
      "tensor([[0.0000, 0.0012, 0.0000, 0.1743]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.00121999 0.         0.1742563 ]]\n",
      "[0.         0.00121999 0.         0.1742563 ]\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.2496]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.         0.         0.24956658]]\n",
      "[0.         0.         0.         0.24956658]\n",
      "tensor([[0.0000, 0.0083, 0.0000, 0.1765]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.00832435 0.         0.17652684]]\n",
      "[0.         0.00832435 0.         0.17652684]\n",
      "tensor([[0.0000, 0.1024, 0.0000, 0.0526]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.10241981 0.         0.0526013 ]]\n",
      "[0.         0.10241981 0.         0.0526013 ]\n",
      "tensor([[0.0000, 0.1104, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.11039905 0.         0.        ]]\n",
      "[0.         0.11039905 0.         0.        ]\n",
      "tensor([[0.0000, 0.1952, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.        0.1951848 0.        0.       ]]\n",
      "[0.        0.1951848 0.        0.       ]\n",
      "tensor([[0.0000, 0.1362, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.13618933 0.         0.        ]]\n",
      "[0.         0.13618933 0.         0.        ]\n",
      "tensor([[0.0000, 0.1326, 0.0000, 0.0308]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.13255453 0.         0.03083272]]\n",
      "[0.         0.13255453 0.         0.03083272]\n",
      "tensor([[0.0000, 0.1876, 0.0000, 0.0289]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.18756838 0.         0.0289062 ]]\n",
      "[0.         0.18756838 0.         0.0289062 ]\n",
      "tensor([[0.0000, 0.1146, 0.0000, 0.0519]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.11459012 0.         0.05188781]]\n",
      "[0.         0.11459012 0.         0.05188781]\n",
      "tensor([[0.0000, 0.0742, 0.0000, 0.0147]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.07423132 0.         0.0147408 ]]\n",
      "[0.         0.07423132 0.         0.0147408 ]\n",
      "tensor([[0.0000, 0.0049, 0.0000, 0.0992]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.00485454 0.         0.09920812]]\n",
      "[0.         0.00485454 0.         0.09920812]\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.1591]], grad_fn=<ReluBackward0>)\n",
      "[[0.       0.       0.       0.159076]]\n",
      "[0.       0.       0.       0.159076]\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.2389]], grad_fn=<ReluBackward0>)\n",
      "[[0.        0.        0.        0.2389436]]\n",
      "[0.        0.        0.        0.2389436]\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.3074]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.         0.         0.30738163]]\n",
      "[0.         0.         0.         0.30738163]\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.1071]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.         0.         0.10705294]]\n",
      "[0.         0.         0.         0.10705294]\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0501]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.         0.         0.05007687]]\n",
      "[0.         0.         0.         0.05007687]\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0672]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.         0.         0.06722701]]\n",
      "[0.         0.         0.         0.06722701]\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.2195]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.         0.         0.21949585]]\n",
      "[0.         0.         0.         0.21949585]\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.2520]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.         0.         0.25200146]]\n",
      "[0.         0.         0.         0.25200146]\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.2832]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.         0.         0.28321457]]\n",
      "[0.         0.         0.         0.28321457]\n",
      "tensor([[0.0000, 0.0131, 0.0000, 0.0880]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.01308784 0.         0.08799426]]\n",
      "[0.         0.01308784 0.         0.08799426]\n",
      "tensor([[0.0000, 0.1726, 0.0442, 0.0321]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.17262869 0.04422873 0.03211347]]\n",
      "[0.         0.17262869 0.04422873 0.03211347]\n",
      "tensor([[0.0000, 0.1997, 0.0000, 0.0125]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.19970928 0.         0.01254733]]\n",
      "[0.         0.19970928 0.         0.01254733]\n",
      "tensor([[0.0000, 0.2532, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.25316447 0.         0.        ]]\n",
      "[0.         0.25316447 0.         0.        ]\n",
      "tensor([[0.0000, 0.2379, 0.0761, 0.1160]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.23786524 0.07606602 0.11600122]]\n",
      "[0.         0.23786524 0.07606602 0.11600122]\n",
      "tensor([[0.0000, 0.1787, 0.0000, 0.0600]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.17867413 0.         0.05995907]]\n",
      "[0.         0.17867413 0.         0.05995907]\n",
      "tensor([[0.0084, 0.1488, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.00842708 0.1487875  0.         0.        ]]\n",
      "[0.00842708 0.1487875  0.         0.        ]\n",
      "tensor([[0.0000, 0.1634, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.16341719 0.         0.        ]]\n",
      "[0.         0.16341719 0.         0.        ]\n",
      "tensor([[0.0000, 0.1361, 0.0000, 0.0676]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.1360951  0.         0.06755292]]\n",
      "[0.         0.1360951  0.         0.06755292]\n",
      "tensor([[0.0000, 0.1033, 0.0000, 0.0380]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.10329988 0.         0.03799111]]\n",
      "[0.         0.10329988 0.         0.03799111]\n",
      "tensor([[0.0000, 0.1519, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.15192607 0.         0.        ]]\n",
      "[0.         0.15192607 0.         0.        ]\n",
      "tensor([[0.0000, 0.2077, 0.0000, 0.0334]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.20768143 0.         0.03342947]]\n",
      "[0.         0.20768143 0.         0.03342947]\n",
      "tensor([[0.0000, 0.2108, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.21082783 0.         0.        ]]\n",
      "[0.         0.21082783 0.         0.        ]\n",
      "tensor([[0.0000, 0.2906, 0.0000, 0.0366]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.2906195  0.         0.03660623]]\n",
      "[0.         0.2906195  0.         0.03660623]\n",
      "tensor([[0.0000, 0.3210, 0.0000, 0.1143]], grad_fn=<ReluBackward0>)\n",
      "[[0.        0.321033  0.        0.1142831]]\n",
      "[0.        0.321033  0.        0.1142831]\n",
      "tensor([[0.0000, 0.2059, 0.0000, 0.0296]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.20588379 0.         0.02957452]]\n",
      "[0.         0.20588379 0.         0.02957452]\n",
      "tensor([[0.0000, 0.1546, 0.0000, 0.0918]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.15462132 0.         0.09175327]]\n",
      "[0.         0.15462132 0.         0.09175327]\n",
      "tensor([[0.0000, 0.1388, 0.0000, 0.0842]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.13878709 0.         0.08421272]]\n",
      "[0.         0.13878709 0.         0.08421272]\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0952]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.         0.         0.09519835]]\n",
      "[0.         0.         0.         0.09519835]\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0795]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.         0.         0.07951234]]\n",
      "[0.         0.         0.         0.07951234]\n",
      "tensor([[0.0000, 0.0606, 0.0000, 0.0026]], grad_fn=<ReluBackward0>)\n",
      "[[0.        0.0606216 0.        0.0026075]]\n",
      "[0.        0.0606216 0.        0.0026075]\n",
      "tensor([[0.0000, 0.0506, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.        0.0506243 0.        0.       ]]\n",
      "[0.        0.0506243 0.        0.       ]\n",
      "tensor([[0.0000, 0.0467, 0.0000, 0.0923]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.04666951 0.         0.09225582]]\n",
      "[0.         0.04666951 0.         0.09225582]\n",
      "tensor([[0.0000, 0.1095, 0.0000, 0.0409]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.10954449 0.         0.04086031]]\n",
      "[0.         0.10954449 0.         0.04086031]\n",
      "tensor([[0.0000, 0.0808, 0.0000, 0.0954]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.08084793 0.         0.0953788 ]]\n",
      "[0.         0.08084793 0.         0.0953788 ]\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.1503]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.         0.         0.15027812]]\n",
      "[0.         0.         0.         0.15027812]\n",
      "tensor([[0.0000, 0.0863, 0.0000, 0.0654]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.08631162 0.         0.06539259]]\n",
      "[0.         0.08631162 0.         0.06539259]\n",
      "tensor([[0.0000, 0.0498, 0.0000, 0.0771]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.04976431 0.         0.07713541]]\n",
      "[0.         0.04976431 0.         0.07713541]\n",
      "tensor([[0.0000, 0.0926, 0.0000, 0.0175]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.09262685 0.         0.0175226 ]]\n",
      "[0.         0.09262685 0.         0.0175226 ]\n",
      "tensor([[0.0000, 0.1389, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.13893585 0.         0.        ]]\n",
      "[0.         0.13893585 0.         0.        ]\n",
      "tensor([[0.0000, 0.0757, 0.0000, 0.0035]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.07574786 0.         0.00348814]]\n",
      "[0.         0.07574786 0.         0.00348814]\n",
      "tensor([[0.0000, 0.0315, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.03151535 0.         0.        ]]\n",
      "[0.         0.03151535 0.         0.        ]\n",
      "tensor([[0.0000, 0.0563, 0.0215, 0.0299]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.05630262 0.02145369 0.02987587]]\n",
      "[0.         0.05630262 0.02145369 0.02987587]\n",
      "tensor([[0.0000, 0.0298, 0.0000, 0.0166]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.02978972 0.         0.01658724]]\n",
      "[0.         0.02978972 0.         0.01658724]\n",
      "tensor([[0.0739, 0.0844, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.07392114 0.08438531 0.         0.        ]]\n",
      "[0.07392114 0.08438531 0.         0.        ]\n",
      "tensor([[0.0362, 0.1236, 0.0192, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.03620909 0.12364689 0.01920946 0.        ]]\n",
      "[0.03620909 0.12364689 0.01920946 0.        ]\n",
      "tensor([[0.0801, 0.2053, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "[[0.08011582 0.20534286 0.         0.        ]]\n",
      "[0.08011582 0.20534286 0.         0.        ]\n",
      "tensor([[0.0127, 0.1823, 0.0000, 0.0100]], grad_fn=<ReluBackward0>)\n",
      "[[0.01269487 0.18229197 0.         0.00998176]]\n",
      "[0.01269487 0.18229197 0.         0.00998176]\n",
      "tensor([[0.0000, 0.1980, 0.0089, 0.0341]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.198049   0.00885618 0.03405208]]\n",
      "[0.         0.198049   0.00885618 0.03405208]\n",
      "tensor([[0.0000, 0.2278, 0.0129, 0.0423]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.22776756 0.01290331 0.04229887]]\n",
      "[0.         0.22776756 0.01290331 0.04229887]\n",
      "tensor([[0.0000, 0.1752, 0.0005, 0.0555]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.17523155 0.00048365 0.05553193]]\n",
      "[0.         0.17523155 0.00048365 0.05553193]\n",
      "tensor([[0.0000, 0.2401, 0.0000, 0.0692]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.24013837 0.         0.06918252]]\n",
      "[0.         0.24013837 0.         0.06918252]\n",
      "tensor([[0.0000, 0.2238, 0.0000, 0.1630]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.22384608 0.         0.1630039 ]]\n",
      "[0.         0.22384608 0.         0.1630039 ]\n",
      "tensor([[0.0000, 0.3654, 0.0000, 0.1792]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.36538962 0.         0.1792207 ]]\n",
      "[0.         0.36538962 0.         0.1792207 ]\n",
      "tensor([[0.0000, 0.2865, 0.0595, 0.1815]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.28645158 0.05948637 0.18151481]]\n",
      "[0.         0.28645158 0.05948637 0.18151481]\n",
      "tensor([[0.0000, 0.2031, 0.0212, 0.1905]], grad_fn=<ReluBackward0>)\n",
      "[[0.         0.20306331 0.02120038 0.19048801]]\n",
      "[0.         0.20306331 0.02120038 0.19048801]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Model' object has no attribute 'online_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-75b92251f4db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR_avg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loop_ddqn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mddqn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplay_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_episodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menable_visualization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menable_visualization\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-26-724e1419375a>\u001b[0m in \u001b[0;36mtrain_loop_ddqn\u001b[0;34m(ddqn, model, env, replay_buffer, num_episodes, enable_visualization, batch_size, gamma)\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;31m# If replay buffer contains more than 1000 samples, perform one training step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreplay_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer_length\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_batch_and_calculate_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplay_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-137f9d33cfb2>\u001b[0m in \u001b[0;36msample_batch_and_calculate_loss\u001b[0;34m(ddqn, replay_buffer, batch_size, gamma)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# Define the q_online_curr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mq_online_curr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mddqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monline_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# Define the q_online_next\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    583\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 585\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Model' object has no attribute 'online_model'"
     ]
    }
   ],
   "source": [
    "R, R_avg = train_loop_ddqn(ddqn, model, env, replay_buffer, num_episodes, enable_visualization=enable_visualization, batch_size=batch_size, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
