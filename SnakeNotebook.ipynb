{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snake with Q-learning algorithm from HA3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the model\n",
    "import dqn_model\n",
    "\n",
    "# Import dependencies\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import gym\n",
    "from collections import namedtuple\n",
    "from dqn_model import DoubleQLearningModel, ExperienceReplay\n",
    "\n",
    "# Set device as cpu/cuda\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the greedy policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eps_greedy_policy(q_values, eps):\n",
    "    '''\n",
    "    Creates an epsilon-greedy policy\n",
    "    :param q_values: set of Q-values of shape (num actions,)\n",
    "    :param eps: probability of taking a uniform random action \n",
    "    :return: policy of shape (num actions,)\n",
    "    '''\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    policy = np.zeros(len(q_values))\n",
    "    Pe = random.uniform(0,1)\n",
    "    \n",
    "    if(Pe <= eps):\n",
    "        policy = np.full((len(q_values),), 1/len(q_values))\n",
    "    else:\n",
    "        policy[np.argmax(q_values)] = 1\n",
    "    \n",
    "    return policy\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the three functions needed for dqn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_q_and_take_action(ddqn, state, eps):\n",
    "    '''\n",
    "    Calculate Q-values for current state, and take an action according to an epsilon-greedy policy.\n",
    "    Inputs:\n",
    "        ddqn   - DDQN model. An object holding the online / offline Q-networks, and some related methods.\n",
    "        state  - Current state. Numpy array, shape (1, num_states).\n",
    "        eps    - Exploration parameter.\n",
    "    Returns:\n",
    "        q_online_curr   - Q(s,a) for current state s. Numpy array, shape (1, num_actions) or  (num_actions,).\n",
    "        curr_action     - Selected action (0 or 1, i.e. left or right), sampled from epsilon-greedy policy. Integer.\n",
    "    '''\n",
    "    # FYI:\n",
    "    # ddqn.online_model & ddqn.offline_model are Pytorch modules for online / offline Q-networks, which take the state as input, and output the Q-values for all actions.\n",
    "    # Input shape (batch_size, num_states). Output shape (batch_size, num_actions).\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # Convert the state into a tensor\n",
    "    torch_state = torch.tensor(state,dtype=torch.float32)\n",
    "    \n",
    "    # --------- Online -------- # \n",
    "    \n",
    "    # Compute the Q-values and convert into numpy\n",
    "    Q_tensor_online = ddqn.online_model(torch_state)  \n",
    "    q_online_curr = Q_tensor_online.detach().cpu().numpy()\n",
    "    \n",
    "    # Compute the policy and convert into tensor\n",
    "    policy = eps_greedy_policy(q_online_curr, eps)\n",
    "    torch_policy = torch.tensor(policy,dtype=torch.float32)\n",
    "    \n",
    "    # Compute the action from the policy\n",
    "    actions = [i for i in range(len(policy))]\n",
    "    curr_action = random.choices(actions,policy)\n",
    "    \n",
    "    # Convert the curr_action into an int\n",
    "    curr_action = int(curr_action[0])\n",
    "    \n",
    "    return q_online_curr, curr_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_q_targets(q1_batch, q2_batch, r_batch, nonterminal_batch, gamma=.99):\n",
    "    '''\n",
    "    Calculates the Q target used for the loss\n",
    "    : param q1_batch: Batch of Q(s', a) from online network. FloatTensor, shape (N, num actions)\n",
    "    : param q2_batch: Batch of Q(s', a) from target network. FloatTensor, shape (N, num actions)\n",
    "    : param r_batch: Batch of rewards. FloatTensor, shape (N,)\n",
    "    : param nonterminal_batch: Batch of booleans, with False elements if state s' is terminal and True otherwise. BoolTensor, shape (N,)\n",
    "    : param gamma: Discount factor, float.\n",
    "    : return: Q target. FloatTensor, shape (N,)\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    N = q1_batch.shape[0]\n",
    "    Y = np.zeros(N)\n",
    "    \n",
    "    for i in range(N): \n",
    "        \n",
    "        if nonterminal_batch[i] == True:\n",
    "            values, a = q1_batch[i,:].max(0)\n",
    "            Y[i] = r_batch[i] + gamma * q2_batch[i,a]\n",
    "        \n",
    "        else: \n",
    "            Y[i] = r_batch[i]\n",
    "    \n",
    "    Y = torch.tensor(Y,dtype=torch.float32)\n",
    "    \n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_batch_and_calculate_loss(ddqn, replay_buffer, batch_size, gamma):\n",
    "    '''\n",
    "    Sample mini-batch from replay buffer, and compute the mini-batch loss\n",
    "    Inputs:\n",
    "        ddqn          - DDQN model. An object holding the online / offline Q-networks, and some related methods.\n",
    "        replay_buffer - Replay buffer object (from which smaples will be drawn)\n",
    "        batch_size    - Batch size\n",
    "        gamma         - Discount factor\n",
    "    Returns:\n",
    "        Mini-batch loss, on which .backward() will be called to compute gradient.\n",
    "    '''\n",
    "    # Sample a minibatch of transitions from replay buffer\n",
    "    curr_state, curr_action, reward, next_state, nonterminal = replay_buffer.sample_minibatch(batch_size)\n",
    "\n",
    "    # FYI:\n",
    "    # ddqn.online_model & ddqn.offline_model are Pytorch modules for online / offline Q-networks, which take the state as input, and output the Q-values for all actions.\n",
    "    # Input shape (batch_size, num_states). Output shape (batch_size, num_actions).\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # Define the q_online_curr\n",
    "    q_online_curr = ddqn.online_model(curr_state)\n",
    "    \n",
    "    # Define the q_online_next\n",
    "    q_online_next = ddqn.online_model(next_state)      \n",
    "    \n",
    "    # Define the q_offline_next\n",
    "    with torch.no_grad():\n",
    "        q_offline_next = ddqn.offline_model(next_state)  \n",
    "    \n",
    "    \n",
    "    q_target = calculate_q_targets(q_online_next, q_offline_next, reward, nonterminal, gamma=gamma)\n",
    "    loss = ddqn.calc_loss(q_online_curr, q_target, curr_action)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop_ddqn(ddqn, env, replay_buffer, num_episodes, enable_visualization=False, batch_size=64, gamma=.94):        \n",
    "    Transition = namedtuple(\"Transition\", [\"s\", \"a\", \"r\", \"next_s\", \"t\"])\n",
    "    eps = 1.\n",
    "    eps_end = .1 \n",
    "    eps_decay = .001\n",
    "    tau = 1000\n",
    "    cnt_updates = 0\n",
    "    R_buffer = []\n",
    "    R_avg = []\n",
    "    for i in range(num_episodes):\n",
    "        state = env.reset() # Initial state\n",
    "        #state = state[None,:] # Add singleton dimension, to represent as batch of size 1.\n",
    "        finish_episode = False # Initialize\n",
    "        ep_reward = 0 # Initialize \"Episodic reward\", i.e. the total reward for episode, when disregarding discount factor.\n",
    "        q_buffer = []\n",
    "        steps = 0\n",
    "        while not finish_episode:\n",
    "            if enable_visualization:\n",
    "                env.render() # comment this line out if you don't want to / cannot render the environment on your system\n",
    "            steps += 1\n",
    "            #print(steps)\n",
    "            # Take one step in environment. No need to compute gradients,\n",
    "            # we will just store transition to replay buffer, and later sample a whole batch\n",
    "            # from the replay buffer to actually take a gradient step.\n",
    "            q_online_curr, curr_action = calc_q_and_take_action(ddqn, state, eps)\n",
    "            q_buffer.append(q_online_curr)\n",
    "            #print(curr_action)\n",
    "            new_state, reward, finish_episode, _ = env.step(curr_action) # take one step in the evironment\n",
    "           # print(new_state.reshape(5,5))\n",
    "           # print(reward)\n",
    "            #print(finish_episode)\n",
    "           # new_state = new_state[None,:]\n",
    "            \n",
    "            # Assess whether terminal state was reached.\n",
    "            # The episode may end due to having reached 200 steps, but we should not regard this as reaching the terminal state, and hence not disregard Q(s',a) from the Q target.\n",
    "            # https://arxiv.org/abs/1712.00378\n",
    "            #print(steps)\n",
    "            nonterminal_to_buffer = not finish_episode or steps == 2000\n",
    "            #print(nonterminal_to_buffer)\n",
    "            \n",
    "            # Store experienced transition to replay buffer\n",
    "            replay_buffer.add(Transition(s=state, a=curr_action, r=reward, next_s=new_state, t=nonterminal_to_buffer))\n",
    "\n",
    "            state = new_state\n",
    "            ep_reward += reward\n",
    "            \n",
    "            # If replay buffer contains more than 1000 samples, perform one training step\n",
    "            if replay_buffer.buffer_length > 1000:\n",
    "                loss = sample_batch_and_calculate_loss(ddqn, replay_buffer, batch_size, gamma)\n",
    "                ddqn.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                ddqn.optimizer.step()\n",
    "\n",
    "                cnt_updates += 1\n",
    "                if cnt_updates % tau == 0:\n",
    "                    ddqn.update_target_network()\n",
    "            if steps == 200:\n",
    "                break\n",
    "                \n",
    "        eps = max(eps - eps_decay, eps_end) # decrease epsilon        \n",
    "        R_buffer.append(ep_reward)\n",
    "        \n",
    "        # Running average of episodic rewards (total reward, disregarding discount factor)\n",
    "        R_avg.append(.05 * R_buffer[i] + .95 * R_avg[i-1]) if i > 0 else R_avg.append(R_buffer[i])\n",
    "\n",
    "        print('Episode: {:d}, Total Reward (running avg): {:4.0f} ({:.2f}) Epsilon: {:.3f}, Avg Q: {:.4g}'.format(i, ep_reward, R_avg[-1], eps, np.mean(np.array(q_buffer))))\n",
    "        \n",
    "        # If running average > 195 (close to 200), the task is considered solved\n",
    "        if R_avg[-1] > 195:\n",
    "            return R_buffer, R_avg\n",
    "    return R_buffer, R_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "#import universe\n",
    "from gym import error, spaces, utils\n",
    "from gym.utils import seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the environment\n",
    "env = gym.make(\"snake5x5-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Enable visualization? Does not work in all environments.\n",
    "enable_visualization = False;\n",
    "\n",
    "\n",
    "# Initializations\n",
    "num_actions = env.action_space.n\n",
    "num_states = env.observation_space.shape[1]\n",
    "num_episodes = 5\n",
    "batch_size = 10\n",
    "gamma = .99\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# Object holding our online / offline Q-Networks\n",
    "ddqn = DoubleQLearningModel(device, num_states, num_actions, learning_rate)\n",
    "\n",
    "# Create replay buffer, where experience in form of tuples <s,a,r,s',t>, gathered from the environment is stored \n",
    "# for training\n",
    "replay_buffer = ExperienceReplay(device, num_states)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#state = env.step(0)\n",
    "#print(state[0].reshape(9,9))\n",
    "#print(state[1])\n",
    "#print(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R, R_avg = train_loop_ddqn(ddqn, env, replay_buffer, num_episodes, enable_visualization=enable_visualization, batch_size=batch_size, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rewards = plt.plot(R, alpha=None,color='k', label='reward')\n",
    "avg_rewards = plt.plot(R_avg,color='r',label='average reward')\n",
    "plt.legend(bbox_to_anchor=(1.01, 1), loc=2, borderaxespad=0.)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylim(0,10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below for visualizing results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "num_episodes = 100\n",
    "env = gym.make(\"snake5x5-v0\")\n",
    "\n",
    "if True:\n",
    "    for i in range(num_episodes):\n",
    "            state = env.reset() #reset to initial state\n",
    "            state = state[None,:]\n",
    "            terminal = False # reset terminal flag\n",
    "            while not terminal:\n",
    "                env.render()\n",
    "                time.sleep(.05)\n",
    "                with torch.no_grad():\n",
    "                    q_values = ddqn.online_model(torch.tensor(state, dtype=torch.float, device=device)).cpu().numpy()\n",
    "                policy = eps_greedy_policy(q_values.squeeze(), .1) # greedy policy\n",
    "                action = np.random.choice(num_actions, p=policy)\n",
    "                state, reward, terminal, _ = env.step(action) # take one step in the evironment\n",
    "                state = state[None,:]\n",
    "    # close window\n",
    "    env.close();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snake 11x11 Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop_ddqn(ddqn, env, replay_buffer, num_episodes, enable_visualization=False, batch_size=64, gamma=.94):        \n",
    "    Transition = namedtuple(\"Transition\", [\"s\", \"a\", \"r\", \"next_s\", \"t\"])\n",
    "    eps = 1.\n",
    "    eps_end = .2 \n",
    "    eps_decay = .001\n",
    "    tau = 1000\n",
    "    cnt_updates = 0\n",
    "    R_buffer = []\n",
    "    R_avg = []\n",
    "    for i in range(num_episodes):\n",
    "        state = env.reset() # Initial state\n",
    "        #state = state[None,:] # Add singleton dimension, to represent as batch of size 1.\n",
    "        finish_episode = False # Initialize\n",
    "        ep_reward = 0 # Initialize \"Episodic reward\", i.e. the total reward for episode, when disregarding discount factor.\n",
    "        q_buffer = []\n",
    "        steps = 0\n",
    "        while not finish_episode:\n",
    "            if enable_visualization:\n",
    "                env.render() # comment this line out if you don't want to / cannot render the environment on your system\n",
    "            steps += 1\n",
    "            #print(steps)\n",
    "            # Take one step in environment. No need to compute gradients,\n",
    "            # we will just store transition to replay buffer, and later sample a whole batch\n",
    "            # from the replay buffer to actually take a gradient step.\n",
    "            q_online_curr, curr_action = calc_q_and_take_action(ddqn, state, eps)\n",
    "            q_buffer.append(q_online_curr)\n",
    "            \n",
    "            new_state, reward, finish_episode, score = env.step(curr_action) # take one step in the evironment\n",
    "            #print(new_state.reshape(5,5))\n",
    "            #print(reward)\n",
    "            #print(finish_episode)\n",
    "            #new_state = new_state[None,:]\n",
    "            \n",
    "            # Assess whether terminal state was reached.\n",
    "            # The episode may end due to having reached 200 steps, but we should not regard this as reaching the terminal state, and hence not disregard Q(s',a) from the Q target.\n",
    "            # https://arxiv.org/abs/1712.00378\n",
    "            #print(steps)\n",
    "            nonterminal_to_buffer = not finish_episode or steps == 2000\n",
    "            #print(nonterminal_to_buffer)\n",
    "            \n",
    "            # Store experienced transition to replay buffer\n",
    "            replay_buffer.add(Transition(s=state, a=curr_action, r=reward, next_s=new_state, t=nonterminal_to_buffer))\n",
    "\n",
    "            state = new_state\n",
    "            ep_reward += reward\n",
    "            \n",
    "            # If replay buffer contains more than 1000 samples, perform one training step\n",
    "            if replay_buffer.buffer_length > 1000:\n",
    "                loss = sample_batch_and_calculate_loss(ddqn, replay_buffer, batch_size, gamma)\n",
    "                ddqn.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                ddqn.optimizer.step()\n",
    "\n",
    "                cnt_updates += 1\n",
    "                if cnt_updates % tau == 0:\n",
    "                    ddqn.update_target_network()\n",
    "          \n",
    "        eps = max(eps - eps_decay, eps_end) # decrease epsilon        \n",
    "        R_buffer.append(ep_reward)\n",
    "        \n",
    "        # Running average of episodic rewards (total reward, disregarding discount factor)\n",
    "        R_avg.append(.05 * R_buffer[i] + .95 * R_avg[i-1]) if i > 0 else R_avg.append(R_buffer[i])\n",
    "\n",
    "        print('Episode: {:d}, Total Reward (running avg): {:4.0f} ({:.2f}) Epsilon: {:.3f}, Avg Q: {:.4g}, Score {}'.format(i, ep_reward, R_avg[-1], eps, np.mean(np.array(q_buffer)),score))\n",
    "        \n",
    "        # If running average > 195 (close to 200), the task is considered solved\n",
    "        if R_avg[-1] > 195:\n",
    "            return R_buffer, R_avg\n",
    "    return R_buffer, R_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the environment\n",
    "env = gym.make(\"Snake11x11-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable visualization? Does not work in all environments.\n",
    "enable_visualization = False;\n",
    "\n",
    "# Initializations\n",
    "num_actions = env.action_space.n\n",
    "num_states = env.observation_space.shape[1]\n",
    "num_episodes = 1300\n",
    "batch_size = 8\n",
    "gamma = .99\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# Object holding our online / offline Q-Networks\n",
    "ddqn = DoubleQLearningModel(device, num_states, num_actions, learning_rate)\n",
    "\n",
    "# Create replay buffer, where experience in form of tuples <s,a,r,s',t>, gathered from the environment is stored \n",
    "# for training\n",
    "replay_buffer = ExperienceReplay(device, num_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R, R_avg = train_loop_ddqn(ddqn, env, replay_buffer, num_episodes, enable_visualization=enable_visualization, batch_size=batch_size, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rewards = plt.plot(R, alpha=.4, label='R')\n",
    "avg_rewards = plt.plot(R_avg,label='avg R')\n",
    "plt.legend(bbox_to_anchor=(1.01, 1), loc=2, borderaxespad=0.)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylim(0, 10)\n",
    "plt.xlim(0, 1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "num_episodes = 10\n",
    "#env = gym.make(\"Snake11x11-v0\")\n",
    "\n",
    "if True:\n",
    "    for i in range(num_episodes):\n",
    "            state = env.reset() #reset to initial state\n",
    "            state = state[None,:]\n",
    "            terminal = False # reset terminal flag\n",
    "            while not terminal:\n",
    "                env.render()\n",
    "                time.sleep(.05)\n",
    "                with torch.no_grad():\n",
    "                    q_values = ddqn.online_model(torch.tensor(state, dtype=torch.float, device=device)).cpu().numpy()\n",
    "                policy = eps_greedy_policy(q_values.squeeze(), .1) # greedy policy\n",
    "                action = np.random.choice(num_actions, p=policy)\n",
    "                state, reward, terminal, _ = env.step(action) # take one step in the evironment\n",
    "                state = state[None,:]\n",
    "    # close window\n",
    "    env.close();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
